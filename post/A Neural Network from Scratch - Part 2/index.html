<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Jonathan Weisberg's Homepage">
    <meta name="author" content="Jonathan Weisberg">
    
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@jweisber">
    <meta name="twitter:creator" content="@jweisber">
    <meta name="twitter:title" content="Building a Neural Network from Scratch: Part 2">
    <meta name="twitter:description" content="In this post we&rsquo;ll improve our training algorithm from the previous post. When we&rsquo;re done we&rsquo;ll be able to achieve 98% precision on the MNIST data set, after just 9 epochs of training&mdash;which only takes about 30 seconds to run on my laptop.
For comparison, last time we only achieved 92% precision after 2,000 epochs of training, which took over an hour!
The main driver in this improvement is just switching from batch gradient descent to mini-batch gradient descent.">
    <meta name="twitter:image" content="https://i.imgflip.com/2252up.jpg">

    <meta property="og:url" content="/post/A%20Neural%20Network%20from%20Scratch%20-%20Part%202/" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="Building a Neural Network from Scratch: Part 2" />
    <meta property="og:description" content="In this post we&rsquo;ll improve our training algorithm from the previous post. When we&rsquo;re done we&rsquo;ll be able to achieve 98% precision on the MNIST data set, after just 9 epochs of training&mdash;which only takes about 30 seconds to run on my laptop.
For comparison, last time we only achieved 92% precision after 2,000 epochs of training, which took over an hour!
The main driver in this improvement is just switching from batch gradient descent to mini-batch gradient descent." />
    <meta property="og:image" content="https://i.imgflip.com/2252up.jpg" />


    <title>Building a Neural Network from Scratch: Part 2</title>

    <link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet">
    <link href="/css/prism.css" rel="stylesheet" />
    <link href="/css/scarab.css" rel="stylesheet">
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        "HTML-CSS": {
          availableFonts: ["Gyre-Pagella"],
          preferredFont: "Gyre-Pagella",
          webFont: "Gyre-Pagella",
          imageFont: "Gyre-Pagella",
        }
      });
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    
    
    

</head>

<body>

<nav class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-navbar-collapse-1">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="http://jonathanweisberg.org/">Jonathan Weisberg</a>
    </div>
    
    <div class="collapse navbar-collapse" id="bs-navbar-collapse-1">
      <ul class="nav navbar-nav">
        <li>
          <a href="/pdf/CV.pdf">
            <i class="fa fa-briefcase"></i>&nbsp;
            CV
          </a>
        </li>
        <li>
          <a href="/post/">
            <i class="fa fa-code"></i>&nbsp;
            Blog
          </a>
        </li>
        <li>
          <a href="/publication/">
            <i class="fa fa-files-o"></i>&nbsp;
            Research
          </a>
        </li>
      </ul>
    </div>
  </div>
</nav>



  
  <div class="container">

    
    <div class="row">
      <div class="col-md-3 hidden-sm hidden-xs text-right">
        <h1 class="post-title">&nbsp;</h1>
        <p>
          <em>Mon, Mar 5, 2018</em><br />
          
          
          
          <span class="post-tags">
            
            <a class="post-tag" href="http://jonathanweisberg.org/tags/machine-learning">Machine Learning</a>
             &bull;   
            
            <a class="post-tag" href="http://jonathanweisberg.org/tags/neural-networks">Neural Networks</a>
             &bull;  
            
            <a class="post-tag" href="http://jonathanweisberg.org/tags/python">Python</a>
            
            
          </span>
          
          
        </p>
      </div>

      <div class="col-md-7 blog-post">

        <h1>Building a Neural Network from Scratch: Part 2</h1>

        <div class="hidden-md hidden-lg">
          <div>
            <em>Mon, Mar 5, 2018</em>
          </div>
          
          
          
          <p class="post-tags">
            
            <a class="post-tag" href="http://jonathanweisberg.org/tags/machine-learning">Machine Learning</a>
             &bull;   
            
            <a class="post-tag" href="http://jonathanweisberg.org/tags/neural-networks">Neural Networks</a>
             &bull;  
            
            <a class="post-tag" href="http://jonathanweisberg.org/tags/python">Python</a>
            
            
          </p>
          
          
        </div>

        <div class="post-body">
          <p>In this post we&rsquo;ll improve our training algorithm from the <a href="/post/A%20Neural%20Network%20from%20Scratch%20-%20Part%201/">previous post</a>. When we&rsquo;re done we&rsquo;ll be able to achieve 98% precision on the MNIST data set, after just 9 epochs of training&mdash;which only takes about 30 seconds to run on my laptop.</p>

<p>For comparison, last time we only achieved 92% precision after 2,000 epochs of training, which took over an hour!</p>

<p>The main driver in this improvement is just switching from batch gradient descent to <em>mini</em>-batch gradient descent. But we&rsquo;ll also make two other, smaller improvements: we&rsquo;ll add momentum to our descent algorithm, and we&rsquo;ll smarten up the initialization of our network&rsquo;s weights.</p>

<p>We&rsquo;ll also reorganize our code a bit while we&rsquo;re at it, making things more modular.</p>

<p>But first we need to import and massage our data. These steps are the same as in the previous post:</p>

<pre><code class="language-python">from sklearn.datasets import fetch_mldata
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

# import
mnist = fetch_mldata('MNIST original')
X, y = mnist[&quot;data&quot;], mnist[&quot;target&quot;]

# scale
X = X / 255

# one-hot encode labels
digits = 10
examples = y.shape[0]
y = y.reshape(1, examples)
Y_new = np.eye(digits)[y.astype('int32')]
Y_new = Y_new.T.reshape(digits, examples)

# split, reshape, shuffle
m = 60000
m_test = X.shape[0] - m
X_train, X_test = X[:m].T, X[m:].T
Y_train, Y_test = Y_new[:,:m], Y_new[:,m:]
shuffle_index = np.random.permutation(m)
X_train, Y_train = X_train[:, shuffle_index], Y_train[:, shuffle_index]
</code></pre>

<p>Then we&rsquo;ll define our key functions. Only the last two are new, and they just put the steps of forward and backward propagation into their own functions. This tidies up the training code to follow, so that we can focus on the novel elements, especially mini-batch descent and momentum.</p>

<p>Notice that in the process we introduce three dictionaries:<code>params</code>, <code>cache</code>, and <code>grads</code>. These are for conveniently passing information back and forth between the forward and backward passes.</p>

<pre><code class="language-python">def sigmoid(z):
    s = 1. / (1. + np.exp(-z))
    return s

def compute_loss(Y, Y_hat):

    L_sum = np.sum(np.multiply(Y, np.log(Y_hat)))
    m = Y.shape[1]
    L = -(1./m) * L_sum

    return L

def feed_forward(X, params):

    cache = {}

    cache[&quot;Z1&quot;] = np.matmul(params[&quot;W1&quot;], X) + params[&quot;b1&quot;]
    cache[&quot;A1&quot;] = sigmoid(cache[&quot;Z1&quot;])
    cache[&quot;Z2&quot;] = np.matmul(params[&quot;W2&quot;], cache[&quot;A1&quot;]) + params[&quot;b2&quot;]
    cache[&quot;A2&quot;] = np.exp(cache[&quot;Z2&quot;]) / np.sum(np.exp(cache[&quot;Z2&quot;]), axis=0)

    return cache

def back_propagate(X, Y, params, cache):

    dZ2 = cache[&quot;A2&quot;] - Y
    dW2 = (1./m_batch) * np.matmul(dZ2, cache[&quot;A1&quot;].T)
    db2 = (1./m_batch) * np.sum(dZ2, axis=1, keepdims=True)

    dA1 = np.matmul(params[&quot;W2&quot;].T, dZ2)
    dZ1 = dA1 * sigmoid(cache[&quot;Z1&quot;]) * (1 - sigmoid(cache[&quot;Z1&quot;]))
    dW1 = (1./m_batch) * np.matmul(dZ1, X.T)
    db1 = (1./m_batch) * np.sum(dZ1, axis=1, keepdims=True)

    grads = {&quot;dW1&quot;: dW1, &quot;db1&quot;: db1, &quot;dW2&quot;: dW2, &quot;db2&quot;: db2}

    return grads
</code></pre>

<p>Now for the substantive stuff.</p>

<p>To switch to mini-batch descent, we add another <code>for</code> loop inside the pass through each epoch. At each pass we randomly shuffle the training set, then iterate through it in chunks of <code>batch_size</code>, which we&rsquo;ll arbitrarily set to 128. We&rsquo;ll see the code for all this in a moment.</p>

<p>Next, to add momentum, we keep a moving average of our gradients. So instead of updating our parameters by doing e.g.:</p>

<pre><code class="language-python">params[&quot;W1&quot;] = params[&quot;W1&quot;] - learning_rate * grads[&quot;dW1&quot;]
</code></pre>

<p>we do this:</p>

<pre><code class="language-python">V_dW1 = (beta * V_dW1 + (1. - beta) * grads[&quot;dW1&quot;])
params[&quot;W1&quot;] = params[&quot;W1&quot;] - learning_rate * V_dW1
</code></pre>

<p>Finally, to smarten up our initialization, we shrink the variance of the weights in each layer. Following <a href="https://www.coursera.org/learn/deep-neural-network/lecture/RwqYe/weight-initialization-for-deep-networks" target="_blank">this nice video</a> by Andrew Ng (whose excellent Coursera materials I&rsquo;ve been relying on heavily in these posts), we&rsquo;ll set the variance for each layer to $1/n$, where $n$ is the number of inputs feeding into that layer.</p>

<p>We&rsquo;ve been using the <code>np.random.randn()</code> function to get our initial weights. And this function draws from the standard normal distribution. So to adjust the variance to $1/n$, we just divide by $\sqrt{n}$. In code this means that instead of doing e.g. <code>np.random.randn(n_h, n_x)</code>, we do <code>np.random.randn(n_h, n_x) * np.sqrt(1. / n_x)</code>.</p>

<p>Ok that covers our three improvements. Let&rsquo;s build and train!</p>

<pre><code class="language-python">np.random.seed(138)

# hyperparameters
n_x = X_train.shape[0]
n_h = 64
learning_rate = 4
beta = .9
batch_size = 128
batches = -(-m // batch_size)

# initialization
params = { &quot;W1&quot;: np.random.randn(n_h, n_x) * np.sqrt(1. / n_x),
           &quot;b1&quot;: np.zeros((n_h, 1)) * np.sqrt(1. / n_x),
           &quot;W2&quot;: np.random.randn(digits, n_h) * np.sqrt(1. / n_h),
           &quot;b2&quot;: np.zeros((digits, 1)) * np.sqrt(1. / n_h) }

V_dW1 = np.zeros(params[&quot;W1&quot;].shape)
V_db1 = np.zeros(params[&quot;b1&quot;].shape)
V_dW2 = np.zeros(params[&quot;W2&quot;].shape)
V_db2 = np.zeros(params[&quot;b2&quot;].shape)

# train
for i in range(9):

    permutation = np.random.permutation(X_train.shape[1])
    X_train_shuffled = X_train[:, permutation]
    Y_train_shuffled = Y_train[:, permutation]

    for j in range(batches):

        begin = j * batch_size
        end = min(begin + batch_size, X_train.shape[1] - 1)
        X = X_train_shuffled[:, begin:end]
        Y = Y_train_shuffled[:, begin:end]
        m_batch = end - begin

        cache = feed_forward(X, params)
        grads = back_propagate(X, Y, params, cache)

        V_dW1 = (beta * V_dW1 + (1. - beta) * grads[&quot;dW1&quot;])
        V_db1 = (beta * V_db1 + (1. - beta) * grads[&quot;db1&quot;])
        V_dW2 = (beta * V_dW2 + (1. - beta) * grads[&quot;dW2&quot;])
        V_db2 = (beta * V_db2 + (1. - beta) * grads[&quot;db2&quot;])

        params[&quot;W1&quot;] = params[&quot;W1&quot;] - learning_rate * V_dW1
        params[&quot;b1&quot;] = params[&quot;b1&quot;] - learning_rate * V_db1
        params[&quot;W2&quot;] = params[&quot;W2&quot;] - learning_rate * V_dW2
        params[&quot;b2&quot;] = params[&quot;b2&quot;] - learning_rate * V_db2

    cache = feed_forward(X_train, params)
    train_cost = compute_loss(Y_train, cache[&quot;A2&quot;])
    cache = feed_forward(X_test, params)
    test_cost = compute_loss(Y_test, cache[&quot;A2&quot;])
    print(&quot;Epoch {}: training cost = {}, test cost = {}&quot;.format(i+1 ,train_cost, test_cost))

print(&quot;Done.&quot;)
</code></pre>

<pre><code>Epoch 1: training cost = 0.15587093418167058, test cost = 0.16223940981168986
Epoch 2: training cost = 0.09417519634799829, test cost = 0.11032242938356147
Epoch 3: training cost = 0.07205872840102934, test cost = 0.0958078559246339
Epoch 4: training cost = 0.07008115814138867, test cost = 0.1010270024817398
Epoch 5: training cost = 0.05501929068580713, test cost = 0.09527116695490956
Epoch 6: training cost = 0.042663638371140164, test cost = 0.08268937190759178
Epoch 7: training cost = 0.03615501088752129, test cost = 0.08188431384719108
Epoch 8: training cost = 0.03610956910064329, test cost = 0.08675249924246693
Epoch 9: training cost = 0.027582647825206745, test cost = 0.08023754855128316
Done.
</code></pre>

<p>How&rsquo;d we do?</p>

<pre><code class="language-python">cache = feed_forward(X_test, params)
predictions = np.argmax(cache[&quot;A2&quot;], axis=0)
labels = np.argmax(Y_test, axis=0)

print(classification_report(predictions, labels))
</code></pre>

<pre><code>             precision    recall  f1-score   support

          0       0.99      0.98      0.98       984
          1       0.99      0.99      0.99      1136
          2       0.98      0.98      0.98      1037
          3       0.97      0.98      0.97      1004
          4       0.97      0.98      0.98       970
          5       0.97      0.96      0.97       900
          6       0.97      0.99      0.98       942
          7       0.97      0.97      0.97      1026
          8       0.97      0.96      0.97       982
          9       0.97      0.96      0.97      1019

avg / total       0.98      0.98      0.98     10000
</code></pre>

<p>And there it is: 98% precision in just 9 epochs of training.</p>


          &nbsp;
        </div>

        <div>
          <a href="javascript: history.back()"><i class="fa fa-arrow-left"></i> Back</a>

        </div>
      </div>
    </div>
    

<footer style="padding-top: 1em;">
  <div class="container-fluid">
    <div class="row">
      <div class="col-lg-12 text-center">
        <p>
          Powered by <a href="https://gohugo.io/">Hugo</a>;
          styled with <a href="http://getbootstrap.com/">Bootstrap</a>;
          mostly <a href="https://github.com/lambdafu/hugo-finite">cribbed</a> <a href="https://kieranhealy.org/">from</a> <a href="http://consequently.org/">others</a>.
        </p>
      </div>
    </div>
  </div>
</footer>

<script src="https://code.jquery.com/jquery-3.1.1.min.js" integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8=" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<script src="/js/prism.js"></script>

<script type="text/javascript">
var sc_project= 11213692 ;
var sc_invisible= 1 ;
var sc_security="b1954803";
var scJsHost = (("https:" == document.location.protocol) ?
"https://secure." : "http://www.");
document.write("<sc"+"ript type='text/javascript' src='" +
scJsHost+
"statcounter.com/counter/counter.js'></"+"script>");
</script>
<noscript><div class="statcounter"><a title="web analytics"
href="http://statcounter.com/" target="_blank"><img
class="statcounter"
src="//c.statcounter.com/11213692/0/b1954803/1/" alt="web
analytics"></a></div></noscript>


</body>
</html>

