<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Jonathan Weisberg's Homepage">
    <meta name="author" content="Jonathan Weisberg">
    
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@jweisber">
    <meta name="twitter:creator" content="@jweisber">
    <meta name="twitter:title" content="Building a Neural Network from Scratch: Part 1">
    <meta name="twitter:description" content="In this post we&rsquo;re going to build a neural network from scratch. We&rsquo;ll train it to recognize hand-written digits, using the famous MNIST data set.
We&rsquo;ll use just basic Python with NumPy to build our network (no high-level stuff like Keras or TensorFlow). We will dip into scikit-learn, but only to get the MNIST data and to assess our model once its built.
We&rsquo;ll start with the simplest possible &ldquo;network&rdquo;: a single node that recognizes just the digit 0.">
    <meta name="twitter:image" content="https://i.kinja-img.com/gawker-media/image/upload/s--B5ow_2RG--/c_fit,fl_progressive,q_80,w_636/yccc3f4vcwsxyj6eydy2.jpg">

    <meta property="og:url" content="/post/A%20Neural%20Network%20from%20Scratch%20-%20Part%201/" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="Building a Neural Network from Scratch: Part 1" />
    <meta property="og:description" content="In this post we&rsquo;re going to build a neural network from scratch. We&rsquo;ll train it to recognize hand-written digits, using the famous MNIST data set.
We&rsquo;ll use just basic Python with NumPy to build our network (no high-level stuff like Keras or TensorFlow). We will dip into scikit-learn, but only to get the MNIST data and to assess our model once its built.
We&rsquo;ll start with the simplest possible &ldquo;network&rdquo;: a single node that recognizes just the digit 0." />
    <meta property="og:image" content="https://i.kinja-img.com/gawker-media/image/upload/s--B5ow_2RG--/c_fit,fl_progressive,q_80,w_636/yccc3f4vcwsxyj6eydy2.jpg" />


    <title>Building a Neural Network from Scratch: Part 1</title>

    <link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet">
    <link href="/css/prism.css" rel="stylesheet" />
    <link href="/css/scarab.css" rel="stylesheet">
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        "HTML-CSS": {
          availableFonts: ["Gyre-Pagella"],
          preferredFont: "Gyre-Pagella",
          webFont: "Gyre-Pagella",
          imageFont: "Gyre-Pagella",
        }
      });
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    
    
    

</head>

<body>

<nav class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-navbar-collapse-1">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="http://jonathanweisberg.org/">Jonathan Weisberg</a>
    </div>
    
    <div class="collapse navbar-collapse" id="bs-navbar-collapse-1">
      <ul class="nav navbar-nav">
        <li>
          <a href="/pdf/CV.pdf">
            <i class="fa fa-briefcase"></i>&nbsp;
            CV
          </a>
        </li>
        <li>
          <a href="/post/">
            <i class="fa fa-code"></i>&nbsp;
            Blog
          </a>
        </li>
        <li>
          <a href="/publication/">
            <i class="fa fa-files-o"></i>&nbsp;
            Research
          </a>
        </li>
      </ul>
    </div>
  </div>
</nav>



  
  <div class="container">

    
    <div class="row">
      <div class="col-md-3 hidden-sm hidden-xs text-right">
        <h1 class="post-title">&nbsp;</h1>
        <p>
          <em>Mon, Mar 5, 2018</em><br />
          
          
          
          <span class="post-tags">
            
            <a class="post-tag" href="http://jonathanweisberg.org/tags/machine-learning">Machine Learning</a>
             &bull;   
            
            <a class="post-tag" href="http://jonathanweisberg.org/tags/neural-networks">Neural Networks</a>
             &bull;  
            
            <a class="post-tag" href="http://jonathanweisberg.org/tags/python">Python</a>
            
            
          </span>
          
          
        </p>
      </div>

      <div class="col-md-7 blog-post">

        <h1>Building a Neural Network from Scratch: Part 1</h1>

        <div class="hidden-md hidden-lg">
          <div>
            <em>Mon, Mar 5, 2018</em>
          </div>
          
          
          
          <p class="post-tags">
            
            <a class="post-tag" href="http://jonathanweisberg.org/tags/machine-learning">Machine Learning</a>
             &bull;   
            
            <a class="post-tag" href="http://jonathanweisberg.org/tags/neural-networks">Neural Networks</a>
             &bull;  
            
            <a class="post-tag" href="http://jonathanweisberg.org/tags/python">Python</a>
            
            
          </p>
          
          
        </div>

        <div class="post-body">
          

<p>In this post we&rsquo;re going to build a neural network from scratch. We&rsquo;ll train it to recognize hand-written digits, using the famous MNIST data set.</p>

<p>We&rsquo;ll use just basic Python with NumPy to build our network (no high-level stuff like Keras or TensorFlow). We will dip into scikit-learn, but only to get the MNIST data and to assess our model once its built.</p>

<p>We&rsquo;ll start with the simplest possible &ldquo;network&rdquo;: a single node that recognizes just the digit 0. This is actually just an implementation of logistic regression, which may seem kind of silly. But it&rsquo;ll help us get some key components working before things get more complicated.</p>

<p>Then we&rsquo;ll extend that into a network with one hidden layer, still recognizing just 0. Then we&rsquo;ll add a softmax for recognizing all the digits 0 through 9. That&rsquo;ll give us a 92% accurate digit-recognizer, bringing us up to the cutting edge of 1985 technology.</p>

<p>In a followup post we&rsquo;ll bring that up into the high nineties by making sundry improvements: better optimization, more hidden layers, and smarter initialization.</p>

<h1 id="1-hello-mnist">1. Hello, MNIST</h1>

<p><a href="https://en.wikipedia.org/wiki/MNIST_database" target="_blank">MNIST</a> contains 70,000 images of hand-written digits, each 28 x 28 pixels, in greyscale with pixel-values from 0 to 255. We could <a href="http://yann.lecun.com/exdb/mnist/" target="_blank">download</a> and preprocess the data ourselves. But the makers of scikit-learn already did that for us. Since it would be rude to neglect their efforts, we&rsquo;ll just import it:</p>

<pre><code class="language-python">from sklearn.datasets import fetch_mldata
mnist = fetch_mldata('MNIST original')
X, y = mnist[&quot;data&quot;], mnist[&quot;target&quot;]
</code></pre>

<p>We&rsquo;ll normalize the data to keep our gradients manageable:</p>

<pre><code class="language-python">X = X / 255
</code></pre>

<p>The default MNIST labels record <code>7</code> for an image of a seven, <code>4</code> for an image of a four, etc. But we&rsquo;re just building a zero-classifier for now. So we want our labels to say <code>1</code> when we have a zero, and <code>0</code> otherwise (intuitive, I know). So we&rsquo;ll overwrite the labels to make that happen:</p>

<pre><code class="language-python">import numpy as np

y_new = np.zeros(y.shape)
y_new[np.where(y == 0.0)[0]] = 1
y = y_new
</code></pre>

<p>Now we can make our train/test split. The MNIST images are pre-arranged so that the first 60,000 can be used for training, and the last 10,000 for testing. We&rsquo;ll also transform the data into the shape we want, with each example in a column (instead of a row):</p>

<pre><code class="language-python">m = 60000
m_test = X.shape[0] - m

X_train, X_test = X[:m].T, X[m:].T
y_train, y_test = y[:m].reshape(1,m), y[m:].reshape(1,m_test)
</code></pre>

<p>Finally we&rsquo;ll shuffle the training set for good measure:</p>

<pre><code class="language-python">np.random.seed(138)
shuffle_index = np.random.permutation(m)
X_train, y_train = X_train[:,shuffle_index], y_train[:,shuffle_index]
</code></pre>

<p>Let&rsquo;s have a look at a random image and label just to make sure we didn&rsquo;t throw anything out of wack:</p>

<pre><code class="language-python">%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt

i = 3
plt.imshow(X_train[:,i].reshape(28,28), cmap = matplotlib.cm.binary)
plt.axis(&quot;off&quot;)
plt.show()
print(y_train[:,i])
</code></pre>

<p><img src="/img/nn_from_scratch/output_13_0.png" alt="png" /></p>

<pre><code>[1.]
</code></pre>

<p>That&rsquo;s a zero, so we want the label to be <code>1</code>, which it is. Looks good, so let&rsquo;s build our first network.</p>

<h1 id="2-a-single-neuron-aka-logistic-regression">2. A Single Neuron (aka Logistic Regression)</h1>

<p>We want to build a simple, feed-forward network with 784 inputs (=28 x 28), and a single sigmoid unit generating the output.</p>

<h2 id="2-1-forward-propogation">2.1 Forward Propogation</h2>

<p>The forward pass on a single example $x$ executes the following computation:
$$ \hat{y} = \sigma(w^T x + b). $$
Here $\sigma$ is the sigmoid function:
$$ \sigma(z) = \frac{1}{1 + e^{-z}}. $$
So let&rsquo;s define:</p>

<pre><code class="language-python">def sigmoid(z):
    s = 1 / (1 + np.exp(-z))
    return s
</code></pre>

<p>We&rsquo;ll vectorize by stacking examples side-by-side, so that our input matrix $X$ has an example in each column. The vectorized form of the forward pass is then:
$$ \hat{y} = \sigma(w^T X + b). $$
Note that $\hat{y}$ is now a vector, not a scalar as it was in the previous equation.</p>

<p>In our code we&rsquo;ll compute this in two stages: <code>Z = np.matmul(W.T, X) + b</code> and then <code>A = sigmoid(Z)</code>. (<code>A</code> for Activation.) Breaking things up into stages like this is just for tidinessâ€”it&rsquo;ll make our forward propagation computations mirror the steps in our backward propagation computations.</p>

<h2 id="2-2-cost-function">2.2 Cost Function</h2>

<p>We&rsquo;ll use cross-entropy for our cost function. The formula for a single training example is:
$$ L(y, \hat{y}) = -y \log(\hat{y}) - (1-y) \log(1-\hat{y}). $$
Averaging over a training set of $m$ examples we then have:
$$ L(Y, \hat{Y}) = -\frac{1}{m} \sum_{i=1}^m \left( y^{(i)} \log(\hat{y}^{(i)}) + (1-y^{(i)}) \log(1-\hat{y}^{(i)}) \right). $$
So let&rsquo;s define:</p>

<pre><code class="language-python">def compute_loss(Y, Y_hat):

    m = Y.shape[1]
    L = -(1./m) * ( np.sum( np.multiply(np.log(Y_hat),Y) ) + np.sum( np.multiply(np.log(1-Y_hat),(1-Y)) ) )

    return L
</code></pre>

<h2 id="2-3-backward-propagation">2.3 Backward Propagation</h2>

<p>For backpropagation, we&rsquo;ll need to know how $L$ changes with respect to each component $w_j$ of $w$. That is, we must compute each $\partial L / \partial w_j$.</p>

<p>Focusing on a single example will make it easier to derive the formulas we need. Holding all values except $w_j$ fixed, we can think of $L$ as being computed in three steps: $w_j \rightarrow z \rightarrow \hat{y} \rightarrow L$. The formulas for these steps are:
$$
  \begin{align}
  z &amp;= w^T x + b,\newline
  \hat{y} &amp;= \sigma(z),\newline
  L(y, \hat{y}) &amp;= -y \log(\hat{y}) - (1-y) \log(1-\hat{y}).
  \end{align}
$$
And the chain rule tells us:
$$
  \frac{\partial L}{\partial w_j} = \frac{\partial L}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial z} \frac{\partial z}{\partial w_j}.
$$
Looking at $\partial L / \partial \hat{y}$ first:
$$
  \begin{align}
    \frac{\partial L}{\partial \hat{y}} &amp;= \frac{\partial}{\partial \hat{y}} \left( -y \log(\hat{y}) - (1-y) \log(1-\hat{y}) \right)\newline
      &amp;= -y \frac{\partial}{\partial \hat{y}} \log(\hat{y}) - (1-y) \frac{\partial}{\partial \hat{y}} \log(1-\hat{y})\newline
      &amp;= \frac{-y}{\hat{y}} + \frac{(1-y) }{1 - \hat{y}}\newline
      &amp;= \frac{\hat{y} - y}{\hat{y}(1 - \hat{y})}.
  \end{align}
$$
Next we want $\partial \hat{y} / \partial z$:
$$
  \begin{align}
    \frac{\partial}{\partial z} \sigma(z) &amp;= \frac{\partial}{\partial z} \left( \frac{1}{1 + e^{-z}} \right)\newline
      &amp;= - \frac{1}{(1 + e^{-z})^2} \frac{\partial}{\partial z} \left( 1 + e^{-z} \right)\newline
      &amp;= \frac{e^{-z}}{(1 + e^{-z})^2}\newline
      &amp;= \frac{1}{1 + e^{-z}} \frac{e^{-z}}{1 + e^{-z}}\newline
      &amp;= \sigma(z) \frac{e^{-z}}{1 + e^{-z}}\newline
      &amp;= \sigma(z) \left( 1 - \frac{1}{1 + e^{-z}} \right)\newline
      &amp;= \sigma(z) \left( 1 - \sigma(z) \right)\newline
      &amp;= \hat{y} (1-\hat{y}).
  \end{align}
$$
Lastly we tackle $\partial z / \partial w_j$:
$$
  \begin{align}
    \frac{\partial}{\partial w_j} (w^T x + b) &amp;= \frac{\partial}{\partial w_j} (w_0 x_0 + \ldots + w_n x_n + b)\newline
      &amp;= w_j.
  \end{align}
$$
Finally we can substitute into the chain rule to find:
$$
  \begin{align}
    \frac{\partial L}{\partial w_j} &amp;= \frac{\partial L}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial z} \frac{\partial z}{\partial w_j}\newline
    &amp;= \frac{\hat{y} - y}{\hat{y}(1 - \hat{y})} \hat{y} (1-\hat{y}) w_j\newline
    &amp;= (\hat{y} - y) w_j.\newline
  \end{align}
$$
In vectorized form with $m$ training examples this gives us:
$$
  \frac{\partial L}{\partial w} = \frac{1}{m} X (\hat{y} - y)^T.
$$
What about $\partial L / \partial b$? A very similar derivation yields, for a single example:
$$
  \begin{align}
    \frac{\partial L}{\partial b} &amp;= (\hat{y} - y).
  \end{align}
$$
Which in vectorized form amounts to:
$$
  \frac{\partial L}{\partial b} = \frac{1}{m} \sum_{i=1}^m (\hat{y}^{(i)} - y^{(i)}).
$$
In our code we&rsquo;ll label these gradients according to their denominators, as <code>dW</code> and <code>db</code>. So for backpropagation we&rsquo;ll compute <code>dW = (1/m) * np.matmul(X, (A-Y).T)</code> and <code>db = (1/m) * np.sum(A-Y, axis=1, keepdims=True)</code>.</p>

<h2 id="2-4-build-train">2.4 Build &amp; Train</h2>

<p>Ok we&rsquo;re ready to build and train our network!</p>

<pre><code class="language-python">learning_rate = 1

X = X_train
Y = y_train

n_x = X.shape[0]
m = X.shape[1]

W = np.random.randn(n_x, 1) * 0.01
b = np.zeros((1, 1))

for i in range(2000):
    Z = np.matmul(W.T, X) + b
    A = sigmoid(Z)

    cost = compute_loss(Y, A)

    dW = (1/m) * np.matmul(X, (A-Y).T)
    db = (1/m) * np.sum(A-Y, axis=1, keepdims=True)

    W = W - learning_rate * dW
    b = b - learning_rate * db

    if (i % 100 == 0):
        print(&quot;Epoch&quot;, i, &quot;cost: &quot;, cost)

print(&quot;Final cost:&quot;, cost)
</code></pre>

<pre><code>Epoch 0 cost:  0.6840801595436431
Epoch 100 cost:  0.041305162058342754
... *snip* ...
Final cost: 0.02514156608481825
</code></pre>

<p>We could probably eek out a bit more accuracy with some more training. But the gains have slowed considerably. So let&rsquo;s just see how we did, by looking at the confusion matrix:</p>

<pre><code class="language-python">from sklearn.metrics import classification_report, confusion_matrix

Z = np.matmul(W.T, X_test) + b
A = sigmoid(Z)

predictions = (A&gt;.5)[0,:]
labels = (y_test == 1)[0,:]

print(confusion_matrix(predictions, labels))
</code></pre>

<pre><code>[[8980   33]
 [  40  947]]
</code></pre>

<p>Hey, that&rsquo;s actually pretty good! We got 947 of the zeros and missed only 33, while getting nearly all the negative cases right. In terms of f1-score that&rsquo;s 0.99:</p>

<pre><code class="language-python">print(classification_report(predictions, labels))
</code></pre>

<pre><code>             precision    recall  f1-score   support

      False       1.00      1.00      1.00      9013
       True       0.97      0.96      0.96       987

avg / total       0.99      0.99      0.99     10000
</code></pre>

<p>So, now that we&rsquo;ve got a working model and optimization algorithm, let&rsquo;s enrich it.</p>

<h1 id="3-one-hidden-layer">3. One Hidden Layer</h1>

<p>Let&rsquo;s add a hidden layer now, with 64 units (a mostly arbitrary choice). I won&rsquo;t go through the derivations of all the formulas for the forward and backward passes this time; they&rsquo;re a pretty direct extension of the work we did earlier. Instead let&rsquo;s just dive right in and build the model:</p>

<pre><code class="language-python">X = X_train
Y = y_train

n_x = X.shape[0]
n_h = 64
learning_rate = 1

W1 = np.random.randn(n_h, n_x)
b1 = np.zeros((n_h, 1))
W2 = np.random.randn(1, n_h)
b2 = np.zeros((1, 1))

for i in range(2000):

    Z1 = np.matmul(W1, X) + b1
    A1 = sigmoid(Z1)
    Z2 = np.matmul(W2, A1) + b2
    A2 = sigmoid(Z2)

    cost = compute_loss(Y, A2)

    dZ2 = A2-Y
    dW2 = (1./m) * np.matmul(dZ2, A1.T)
    db2 = (1./m) * np.sum(dZ2, axis=1, keepdims=True)

    dA1 = np.matmul(W2.T, dZ2)
    dZ1 = dA1 * sigmoid(Z1) * (1 - sigmoid(Z1))
    dW1 = (1./m) * np.matmul(dZ1, X.T)
    db1 = (1./m) * np.sum(dZ1, axis=1, keepdims=True)

    W2 = W2 - learning_rate * dW2
    b2 = b2 - learning_rate * db2
    W1 = W1 - learning_rate * dW1
    b1 = b1 - learning_rate * db1

    if i % 100 == 0:
        print(&quot;Epoch&quot;, i, &quot;cost: &quot;, cost)

print(&quot;Final cost:&quot;, cost)
</code></pre>

<pre><code>Epoch 0 cost:  0.9144384083567224
Epoch 100 cost:  0.08856953026938433
... *snip* ...
Final cost: 0.024249298861903648
</code></pre>

<p>How&rsquo;d we do?</p>

<pre><code class="language-python">Z1 = np.matmul(W1, X_test) + b1
A1 = sigmoid(Z1)
Z2 = np.matmul(W2, A1) + b2
A2 = sigmoid(Z2)

predictions = (A2&gt;.5)[0,:]
labels = (y_test == 1)[0,:]

print(confusion_matrix(predictions, labels))
print(classification_report(predictions, labels))
</code></pre>

<pre><code>[[8984   36]
 [  36  944]]
             precision    recall  f1-score   support

      False       1.00      1.00      1.00      9020
       True       0.96      0.96      0.96       980

avg / total       0.99      0.99      0.99     10000
</code></pre>

<p>Hmm, not bad, but about the same as our one-neuron model did. We could do more training and add more nodes/layers. But it&rsquo;ll be slow going until we improve our optimization algorithm, which we&rsquo;ll do in a followup post.</p>

<p>So for now let&rsquo;s turn to recognizing all ten digits.</p>

<h1 id="4-upgrading-to-multiclass">4. Upgrading to Multiclass</h1>

<p><img src="https://medias.spotern.com/spots/w1280/3477.jpg" alt="" /></p>

<h2 id="4-1-labels">4.1 Labels</h2>

<p>First we need to redo our labels. We&rsquo;ll re-import everything, so that we don&rsquo;t have to go back and coordinate with our earlier shuffling:</p>

<pre><code class="language-python">mnist = fetch_mldata('MNIST original')
X, y = mnist[&quot;data&quot;], mnist[&quot;target&quot;]

X = X / 255
</code></pre>

<p>Then we&rsquo;ll one-hot encode MNIST&rsquo;s labels, to get a 10 x 70,000 array.</p>

<pre><code class="language-python">digits = 10
examples = y.shape[0]

y = y.reshape(1, examples)

Y_new = np.eye(digits)[y.astype('int32')]
Y_new = Y_new.T.reshape(digits, examples)
</code></pre>

<p>Then we re-split, re-shape, and re-shuffle our training set:</p>

<pre><code class="language-python">m = 60000
m_test = X.shape[0] - m

X_train, X_test = X[:m].T, X[m:].T
Y_train, Y_test = Y_new[:,:m], Y_new[:,m:]

shuffle_index = np.random.permutation(m)
X_train, Y_train = X_train[:, shuffle_index], Y_train[:, shuffle_index]
</code></pre>

<p>A quick check that things are as they should be:</p>

<pre><code class="language-python">i = 12
plt.imshow(X_train[:,i].reshape(28,28), cmap = matplotlib.cm.binary)
plt.axis(&quot;off&quot;)
plt.show()
Y_train[:,i]
</code></pre>

<p><img src="/img/nn_from_scratch/output_43_0.png" alt="png" /></p>

<pre><code>array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])
</code></pre>

<p>Looks good, so let&rsquo;s consider what changes we need to make to the model itself.</p>

<h2 id="4-2-forward-propagation">4.2 Forward Propagation</h2>

<p>Only the last layer of our network is changing. To add the softmax, we have to replace our lone, final node with a 10-unit layer. Its final activations are the exponentials of its $z$-values, normalized across all ten such exponentials. So instead of just computing $\sigma(z)$, we compute the activation for each unit $i$:
$$ \frac{e^{z_i}}{\sum_{j=0}^{9} e^{z_j}}.$$
So, in our vectorized code, the last line of forward propagation will be <code>A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0)</code>.</p>

<h2 id="4-3-cost-function">4.3 Cost Function</h2>

<p>Our cost function now has to generalize to more than two classes. The general formula for $n$ classes is:
$$ L(y, \hat{y}) = -\sum_{i = 0}^n y_i \log(\hat{y}_i). $$
Averaging over $m$ training examples this becomes:
$$ L(Y, \hat{Y}) = - \frac{1}{m} \sum_{j = 0}^m \sum_{i = 0}^n y_i^{(j)} \log(\hat{y}_i^{(j)}). $$
So let&rsquo;s define:</p>

<pre><code class="language-python">def compute_multiclass_loss(Y, Y_hat):

    L_sum = np.sum(np.multiply(Y, np.log(Y_hat)))
    m = Y.shape[1]
    L = -(1/m) * L_sum

    return L
</code></pre>

<h2 id="4-4-backprop">4.4 Backprop</h2>

<p>Luckily it turns out that backprop isn&rsquo;t really affected by the switch to a softmax. A softmax generalizes the sigmoid activiation we&rsquo;ve been using, and in such a way that the code we wrote earlier still works. We could verify this by deriving:
$$\frac{\partial L}{\partial z_i} = \hat{y}_i - y_i.$$
But I won&rsquo;t walk through the steps here. Let&rsquo;s just go ahead and build our final network.</p>

<h2 id="4-5-build-train">4.5 Build &amp; Train</h2>

<pre><code class="language-python">n_x = X_train.shape[0]
n_h = 64
learning_rate = 1

W1 = np.random.randn(n_h, n_x)
b1 = np.zeros((n_h, 1))
W2 = np.random.randn(digits, n_h)
b2 = np.zeros((digits, 1))

X = X_train
Y = Y_train

for i in range(2000):

    Z1 = np.matmul(W1,X) + b1
    A1 = sigmoid(Z1)
    Z2 = np.matmul(W2,A1) + b2
    A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0)

    cost = compute_multiclass_loss(Y, A2)

    dZ2 = A2-Y
    dW2 = (1./m) * np.matmul(dZ2, A1.T)
    db2 = (1./m) * np.sum(dZ2, axis=1, keepdims=True)

    dA1 = np.matmul(W2.T, dZ2)
    dZ1 = dA1 * sigmoid(Z1) * (1 - sigmoid(Z1))
    dW1 = (1./m) * np.matmul(dZ1, X.T)
    db1 = (1./m) * np.sum(dZ1, axis=1, keepdims=True)

    W2 = W2 - learning_rate * dW2
    b2 = b2 - learning_rate * db2
    W1 = W1 - learning_rate * dW1
    b1 = b1 - learning_rate * db1

    if (i % 100 == 0):
        print(&quot;Epoch&quot;, i, &quot;cost: &quot;, cost)

print(&quot;Final cost:&quot;, cost)
</code></pre>

<pre><code>Epoch 0 cost:  9.243960401572568
... *snip* ...
Epoch 1900 cost:  0.24585173887243117
Final cost: 0.24072776877870128
</code></pre>

<p>Let&rsquo;s see how we did:</p>

<pre><code class="language-python">Z1 = np.matmul(W1, X_test) + b1
A1 = sigmoid(Z1)
Z2 = np.matmul(W2, A1) + b2
A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0)

predictions = np.argmax(A2, axis=0)
labels = np.argmax(Y_test, axis=0)

print(confusion_matrix(predictions, labels))
print(classification_report(predictions, labels))
</code></pre>

<pre><code>[[ 946    0   14    3    3   10   12    2    9    4]
 [   0 1112    3    2    1    1    2    8    3    4]
 [   3    4  937   24   10    7    8   18    8    3]
 [   4    2   17  924    1   39    4   13   26    9]
 [   0    1   10    0  905    9   11    9   10   40]
 [  12    5    2   26    3  786   15    3   24   14]
 [   8    1   19    2    9   10  902    1    9    1]
 [   2    1   13   14    3    5    1  946    9   25]
 [   5    9   16   11    5   18    3    5  868    9]
 [   0    0    1    4   42    7    0   23    8  900]]
             precision    recall  f1-score   support

          0       0.97      0.94      0.95      1003
          1       0.98      0.98      0.98      1136
          2       0.91      0.92      0.91      1022
          3       0.91      0.89      0.90      1039
          4       0.92      0.91      0.92       995
          5       0.88      0.88      0.88       890
          6       0.94      0.94      0.94       962
          7       0.92      0.93      0.92      1019
          8       0.89      0.91      0.90       949
          9       0.89      0.91      0.90       985

avg / total       0.92      0.92      0.92     10000
</code></pre>

<p>We&rsquo;re at 92% accuracy across all digits, not bad! And it looks like we could still improve with more training.</p>

<p>But let&rsquo;s work on speeding up our optimization alogirthm first. We&rsquo;ll pick things up there in the next post.</p>


          &nbsp;
        </div>

        <div>
          <a href="javascript: history.back()"><i class="fa fa-arrow-left"></i> Back</a>

        </div>
      </div>
    </div>
    

<footer style="padding-top: 1em;">
  <div class="container-fluid">
    <div class="row">
      <div class="col-lg-12 text-center">
        <p>
          Powered by <a href="https://gohugo.io/">Hugo</a>;
          styled with <a href="http://getbootstrap.com/">Bootstrap</a>;
          mostly <a href="https://github.com/lambdafu/hugo-finite">cribbed</a> <a href="https://kieranhealy.org/">from</a> <a href="http://consequently.org/">others</a>.
        </p>
      </div>
    </div>
  </div>
</footer>

<script src="https://code.jquery.com/jquery-3.1.1.min.js" integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8=" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<script src="/js/prism.js"></script>

<script type="text/javascript">
var sc_project= 11213692 ;
var sc_invisible= 1 ;
var sc_security="b1954803";
var scJsHost = (("https:" == document.location.protocol) ?
"https://secure." : "http://www.");
document.write("<sc"+"ript type='text/javascript' src='" +
scJsHost+
"statcounter.com/counter/counter.js'></"+"script>");
</script>
<noscript><div class="statcounter"><a title="web analytics"
href="http://statcounter.com/" target="_blank"><img
class="statcounter"
src="//c.statcounter.com/11213692/0/b1954803/1/" alt="web
analytics"></a></div></noscript>


</body>
</html>

