<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Social Epistemology on Jonathan Weisberg</title>
    <link>http://jonathanweisberg.org/tags/social-epistemology/index.xml</link>
    <description>Recent content in Social Epistemology on Jonathan Weisberg</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="http://jonathanweisberg.org/tags/social-epistemology/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>How Scientific is Scientific Polarization?</title>
      <link>http://jonathanweisberg.org/post/ow-commutativity/</link>
      <pubDate>Tue, 10 Nov 2020 00:00:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/ow-commutativity/</guid>
      <description>

&lt;p&gt;As Joe Biden cleared 270 last week, some people remarked on how different the narrative would&amp;rsquo;ve been had the votes been counted in a different order:&lt;/p&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;It&amp;#39;s staggering to think about how differently PA would be viewed/covered right now if the EDay/mail ballots were being counted in the opposite order.&lt;/p&gt;&amp;mdash; Dave Wasserman (@Redistrict) &lt;a href=&#34;https://twitter.com/Redistrict/status/1324456769817640961?ref_src=twsrc%5Etfw&#34;&gt;November 5, 2020&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The idea that order shouldn&amp;rsquo;t affect your final take is a classic criterion of rationality. Whatever order the evidence comes in, your final opinion should be the same it&amp;rsquo;s the same total evidence in the end.&lt;/p&gt;

&lt;p&gt;This post is about how &lt;a href=&#34;https://doi.org/10.1007/s13194-018-0213-9&#34; target=&#34;_blank&#34;&gt;O&amp;rsquo;Connor &amp;amp; Weatherall&amp;rsquo;s model&lt;/a&gt; of &amp;ldquo;scientific polarization&amp;rdquo; runs afoul of this constraint. In their model, divergent opinions arise from a shared body of evidence, despite everyone involved being rational. Just how rational is what we&amp;rsquo;re considering here.&lt;/p&gt;

&lt;h1 id=&#34;the-model&#34;&gt;The Model&lt;/h1&gt;

&lt;p&gt;Here&amp;rsquo;s the quick version of O&amp;rsquo;Connor &amp;amp; Weatherall&amp;rsquo;s model. You can find the gory details in my &lt;a href=&#34;http://jonathanweisberg.org/post/ow&#34;&gt;previous post&lt;/a&gt;, but we won&amp;rsquo;t need them here.&lt;/p&gt;

&lt;p&gt;A community of medical doctors is faced with a novel treatment for some condition. Currently, patients with this condition have a .5 chance of recovering. The new treatment either increases that chance or decreases it. In actual fact it increases the chance of recovery, but our doctors don&amp;rsquo;t know that yet.&lt;/p&gt;

&lt;p&gt;Some doctors start out more skeptical of the new treatment, others more optimistic. Those with credence &amp;gt; .5 try the new treatment on their patients, and share the results with the others. Everybody then updates their credence in the new treatment. The cycle of experimentation, sharing, and updating then repeats.&lt;/p&gt;

&lt;p&gt;Crucially though, our doctors don&amp;rsquo;t fully trust one another&amp;rsquo;s results. If a doctor has a very different opinion about the new treatment than her colleague, she won&amp;rsquo;t fully trust that colleague&amp;rsquo;s data. She may even discount them entirely, if their credences differ enough.&lt;/p&gt;

&lt;p&gt;As things develop, this medical community is apt to split. Some doctors learn the truth about the new treatment&amp;rsquo;s superiority, while others remain skeptical and even come to completely disregard the results reported by their colleagues. This won&amp;rsquo;t always happen, but it&amp;rsquo;s the likely outcome given certain assumptions. Crucial for us here: doctors must discount one other&amp;rsquo;s data entirely when their credences differ significantly&amp;mdash;by .5 let&amp;rsquo;s say, just for concreteness.&lt;/p&gt;

&lt;h1 id=&#34;the-problem&#34;&gt;The Problem&lt;/h1&gt;

&lt;p&gt;This way of evaluating evidence depends on the order. Here&amp;rsquo;s an extreme example to make the point vivid.&lt;/p&gt;

&lt;p&gt;Suppose Dr. Hibbert has credence .501 in the new treatment&amp;rsquo;s benefits, and his colleagues Nick and Zoidberg are both at 1.0. Nick and Zoidberg each have a report to share with Hibbert, containing bad news about the new treatment. Nick found that it failed in all but 1 of his 10 patients, while Zoidberg found that it failed in all 10 of his. Whose report should Dr. Hibbert update on first?&lt;/p&gt;

&lt;p&gt;If he listens to Nick first, he&amp;rsquo;ll fall below .5 and ignore Zoidberg&amp;rsquo;s report as a result. His difference of opinion with Zoidberg will be so large that Hibbert will come to discount him entirely. But if he listens to Zoidberg first, he&amp;rsquo;ll ignore Nick then, for the same reason.&lt;/p&gt;

&lt;p&gt;So Hibbert can only really listen to one of them. And since their reports are different, he&amp;rsquo;ll end up with different credences depending on who he listens to. Zoidberg&amp;rsquo;s report is slightly more discouraging. So Hibbert will end up more skeptical of the new treatment if he listens to Zoidberg first, than if he listens to Nick first.&lt;/p&gt;

&lt;h1 id=&#34;can-it-be-fixed&#34;&gt;Can It Be Fixed?&lt;/h1&gt;

&lt;p&gt;This problem isn&amp;rsquo;t an artifact of the particulars of O&amp;rsquo;Connor &amp;amp; Weatherall&amp;rsquo;s model. It&amp;rsquo;s in the nature of the project. Any polarization model of the same, broad kind must have the same bug.&lt;/p&gt;

&lt;p&gt;Polarization happens because skeptical agents come to ignore their optimistic colleagues at some point. Otherwise, skeptics would eventually be drawn to the truth. As long as they&amp;rsquo;re still willing to give some credence to the experimental results, they&amp;rsquo;ll eventually see that those results favour optimism about the new treatment.&lt;/p&gt;

&lt;p&gt;But even if our agents never ignored one another completely, we&amp;rsquo;d still have this problem. Suppose all three of our characters have the same credence. And Nick has one success to report where Zoidberg has one failure. Intuitively, once Hibbert hears them both out, he should end up right back where he started, no matter who he listens to first.&lt;/p&gt;

&lt;p&gt;But if he listens to Nick first, his credence will move away from Zoidberg&amp;rsquo;s. So when he gets to Zoidberg&amp;rsquo;s report it&amp;rsquo;ll carry less weight than Nick&amp;rsquo;s did. He&amp;rsquo;ll end up more confident than he started. Whereas he&amp;rsquo;ll end up less confident if he proceeds in reverse order.&lt;/p&gt;

&lt;h1 id=&#34;does-it-matter&#34;&gt;Does It Matter?&lt;/h1&gt;

&lt;p&gt;It seems like polarization can&amp;rsquo;t be fully scientific if it&amp;rsquo;s driven by mistrust based on difference of opinion. But that doesn&amp;rsquo;t make the model worthless, or even uninteresting. O&amp;rsquo;Connor &amp;amp; Weatherall are already clear that their agents aren&amp;rsquo;t meant to be &amp;ldquo;rational with a capital &amp;lsquo;R&amp;rsquo;&amp;rdquo; anyway.&lt;/p&gt;

&lt;p&gt;Quite plausibly, real people behave something like the agents in this model a lot of the time. The model might be capturing a very real phenomenon, even if it&amp;rsquo;s an irrational one. We just have to take the &amp;ldquo;scientific&amp;rdquo; in &amp;ldquo;scientific polarization&amp;rdquo; with the right amount of salt.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mistrust &amp; Polarization</title>
      <link>http://jonathanweisberg.org/post/ow/</link>
      <pubDate>Mon, 09 Nov 2020 00:00:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/ow/</guid>
      <description>

&lt;p&gt;This is post 3 of 3 on simulated epistemic networks (code &lt;a href=&#34;https://github.com/jweisber/sep-sen&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;):&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://jonathanweisberg.org/post/zollman/&#34;&gt;The Zollman Effect&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://jonathanweisberg.org/post/rbo&#34;&gt;How Robust is the Zollman Effect?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://jonathanweisberg.org/post/ow&#34;&gt;Mistrust &amp;amp; Polarization&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The first post introduced a simple model of collective inquiry. Agents experiment with a new treatment and share their data, then update on all data as if it were their own. But what if they mistrust one another?&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s natural to have less than full faith in those whose opinions differ from your own. They seem to have gone astray somewhere, after all. And even if not, their views may have illicitly influenced their research.&lt;/p&gt;

&lt;p&gt;So maybe our agents won&amp;rsquo;t take the data shared by others at face value. Maybe they&amp;rsquo;ll discount it, especially when the source&amp;rsquo;s viewpoint differs greatly from their own. &lt;a href=&#34;https://doi.org/10.1007/s13194-018-0213-9&#34; target=&#34;_blank&#34;&gt;O&amp;rsquo;Connor &amp;amp; Weatherall&lt;/a&gt; (O&amp;amp;W) explore this possibility, and find that it can lead to polarization.&lt;/p&gt;

&lt;h1 id=&#34;polarization&#34;&gt;Polarization&lt;/h1&gt;

&lt;p&gt;Until now, our communities always reached a consensus. Now though, some agents in the community may conclude the novel treatment is superior, while others abandon it, and even ignore the results of their peers using the new treatment.&lt;/p&gt;

&lt;p&gt;In the example animated below, agents in blue have credence &amp;gt;.5 so they experiment with the new treatment, sharing the results with everyone. Agents in green have credence ≤.5 but are still persuadable. They still trust the blue agents enough to update on their results&amp;mdash;though they discount these results more the greater their difference of opinion with the agent who generated them. Finally, red agents ignore results entirely. They&amp;rsquo;re so far from all the blue agents that they don&amp;rsquo;t trust them at all.&lt;/p&gt;

&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;video width=&#34;500&#34; height=&#34;300&#34; controls&gt;
    &lt;source src=&#34;http://jonathanweisberg.org/img/sep-sen/ow-animate.mp4&#34; type=&#34;video/mp4&#34;&gt;
  &lt;/video&gt;
  &lt;figcaption style=&#34;font-style:italic; font-size: .8em; text-align: center; padding-bottom: .75em;&#34;&gt;
      Fig. 1. Example of polarization in the O&#39;Connor–Weatherall model
  &lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;In this simulation, we reach a point where there are no more green agents, only unpersuadable skeptics in red and highly confident believers in blue. And the blues have become so confident, they&amp;rsquo;re unlikely to ever move close enough to any of the reds to get their ear. So we&amp;rsquo;ve reached a stable state of polarization.&lt;/p&gt;

&lt;p&gt;How often does such polarization occur? It depends on the size of the community, and on the &amp;ldquo;rate of mistrust,&amp;rdquo; $m$. Details on this parameter are below, but it&amp;rsquo;s basically the rate at which difference of opinion increases discounting. The larger $m$ is, the more a given difference in our opinions will cause you to discount data I share with you.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s how these two factors affect the probability of polarization. (Note: we&amp;rsquo;re considering only complete networks here.)&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://jonathanweisberg.org/img/sep-sen/ow-2.png&#34;  /&gt;
  &lt;figcaption style=&#34;font-style:italic; font-size: .8em; text-align: center; padding-bottom: .75em;&#34;&gt;
      Fig. 2. Probability of polarization depends on community size and rate of mistrust.
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;So the more agents are inclined to mistrust one another, the more likely they are to end up polarized. No surprise there. But larger communities are also more disposed to polarize. Why?&lt;/p&gt;

&lt;p&gt;As O&amp;amp;W explain, the more agents there are, the more likely it is that strong skeptics will be present at the start of inquiry: agents with credence well below .5. These agents will tend to ignore the reports of the optimists experimenting with the new treatment. So they anchor a skeptical segment of the population.&lt;/p&gt;

&lt;p&gt;The mistrust multiplier $m$ is essential for polarization to happen in this model. There&amp;rsquo;s no polarization unless $m &amp;gt; 1$. So let&amp;rsquo;s see the details of how $m$ works.&lt;/p&gt;

&lt;h1 id=&#34;jeffrey-updating&#34;&gt;Jeffrey Updating&lt;/h1&gt;

&lt;p&gt;The more our agents differ in their beliefs, the less they&amp;rsquo;ll trust each other. When Dr. Nick reports evidence $E$ to Dr. Hibbert, Hibbert won&amp;rsquo;t simply &lt;a href=&#34;https://plato.stanford.edu/entries/epistemology-bayesian/#SimPriCon&#34; target=&#34;_blank&#34;&gt;conditionalize&lt;/a&gt; on $E$ to get his new credence $P&amp;rsquo;(H) = P(H \mathbin{\mid} E)$. Instead he&amp;rsquo;ll take a weighted average of $P(H \mathbin{\mid} E)$ and $P(H \mathbin{\mid} \neg E)$. In other words, he&amp;rsquo;ll use &lt;a href=&#34;https://plato.stanford.edu/entries/epistemology-bayesian/#ObjSimPriConRulInfOthObjBayConThe&#34; target=&#34;_blank&#34;&gt;Jeffrey conditionalization&lt;/a&gt;:
$$ P&amp;rsquo;(H) = P(H \mathbin{\mid} E) P&amp;rsquo;(E) + P(H \mathbin{\mid} \neg E) P&amp;rsquo;(\neg E). $$
But to apply this formula we need to know the value for $P&amp;rsquo;(E)$. We need to know how believable Hibbert finds $E$ when Nick reports it.&lt;/p&gt;

&lt;p&gt;O&amp;amp;W note two factors that should affect $P&amp;rsquo;(E)$.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;The more Nick&amp;rsquo;s opinion differs from Hibbert&amp;rsquo;s, the less Hibbert will trust him. So we want $P&amp;rsquo;(E)$ to decrease with the absolute difference between Hibbert&amp;rsquo;s credence in $H$ and Nick&amp;rsquo;s. Call this absolute difference $d$.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;We also want $P&amp;rsquo;(E)$ to decrease with $P(\neg E)$. Nick&amp;rsquo;s report of $E$ has to work against Hibbert&amp;rsquo;s skepticism about $E$ to make $P&amp;rsquo;(E)$ high.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A natural proposal then is that $P&amp;rsquo;(E)$ should decrease with the product $d \cdot P(\neg E)$, which suggests $1 - d \cdot P(\neg E)$ as our formula. When $d = 1$ this would mean Hibbert ignores Nick&amp;rsquo;s report: $P&amp;rsquo;(E) = 1 - P(\neg E) = P(E)$. And when they are simpatico, $d = 0$, Hibbert will trust Nick fully and just conditionalizes on his report, since then $P&amp;rsquo;(E) = 1$.&lt;/p&gt;

&lt;p&gt;This is fine from a formal point of view, but it means that Hibbert will basically never ignore Nick&amp;rsquo;s testimony completely. There is zero chance of $d = 1$ ever happening in our models.&lt;/p&gt;

&lt;p&gt;So, to explore models where agents fully discount one another&amp;rsquo;s testimony, we introduce the mistrust multiplier, $m \geq 0$. This makes our final formula:
$$P&amp;rsquo;(E) = 1 - \min(1, d \cdot m) \cdot P(\neg E).$$
The $\min$ is there to prevent negative values. When $d \cdot m &amp;gt; 1$, we just replace it with $1$ so that $P&amp;rsquo;(E) = P(E)$. Here&amp;rsquo;s what this function looks like for one example, where $m = 1.5$ and $P(E) = .6$:&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://jonathanweisberg.org/img/sep-sen/ow-1.png&#34; /&gt;
  &lt;figcaption style=&#34;font-style:italic; font-size: .8em; text-align: center; padding-bottom: .75em;&#34;&gt;
      Fig. 2. Posterior of the evidence $P&#39;(E)$ when $m = 1.5$ and $P(E) = .6$
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Note the kink, the point after which agents just ignore one another&amp;rsquo;s data.&lt;/p&gt;

&lt;p&gt;O&amp;amp;W also consider models where the line doesn&amp;rsquo;t flatten, but keeps going down. In that case agents don&amp;rsquo;t ignore one another, but rather &amp;ldquo;anti-update.&amp;rdquo; They take a report of $E$ as a reason to &lt;em&gt;decrease&lt;/em&gt; their credence in $E$. This too results in polarization, more frequently and with greater severity, in fact.&lt;/p&gt;

&lt;h1 id=&#34;discussion&#34;&gt;Discussion&lt;/h1&gt;

&lt;p&gt;Polarization only happens when $m &amp;gt; 1$. Only then do some agents mistrust their colleagues enough to fully discount their reports. If this never happened, they would eventually be drawn to the truth (however slowly) by the data coming from their more optimistic colleagues.&lt;/p&gt;

&lt;p&gt;So is $m &amp;gt; 1$ a plausible assumption? I think it can be. People can be so unreliable that their reports aren&amp;rsquo;t believable at all. In some cases a report can even decrease the believability of the proposition reported. Some sources are known for their fabrications.&lt;/p&gt;

&lt;p&gt;Ultimately it comes down to whether $P(E \,\vert\, R_E) &amp;gt; P(E)$, i.e. whether someone reporting $E$ increases the probability of $E$. Nothing in-principle stops this association from being present, absent, or reversed. It&amp;rsquo;s an empirical matter of what one knows about the source of the report.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How Robust is the Zollman Effect?</title>
      <link>http://jonathanweisberg.org/post/rbo/</link>
      <pubDate>Mon, 02 Nov 2020 00:00:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/rbo/</guid>
      <description>

&lt;p&gt;This is the second in a trio of posts on simulated epistemic networks:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://jonathanweisberg.org/post/zollman/&#34;&gt;The Zollman Effect&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://jonathanweisberg.org/post/rbo&#34;&gt;How Robust is the Zollman Effect?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://jonathanweisberg.org/post/ow&#34;&gt;Mistrust &amp;amp; Polarization&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This post summarizes some key ideas from &lt;a href=&#34;http://doi.org/10.1086/690717&#34; target=&#34;_blank&#34;&gt;Rosenstock, Bruner, and O&amp;rsquo;Connor&amp;rsquo;s paper&lt;/a&gt; on the Zollman effect, and reproduces some of their results in Python. As always you can grab &lt;a href=&#34;https://github.com/jweisber/sep-sen&#34; target=&#34;_blank&#34;&gt;the code&lt;/a&gt; from GitHub.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://jonathanweisberg.org/post/zollman/&#34;&gt;Last time&lt;/a&gt; we met the Zollman effect: sharing experimental results in a scientific community can actually hurt its chances of arriving at the truth. Bad luck can generate misleading results, discouraging inquiry into superior options. By limiting the sharing of results, we can increase the chance that alternatives will be explored long enough for their superiority to emerge.&lt;/p&gt;

&lt;p&gt;But is this effect likely to have a big impact on actual research communities? Or is it rare enough, or small enough, that we shouldn&amp;rsquo;t really worry about it?&lt;/p&gt;

&lt;h1 id=&#34;easy-like-sunday-morning&#34;&gt;Easy Like Sunday Morning&lt;/h1&gt;

&lt;p&gt;Last time we saw the Zollman effect can be substantial. The chance of success increased from .89 to .97 when 10 researchers went from full sharing to sharing with just two neighbours (from complete to cycle).&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://jonathanweisberg.org/img/sep-sen/zollman.png&#34; /&gt;
  &lt;figcaption style=&#34;font-style:italic; font-size: .8em; text-align: center; padding-bottom: .75em;&#34;&gt;
      Fig. 1. The Zollman effect: less connected networks can have a better chance of discovering the truth
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;But that was assuming the novel alternative is only slightly better: .501 chance of success instead of .5, a difference of .001. We&amp;rsquo;d be less likely to get misleading results if the difference were .01, or .1. It should be easier to see the new treatment&amp;rsquo;s superiority in the data then.&lt;/p&gt;

&lt;p&gt;So RBO (Rosenstock, Bruner, and O&amp;rsquo;Connor) rerun the simulations with different values for ϵ, the increase in probability of success afforded by the new treatment. Last time we held ϵ fixed at .001, now we&amp;rsquo;ll let it vary up to .1. We&amp;rsquo;ll only consider a complete network vs. a wheel this time, and we&amp;rsquo;ll hold the number of agents fixed at 10. The number of trials each round continues to be 1,000.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://jonathanweisberg.org/img/sep-sen/rbo-2.png&#34; /&gt;
  &lt;figcaption style=&#34;font-style:italic; font-size: .8em; text-align: center; padding-bottom: .75em;&#34;&gt;
      Fig. 2. The Zollman effect vanishes as the difference in efficacy between the two treatments increases
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Here the Zollman effect shrinks as ϵ grows. In fact it&amp;rsquo;s only visible up to about .025 in our simulations.&lt;/p&gt;

&lt;h1 id=&#34;more-trials-fewer-tribulations&#34;&gt;More Trials, Fewer Tribulations&lt;/h1&gt;

&lt;p&gt;Something similar can happen as we increase &lt;em&gt;n&lt;/em&gt;, the number of trials each researcher performs. Last time we held &lt;em&gt;n&lt;/em&gt; fixed at 1,000, now let&amp;rsquo;s have it vary from 10 up to 10,000. We&amp;rsquo;ll stick to 10 agents again, although this time we&amp;rsquo;ll set ϵ to .01 instead of .001.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://jonathanweisberg.org/img/sep-sen/rbo-3.png&#34; /&gt;
  &lt;figcaption style=&#34;font-style:italic; font-size: .8em; text-align: center; padding-bottom: .75em;&#34;&gt;
      Fig. 3. The Zollman effect vanishes as the number of trials per iteration increases
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Again the Zollman effect fades, this time as the parameter &lt;em&gt;n&lt;/em&gt; increases.&lt;/p&gt;

&lt;p&gt;The emerging theme is that the easier the epistemic problem is, the smaller the Zollman effect. Before, we made the problem easier by making the novel treatment more effective. Now we&amp;rsquo;re making things easier by giving our agents more data. These are both ways of making the superiority of the novel treatment easier to see. The easier it is to discern two alternatives, the less our agents need to worry about inquiry being prematurely shut down by the misfortune of misleading data.&lt;/p&gt;

&lt;h1 id=&#34;agent-smith&#34;&gt;Agent Smith&lt;/h1&gt;

&lt;p&gt;Last time we saw that the Zollman effect seemed to grow as our network grew, from 3 up to 10 agents. But RBO note that the effect reverses after a while. Let&amp;rsquo;s return to &lt;em&gt;n&lt;/em&gt; = 1,000 trials and ϵ = .001, so that we&amp;rsquo;re dealing with a hard problem again. And let&amp;rsquo;s see what happens as the number of agents grows from 3 up to 100.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://jonathanweisberg.org/img/sep-sen/rbo-4.png&#34; /&gt;
  &lt;figcaption style=&#34;font-style:italic; font-size: .8em; text-align: center; padding-bottom: .75em;&#34;&gt;
      Fig. 4. The Zollman effect eventually shrinks as the number of agents increases
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The effect grows from 3 agents up to around 10. But then it starts to shrink again, narrowing to a meagre .01 at 100 agents.&lt;/p&gt;

&lt;p&gt;What&amp;rsquo;s happening here? As RBO explain, in the complete network a larger community effectively means a larger sample size at each round. Since the researchers pool their data, a community of 50 will update on the results of 25,000 trials at each round, assuming half the community has credence &amp;gt; 0.5. And a community of 100 people updates on the results of 50,000 trials, etc.&lt;/p&gt;

&lt;p&gt;As the pooled sample size increases, so does the probability it will accurately reflect the novel treatment&amp;rsquo;s superiority. The chance of the community being misled drops away.&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;RBO conclude that the Zollman effect only afflicts epistemically &amp;ldquo;hard&amp;rdquo; problems, where it&amp;rsquo;s difficult to discern the superior alternative from the data. But that doesn&amp;rsquo;t mean it&amp;rsquo;s not an important effect. Its importance just depends on how common it is for interesting problems to be &amp;ldquo;hard.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Do such problems crop up in actual scientific research, and if so how often? It&amp;rsquo;s difficult to say. As RBO note, the model we&amp;rsquo;ve been exploring is both artificially simple and highly idealized. So it&amp;rsquo;s unclear how often real-world problems, which tend to be messier and more complex, will follow similar patterns.&lt;/p&gt;

&lt;p&gt;On the one hand, they argue, our confidence that the Zollman effect is important should be diminished by the fact that it&amp;rsquo;s not robust against variations in the parameters. Fragile effects are less likely to come through in messy, real-world systems. On the other hand, they point to some empirical studies where Zollman-like effects seem to crop up in the real world.&lt;/p&gt;

&lt;p&gt;So it&amp;rsquo;s not clear. Maybe determining whether Zollman-hard problems are a real thing is itself a Zollman-hard problem?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Zollman Effect</title>
      <link>http://jonathanweisberg.org/post/zollman/</link>
      <pubDate>Wed, 28 Oct 2020 00:00:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/zollman/</guid>
      <description>

&lt;p&gt;I&amp;rsquo;m drafting a new social epistemology section for &lt;a href=&#34;https://plato.stanford.edu/entries/formal-epistemology/&#34; target=&#34;_blank&#34;&gt;the SEP entry on formal epistemology&lt;/a&gt;. It&amp;rsquo;ll focus on a series of three papers that study epistemic networks using computer simulations. This post is the first in a series of three explainers, one on each paper.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://jonathanweisberg.org/post/zollman&#34;&gt;The Zollman Effect&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://jonathanweisberg.org/post/rbo&#34;&gt;How Robust is the Zollman Effect?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://jonathanweisberg.org/post/ow&#34;&gt;Mistrust &amp;amp; Polarization&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In each post I&amp;rsquo;ll summarize the main ideas and replicate some key results in Python. You can grab &lt;a href=&#34;https://github.com/jweisber/sep-sen&#34; target=&#34;_blank&#34;&gt;the final code from GitHub&lt;/a&gt; if you want to play along and tinker.&lt;/p&gt;

&lt;h1 id=&#34;the-idea&#34;&gt;The Idea&lt;/h1&gt;

&lt;p&gt;More information generally means a better chance at discovering the truth, at least from an individual perspective. But not as a community, &lt;a href=&#34;https://www.doi.org/10.1086/525605&#34; target=&#34;_blank&#34;&gt;Zollman finds&lt;/a&gt;, at least not always. Sharing all our information with one another can make us less likely to reach the correct answer to a question we&amp;rsquo;re all investigating.&lt;/p&gt;

&lt;p&gt;Imagine there are two treatments available for some medical condition. One treatment is old, and its efficacy is well known: it has a .5 chance of success. The other treatment is new and might be slightly better or slightly worse: a .501 chance of success, or else .499.&lt;/p&gt;

&lt;p&gt;Some doctors are wary of the new treatment, others are more optimistic. So some try it on their patients while others stick to the old ways.&lt;/p&gt;

&lt;p&gt;As it happens the optimists are right: the new treatment is superior (chance .501 of success). So as they gather data about the new treatment and share it with the medical community, its superiority will eventually emerge as a consensus, right? At least, if all our doctors see all the evidence and weigh it fairly?&lt;/p&gt;

&lt;p&gt;Not necessarily. It&amp;rsquo;s possible that those trying the new treatment will hit a string of bad luck. Initial studies may get a run of less-than-stellar results, which don&amp;rsquo;t accurately reflect the new treatment&amp;rsquo;s superiority. After all, it&amp;rsquo;s only slightly better than the traditional treatment. So it might not show its mettle right away. And if it doesn&amp;rsquo;t, the optimists may abandon it before it has a chance to prove itself.&lt;/p&gt;

&lt;p&gt;One way to mitigate this danger, it turns out, is to restrict the flow of information in the medical community. Imagine one doctor gets a run of bad luck&amp;mdash;a string of patients who don&amp;rsquo;t do so well with the new treatment, creating the misleading impression that the new treatment is inferior. If they share this result with everyone, it&amp;rsquo;s more likely the whole community will abandon the new treatment. Whereas if they only share it with a few colleagues, others will keep trying the new treatment a while longer, hopefully giving them time to discover its superiority.&lt;/p&gt;

&lt;h1 id=&#34;the-model&#34;&gt;The Model&lt;/h1&gt;

&lt;p&gt;We can test this story by simulation. We&amp;rsquo;ll create a network of doctors, each with their own initial credence that the new treatment is superior. Those with credence &amp;gt; .5 will try the new treatment, others will stick to the old. Doctors directly connected in the network will share results with their neighbours, and everyone will update on whatever results they see using &lt;a href=&#34;https://en.wikipedia.org/wiki/Bayes%27_theorem&#34; target=&#34;_blank&#34;&gt;Bayes&amp;rsquo; theorem&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll consider networks of different sizes, from 3 to 10 agents. And we&amp;rsquo;ll try three different network &amp;ldquo;shapes&amp;rdquo;: complete, wheel, and cycle.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://jonathanweisberg.org/img/sep-sen/graph-shapes.png&#34;  /&gt;
  &lt;figcaption style=&#34;font-style:italic; font-size: .8em; text-align: center; padding-bottom: .75em;&#34;&gt;
      Fig. 1. Three network configurations, illustrated here with 6 agents each
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;These shapes vary in their connectedness. The complete network is fully connected, while the cycle is the least connected. Each doctor only confers with their two neighbours in the cycle. The wheel is in between.&lt;/p&gt;

&lt;p&gt;Our conjecture is that the cycle will prove most reliable. A doctor who gets a run of bad luck&amp;mdash;a string of misleading results&amp;mdash;will do the least damage there. Sharing their results might discourage their two neighbours from learning the truth. But the others in the network may keep investigating, and ultimately learn the truth about the new treatment&amp;rsquo;s superiority. The wheel should be more vulnerable to accidental misinformation, however, and the complete network most vulnerable.&lt;/p&gt;

&lt;h2 id=&#34;nitty-gritty&#34;&gt;Nitty Gritty&lt;/h2&gt;

&lt;p&gt;Initially, each doctor is assigned a random credence that the new treatment is superior, uniformly from the [0, 1] interval.&lt;/p&gt;

&lt;p&gt;Those with credence &amp;gt; .5 will then try the new treatment on 1,000 patients. The number of successes will be randomly determined, according to the &lt;a href=&#34;https://en.wikipedia.org/wiki/Binomial_distribution&#34; target=&#34;_blank&#34;&gt;binomial distribution&lt;/a&gt; with probability of success .501.&lt;/p&gt;

&lt;p&gt;Each doctor then shares their results with their neighbours, and updates by Bayes&amp;rsquo; theorem on all data available to them (their own + neighbors&amp;rsquo;). Then we do another round of experimenting, sharing, and updating, followed by another, and so on until the community reaches a consensus.&lt;/p&gt;

&lt;p&gt;Consensus can be achieved in either of two ways. Either everyone learns the truth that the new treatment is superior: credence &amp;gt; .99 let&amp;rsquo;s say. Alternatively, everyone might reach credence ≤ .5 in the new treatment. Then no one experiments with it further, so it&amp;rsquo;s impossible for it to make a comeback. (The .99 cutoff is kind of arbitrary, but it&amp;rsquo;s very unlikely the truth could be &amp;ldquo;unlearned&amp;rdquo; after that point.)&lt;/p&gt;

&lt;h1 id=&#34;results&#34;&gt;Results&lt;/h1&gt;

&lt;p&gt;Here&amp;rsquo;s what happens when we run each simulation 10,000 times. Both the shape of the network and the number of agents affect how often the community finds the truth.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://jonathanweisberg.org/img/sep-sen/zollman.png&#34;  /&gt;
  &lt;figcaption style=&#34;font-style:italic; font-size: .8em; text-align: center; padding-bottom: .75em;&#34;&gt;
      Fig. 2. Probability of discovering the truth depends on network configuration and number of agents.
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The less connected the network, the more likely they&amp;rsquo;ll find the truth. And a bigger community is more likely to find the truth too. Why?&lt;/p&gt;

&lt;p&gt;Bigger, less connected networks are better insulated against misleading results. Some doctors are bound to get data that don&amp;rsquo;t reflect the true character of the new treatment once in a while. And when that happens, their misleading results risk polluting the community with misinformation, discouraging others from experimenting with the new treatment. But the more people in the network, the more likely the misleading results will be swamped by accurate, representative results from others. And the fewer people see the misleading results, the fewer people will be misled.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s an animated pair of simulations to illustrate the second effect. Here I set the six scientists&amp;rsquo; starting credences to the same, even spread in both networks: .3, .4, .5, .6, .7, and .8. I also gave them the same sequence of random data. Only the connections in the networks are different, and in this case it makes all the difference. Only the cycle learns the truth. The complete network goes dark very early, abandoning the novel treatment entirely after just 26 iterations.&lt;/p&gt;

&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;video width=&#34;600&#34; height=&#34;300&#34; controls&gt;
    &lt;source src=&#34;http://jonathanweisberg.org/img/sep-sen/zollman.mp4&#34; type=&#34;video/mp4&#34;&gt;
  &lt;/video&gt;
  &lt;figcaption style=&#34;font-style:italic; font-size: .8em; text-align: center; padding-bottom: .75em;&#34;&gt;
      Fig. 3. Two networks with identical priors encounter identical evidence, but only one discovers the truth.
  &lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;What saves the cycle network is the agent who starts with .8 credence (bottom left). She starts out optimistic enough to keep going after the group encounters an initial string of dismaying results. In the complete network, however, she receives so much negative evidence early on that she gives up almost right away. Her optimism is overwhelmed by the negative findings of her many neighbours. Whereas the cycle exposes her to less of this discouraging evidence, giving her time to keep experimenting with the novel treatment, ultimately winning over her neighbours.&lt;/p&gt;

&lt;p&gt;As &lt;a href=&#34;http://doi.org/10.1086/690717&#34; target=&#34;_blank&#34;&gt;Rosenstock, Bruner, and O&amp;rsquo;Connor&lt;/a&gt; put it: sometimes less is more, when it comes to sharing the results of scientific inquiry. But how important is this effect? How often is it present, and is it big enough to worry about in actual practice? Next time we&amp;rsquo;ll follow Rosenstock, Bruner, and O&amp;rsquo;Connor further and explore these questions.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>