<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Social Epistemology on Jonathan Weisberg</title>
    <link>http://jonathanweisberg.org/tags/social-epistemology/index.xml</link>
    <description>Recent content in Social Epistemology on Jonathan Weisberg</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="http://jonathanweisberg.org/tags/social-epistemology/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>How Robust is the Zollman Effect?</title>
      <link>http://jonathanweisberg.org/post/rbo/</link>
      <pubDate>Mon, 02 Nov 2020 00:00:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/rbo/</guid>
      <description>

&lt;p&gt;This is the second in a trio of posts on simulated epistemic networks:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://jonathanweisberg.org/post/zollman/&#34;&gt;The Zollman Effect&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://jonathanweisberg.org/post/rbo&#34;&gt;How Robust is the Zollman Effect?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mistrust &amp;amp; Polarization (coming soon)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This post summarizes some key ideas from &lt;a href=&#34;http://doi.org/10.1086/690717&#34; target=&#34;_blank&#34;&gt;Rosenstock, Bruner, and O&amp;rsquo;Connor&amp;rsquo;s paper&lt;/a&gt; on the Zollman effect, and reproduces some of their results in Python. As always you can grab &lt;a href=&#34;https://github.com/jweisber/sep-sen&#34; target=&#34;_blank&#34;&gt;the code&lt;/a&gt; from GitHub.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://jonathanweisberg.org/post/zollman/&#34;&gt;Last time&lt;/a&gt; we met the Zollman effect: sharing experimental results in a scientific community can actually hurt its chances of arriving at the truth. Bad luck can generate misleading results, discouraging inquiry into superior options. By limiting the sharing of results, we can increase the chance that alternatives will be explored long enough for their superiority to emerge.&lt;/p&gt;

&lt;p&gt;But is this effect likely to have a big impact on actual research communities? Or is it rare enough, or small enough, that we shouldn&amp;rsquo;t really worry about it?&lt;/p&gt;

&lt;h1 id=&#34;easy-like-sunday-morning&#34;&gt;Easy Like Sunday Morning&lt;/h1&gt;

&lt;p&gt;Last time we saw the Zollman effect can be substantial. The chance of success increased from .89 to .97 when 10 researchers went from full sharing to sharing with just two neighbours (from complete to cycle).&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://jonathanweisberg.org/img/sep-sen/zollman.png&#34; /&gt;
  &lt;figcaption style=&#34;font-style:italic; font-size: .8em; text-align: center; padding-bottom: .75em;&#34;&gt;
      Fig. 1. The Zollman effect: less connected networks can have a better chance of discovering the truth
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;But that was assuming the novel alternative is only slightly better: .501 chance of success instead of .5, a difference of .001. We&amp;rsquo;d be less likely to get misleading results if the difference were .01, or .1. It should be easier to see the new treatment&amp;rsquo;s superiority in the data then.&lt;/p&gt;

&lt;p&gt;So RBO (Rosenstock, Bruner, and O&amp;rsquo;Connor) rerun the simulations with different values for ϵ, the increase in probability of success afforded by the new treatment. Last time we held ϵ fixed at .001, now we&amp;rsquo;ll let it vary up to .1. We&amp;rsquo;ll only consider a complete network vs. a wheel this time, and we&amp;rsquo;ll hold the number of agents fixed at 10. The number of trials each round continues to be 1,000.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://jonathanweisberg.org/img/sep-sen/rbo-2.png&#34; /&gt;
  &lt;figcaption style=&#34;font-style:italic; font-size: .8em; text-align: center; padding-bottom: .75em;&#34;&gt;
      Fig. 2. The Zollman effect vanishes as the difference in efficacy between the two treatments increases
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Here the Zollman effect shrinks as ϵ grows. In fact it&amp;rsquo;s only visible up to about .025 in our simulations.&lt;/p&gt;

&lt;h1 id=&#34;more-trials-fewer-tribulations&#34;&gt;More Trials, Fewer Tribulations&lt;/h1&gt;

&lt;p&gt;Something similar can happen as we increase &lt;em&gt;n&lt;/em&gt;, the number of trials each researcher performs. Last time we held &lt;em&gt;n&lt;/em&gt; fixed at 1,000, now let&amp;rsquo;s have it vary from 10 up to 10,000. We&amp;rsquo;ll stick to 10 agents again, although this time we&amp;rsquo;ll set ϵ to .01 instead of .001.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://jonathanweisberg.org/img/sep-sen/rbo-3.png&#34; /&gt;
  &lt;figcaption style=&#34;font-style:italic; font-size: .8em; text-align: center; padding-bottom: .75em;&#34;&gt;
      Fig. 3. The Zollman effect vanishes as the number of trials per iteration increases
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Again the Zollman effect fades, this time as the parameter &lt;em&gt;n&lt;/em&gt; increases.&lt;/p&gt;

&lt;p&gt;The emerging theme is that the easier the epistemic problem is, the smaller the Zollman effect. Before, we made the problem easier by making the novel treatment more effective. Now we&amp;rsquo;re making things easier by giving our agents more data. These are both ways of making the superiority of the novel treatment easier to see. The easier it is to discern two alternatives, the less our agents need to worry about inquiry being prematurely shut down by the misfortune of misleading data.&lt;/p&gt;

&lt;h1 id=&#34;agent-smith&#34;&gt;Agent Smith&lt;/h1&gt;

&lt;p&gt;Last time we saw that the Zollman effect seemed to grow as our network grew, from 3 up to 10 agents. But RBO note that the effect reverses after a while. Let&amp;rsquo;s return to &lt;em&gt;n&lt;/em&gt; = 1,000 trials and ϵ = .001, so that we&amp;rsquo;re dealing with a hard problem again. And let&amp;rsquo;s see what happens as the number of agents grows from 3 up to 100.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://jonathanweisberg.org/img/sep-sen/rbo-4.png&#34; /&gt;
  &lt;figcaption style=&#34;font-style:italic; font-size: .8em; text-align: center; padding-bottom: .75em;&#34;&gt;
      Fig. 4. The Zollman effect eventually shrinks as the number of agents increases
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The effect grows from 3 agents up to around 10. But then it starts to shrink again, narrowing to a meagre .01 at 100 agents.&lt;/p&gt;

&lt;p&gt;What&amp;rsquo;s happening here? As RBO explain, in the complete network a larger community effectively means a larger sample size at each round. Since the researchers pool their data, a community of 50 will update on the results of 25,000 trials at each round, assuming half the community has credence &amp;gt; 0.5. And a community of 100 people updates on the results of 50,000 trials, etc.&lt;/p&gt;

&lt;p&gt;As the pooled sample size increases, so does the probability it will accurately reflect the novel treatment&amp;rsquo;s superiority. The chance of the community being misled drops away.&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;RBO conclude that the Zollman effect only afflicts epistemically &amp;ldquo;hard&amp;rdquo; problems, where it&amp;rsquo;s difficult to discern the superior alternative from the data. But that doesn&amp;rsquo;t mean it&amp;rsquo;s not an important effect. Its importance just depends on how common it is for interesting problems to be &amp;ldquo;hard.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Do such problems crop up in actual scientific research, and if so how often? It&amp;rsquo;s difficult to say. As RBO note, the model we&amp;rsquo;ve been exploring is both artificially simple and highly idealized. So it&amp;rsquo;s unclear how often real-world problems, which tend to be messier and more complex, will follow similar patterns.&lt;/p&gt;

&lt;p&gt;On the one hand, they argue, our confidence that the Zollman effect is important should be diminished by the fact that it&amp;rsquo;s not robust against variations in the parameters. Fragile effects are less likely to come through in messy, real-world systems. On the other hand, they point to some empirical studies where Zollman-like effects seem to crop up in the real world.&lt;/p&gt;

&lt;p&gt;So it&amp;rsquo;s not clear. Maybe determining whether Zollman-hard problems are a real thing is itself a Zollman-hard problem?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Zollman Effect</title>
      <link>http://jonathanweisberg.org/post/zollman/</link>
      <pubDate>Wed, 28 Oct 2020 00:00:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/zollman/</guid>
      <description>

&lt;p&gt;I&amp;rsquo;m drafting a new social epistemology section for &lt;a href=&#34;https://plato.stanford.edu/entries/formal-epistemology/&#34; target=&#34;_blank&#34;&gt;the SEP entry on formal epistemology&lt;/a&gt;. It&amp;rsquo;ll focus on a series of three papers that study epistemic networks using computer simulations. This post is the first in a series of three explainers, one on each paper.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://jonathanweisberg.org/post/zollman&#34;&gt;The Zollman Effect&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://jonathanweisberg.org/post/rbo&#34;&gt;How Robust is the Zollman Effect?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mistrust &amp;amp; Polarization (coming soon)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In each post I&amp;rsquo;ll summarize the main ideas and replicate some key results in Python. You can grab &lt;a href=&#34;https://github.com/jweisber/sep-sen&#34; target=&#34;_blank&#34;&gt;the final code from GitHub&lt;/a&gt; if you want to play along and tinker.&lt;/p&gt;

&lt;h1 id=&#34;the-idea&#34;&gt;The Idea&lt;/h1&gt;

&lt;p&gt;More information generally means a better chance at discovering the truth, at least from an individual perspective. But not as a community, &lt;a href=&#34;https://www.doi.org/10.1086/525605&#34; target=&#34;_blank&#34;&gt;Zollman finds&lt;/a&gt;, at least not always. Sharing all our information with one another can make us less likely to reach the correct answer to a question we&amp;rsquo;re all investigating.&lt;/p&gt;

&lt;p&gt;Imagine there are two treatments available for some medical condition. One treatment is old, and its efficacy is well known: it has a .5 chance of success. The other treatment is new and might be slightly better or slightly worse: a .501 chance of success, or else .499.&lt;/p&gt;

&lt;p&gt;Some doctors are wary of the new treatment, others are more optimistic. So some try it on their patients while others stick to the old ways.&lt;/p&gt;

&lt;p&gt;As it happens the optimists are right: the new treatment is superior (chance .501 of success). So as they gather data about the new treatment and share it with the medical community, its superiority will eventually emerge as a consensus, right? At least, if all our doctors see all the evidence and weigh it fairly?&lt;/p&gt;

&lt;p&gt;Not necessarily. It&amp;rsquo;s possible that those trying the new treatment will hit a string of bad luck. Initial studies may get a run of less-than-stellar results, which don&amp;rsquo;t accurately reflect the new treatment&amp;rsquo;s superiority. After all, it&amp;rsquo;s only slightly better than the traditional treatment. So it might not show its mettle right away. And if it doesn&amp;rsquo;t, the optimists may abandon it before it has a chance to prove itself.&lt;/p&gt;

&lt;p&gt;One way to mitigate this danger, it turns out, is to restrict the flow of information in the medical community. Imagine one doctor gets a run of bad luck&amp;mdash;a string of patients who don&amp;rsquo;t do so well with the new treatment, creating the misleading impression that the new treatment is inferior. If they share this result with everyone, it&amp;rsquo;s more likely the whole community will abandon the new treatment. Whereas if they only share it with a few colleagues, others will keep trying the new treatment a while longer, hopefully giving them time to discover its superiority.&lt;/p&gt;

&lt;h1 id=&#34;the-model&#34;&gt;The Model&lt;/h1&gt;

&lt;p&gt;We can test this story by simulation. We&amp;rsquo;ll create a network of doctors, each with their own initial credence that the new treatment is superior. Those with credence &amp;gt; .5 will try the new treatment, others will stick to the old. Doctors directly connected in the network will share results with their neighbours, and everyone will update on whatever results they see using &lt;a href=&#34;https://en.wikipedia.org/wiki/Bayes%27_theorem&#34; target=&#34;_blank&#34;&gt;Bayes&amp;rsquo; theorem&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll consider networks of different sizes, from 3 to 10 agents. And we&amp;rsquo;ll try three different network &amp;ldquo;shapes&amp;rdquo;: complete, wheel, and cycle.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://jonathanweisberg.org/img/sep-sen/graph-shapes.png&#34;  /&gt;
  &lt;figcaption style=&#34;font-style:italic; font-size: .8em; text-align: center; padding-bottom: .75em;&#34;&gt;
      Fig. 1. Three network configurations, illustrated here with 6 agents each
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;These shapes vary in their connectedness. The complete network is fully connected, while the cycle is the least connected. Each doctor only confers with their two neighbours in the cycle. The wheel is in between.&lt;/p&gt;

&lt;p&gt;Our conjecture is that the cycle will prove most reliable. A doctor who gets a run of bad luck&amp;mdash;a string of misleading results&amp;mdash;will do the least damage there. Sharing their results might discourage their two neighbours from learning the truth. But the others in the network may keep investigating, and ultimately learn the truth about the new treatment&amp;rsquo;s superiority. The wheel should be more vulnerable to accidental misinformation, however, and the complete network most vulnerable.&lt;/p&gt;

&lt;h2 id=&#34;nitty-gritty&#34;&gt;Nitty Gritty&lt;/h2&gt;

&lt;p&gt;Initially, each doctor is assigned a random credence that the new treatment is superior, uniformly from the [0, 1] interval.&lt;/p&gt;

&lt;p&gt;Those with credence &amp;gt; .5 will then try the new treatment on 1,000 patients. The number of successes will be randomly determined, according to the &lt;a href=&#34;https://en.wikipedia.org/wiki/Binomial_distribution&#34; target=&#34;_blank&#34;&gt;binomial distribution&lt;/a&gt; with probability of success .501.&lt;/p&gt;

&lt;p&gt;Each doctor then shares their results with their neighbours, and updates by Bayes&amp;rsquo; theorem on all data available to them (their own + neighbors&amp;rsquo;). Then we do another round of experimenting, sharing, and updating, followed by another, and so on until the community reaches a consensus.&lt;/p&gt;

&lt;p&gt;Consensus can be achieved in either of two ways. Either everyone learns the truth that the new treatment is superior: credence &amp;gt; .99 let&amp;rsquo;s say. Alternatively, everyone might reach credence ≤ .5 in the new treatment. Then no one experiments with it further, so it&amp;rsquo;s impossible for it to make a comeback. (The .99 cutoff is kind of arbitrary, but it&amp;rsquo;s very unlikely the truth could be &amp;ldquo;unlearned&amp;rdquo; after that point.)&lt;/p&gt;

&lt;h1 id=&#34;results&#34;&gt;Results&lt;/h1&gt;

&lt;p&gt;Here&amp;rsquo;s what happens when we run each simulation 10,000 times. Both the shape of the network and the number of agents affect how often the community finds the truth.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&#34;http://jonathanweisberg.org/img/sep-sen/zollman.png&#34;  /&gt;
  &lt;figcaption style=&#34;font-style:italic; font-size: .8em; text-align: center; padding-bottom: .75em;&#34;&gt;
      Fig. 2. Probability of discovering the truth depends on network configuration and number of agents.
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The less connected the network, the more likely they&amp;rsquo;ll find the truth. And a bigger community is more likely to find the truth too. Why?&lt;/p&gt;

&lt;p&gt;Bigger, less connected networks are better insulated against misleading results. Some doctors are bound to get data that don&amp;rsquo;t reflect the true character of the new treatment once in a while. And when that happens, their misleading results risk polluting the community with misinformation, discouraging others from experimenting with the new treatment. But the more people in the network, the more likely the misleading results will be swamped by accurate, representative results from others. And the fewer people see the misleading results, the fewer people will be misled.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s an animated pair of simulations to illustrate the second effect. Here I set the six scientists&amp;rsquo; starting credences to the same, even spread in both networks: .3, .4, .5, .6, .7, and .8. I also gave them the same sequence of random data. Only the connections in the networks are different, and in this case it makes all the difference. Only the cycle learns the truth. The complete network goes dark very early, abandoning the novel treatment entirely after just 26 iterations.&lt;/p&gt;

&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;video width=&#34;600&#34; height=&#34;300&#34; controls&gt;
    &lt;source src=&#34;http://jonathanweisberg.org/img/sep-sen/zollman.mp4&#34; type=&#34;video/mp4&#34;&gt;
  &lt;/video&gt;
  &lt;figcaption style=&#34;font-style:italic; font-size: .8em; text-align: center; padding-bottom: .75em;&#34;&gt;
      Fig. 3. Two networks with identical priors encounter identical evidence, but only one discovers the truth.
  &lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;What saves the cycle network is the agent who starts with .8 credence (bottom left). She starts out optimistic enough to keep going after the group encounters an initial string of dismaying results. In the complete network, however, she receives so much negative evidence early on that she gives up almost right away. Her optimism is overwhelmed by the negative findings of her many neighbours. Whereas the cycle exposes her to less of this discouraging evidence, giving her time to keep experimenting with the novel treatment, ultimately winning over her neighbours.&lt;/p&gt;

&lt;p&gt;As &lt;a href=&#34;http://doi.org/10.1086/690717&#34; target=&#34;_blank&#34;&gt;Rosenstock, Bruner, and O&amp;rsquo;Connor&lt;/a&gt; put it: sometimes less is more, when it comes to sharing the results of scientific inquiry. But how important is this effect? How often is it present, and is it big enough to worry about in actual practice? Next time we&amp;rsquo;ll follow Rosenstock, Bruner, and O&amp;rsquo;Connor further and explore these questions.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>