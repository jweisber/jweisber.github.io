<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Philosophy on Jonathan Weisberg</title>
    <link>http://jonathanweisberg.org/tags/philosophy/index.xml</link>
    <description>Recent content in Philosophy on Jonathan Weisberg</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="http://jonathanweisberg.org/tags/philosophy/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>The Mosteller Hall Puzzle</title>
      <link>http://jonathanweisberg.org/post/Teaching%20Monty%20Hall/</link>
      <pubDate>Wed, 14 Jun 2017 15:21:42 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/Teaching%20Monty%20Hall/</guid>
      <description>&lt;p&gt;One of my favourite probability puzzles to teach is a close cousin of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Monty_Hall_problem&#34; target=&#34;_blank&#34;&gt;Monty Hall problem&lt;/a&gt;. Originally from a 1965 &lt;a href=&#34;https://books.google.ca/books/about/Fifty_Challenging_Problems_in_Probabilit.html?id=QiuqPejnweEC&#34; target=&#34;_blank&#34;&gt;book by Frederick Mosteller&lt;/a&gt;,&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; here&amp;rsquo;s my formulation:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Three prisoners, A, B, and C, are condemned to die in the morning. But the king decides in the night to pardon one of them. He makes his choice at random and communicates it to the guard, who is sworn to secrecy. She can only tell the prisoners that one of them will be released at dawn.&lt;/p&gt;

&lt;p&gt;Prisoner A welcomes the news, as he now has a 1/3 chance of survival. Hoping to go even further, he says to the guard, &amp;ldquo;I know you can&amp;rsquo;t tell me whether I am condemned or pardoned. But at least one other prisoner must still be condemned, so can you just name one who is?&amp;rdquo;. The guard replies (truthfully) that B is still condemned. &amp;ldquo;Ok&amp;rdquo;, says A, &amp;ldquo;then it&amp;rsquo;s either me or C who was pardoned. So my chance of survival has gone up to &amp;frac12;&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Unfortunately for A, he is mistaken. But how?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: turns out the puzzle isn&amp;rsquo;t originally due to Mosteller after all! It appears in &lt;a href=&#34;https://www.nature.com/scientificamerican/journal/v201/n4/pdf/scientificamerican1059-174.pdf&#34; target=&#34;_blank&#34;&gt;a 1959 article&lt;/a&gt; in &lt;em&gt;Scientific American&lt;/em&gt;, by Martin Gardner.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;For me it&amp;rsquo;s really intuitive that A is mistaken. The way he figures things, his chance of survival will go up to &amp;frac12; whoever the guard names in her response. But then A doesn&amp;rsquo;t even have to bother the guard. He can just skip ahead to the conclusion that his chance of survival is &amp;frac12;. And that&amp;rsquo;s absurd.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s a bit harder to say exactly &lt;em&gt;where&lt;/em&gt; A goes wrong. But I&amp;rsquo;ve always taken this puzzle to be, like Monty Hall, a lesson in Carnap&amp;rsquo;s TER: the Total Evidence Requirement.&lt;/p&gt;

&lt;p&gt;What A learns isn&amp;rsquo;t only that B is condemned, but also that the guard reports as much. And this report is more likely if C was pardoned than if A was. If C was pardoned, the guard had to name B, the only other prisoner still condemned. Whereas if A was pardoned, the guard could just as easily have named C instead.&lt;/p&gt;

&lt;p&gt;So when the guard names B, her report fits twice as well with the hypothesis that C was pardoned, not A:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/misc/mosteller_tree_diagram.png&#34; alt=&#34;Tree diagram&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Thus A&amp;rsquo;s chance of being condemned remains twice that of being pardoned.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re like me, this reasoning will actually be less intuitive than the initial, gut feeling that A must be mistaken (because her logic would make it unnecessary to consult the guard). The argument is still instructive though, for several reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;It shows how the initial, gut feeling is consistent with the probability axioms. We&amp;rsquo;ve constructed a plausible probability model that vindicates it.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The Total Evidence Requirement makes the difference in this model. Learning merely that B is condemned would have a different effect in this model. A&amp;rsquo;s chance of survival really would go up to &amp;frac12; then.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;These lessons can be carried over to Monty Hall. The same model yields the correct solution there, with the TER playing out in a parallel way.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;And that last point is the real point of this post. As my colleague &lt;a href=&#34;http://www.sergiotenenbaum.org/&#34; target=&#34;_blank&#34;&gt;Sergio Tenenbaum&lt;/a&gt; pointed out in conversation, it means you can use Mosteller&amp;rsquo;s puzzle to teach Monty Hall. Because, unlike in Monty Hall, &lt;em&gt;the intuitive judgment is the correct one in Mosteller&amp;rsquo;s puzzle&lt;/em&gt;. So you can use it to get students on board with the less intuitive (but entirely correct) argument we used to resolve Mosteller&amp;rsquo;s puzzle.&lt;/p&gt;

&lt;p&gt;Once students have seen how important it is to set up the probability model correctly, so that the Total Evidence Requirement can do its work, they may be more comfortable using the same technique on Monty Hall.&lt;/p&gt;

&lt;p&gt;There are other ways of bringing students around to the correct solution to Monty Hall, of course. You can run them through a variant with a hundred doors instead of three; you can invite them to consider what would happen in the long run in repeated games; you can ask them how things would have been different had Monty opened the other door instead.&lt;/p&gt;

&lt;p&gt;These are all worthy heuristics. And I expect different ones will click for different students.&lt;/p&gt;

&lt;p&gt;But for my money, there&amp;rsquo;s nothing like a simple and concrete model to help me get oriented and shake off that befuddled feeling. And, in this case, Mosteller&amp;rsquo;s puzzle helps make the model more intuitive, hence more memorable.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://68.media.tumblr.com/776dfc1f8b3baa0309b41c6a90ea1a13/tumblr_nd53ozBNz81qj0u7fo1_r1_400.gif&#34; alt=&#34;Fainting Goat&#34; /&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;So I think it actually predates Monty Hall, though I gather this general family of puzzles goes back at least to 1889 and &lt;a href=&#34;https://en.wikipedia.org/wiki/Bertrand%27s_box_paradox&#34; target=&#34;_blank&#34;&gt;Bertrand&amp;rsquo;s box paradox&lt;/a&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Accuracy for Dummies, Part 7: Dominance</title>
      <link>http://jonathanweisberg.org/post/Accuracy%20for%20Dummies%20-%20Part%207%20-%20Brier%20Dominance/</link>
      <pubDate>Wed, 07 Jun 2017 00:00:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/Accuracy%20for%20Dummies%20-%20Part%207%20-%20Brier%20Dominance/</guid>
      <description>

&lt;p&gt;In our &lt;a href=&#34;http://jonathanweisberg.org/post/Accuracy for Dummies - Part 5 - Convexity/&#34;&gt;last&lt;/a&gt; &lt;a href=&#34;http://jonathanweisberg.org/post/Accuracy for Dummies - Part 6 - Obtusity/&#34;&gt;two&lt;/a&gt; posts we established two key facts:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The set of possible probability assignments is convex.&lt;/li&gt;
&lt;li&gt;Convex sets are &amp;ldquo;obtuse&amp;rdquo;. Given a point outside a convex set, there&amp;rsquo;s a point inside that forms a right-or-obtuse angle with any third point in the set.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Today we&amp;rsquo;re putting them together to get the central result of the accuracy framework, the Brier dominance theorem. We&amp;rsquo;ll show that a non-probabilistic credence assignment is always &amp;ldquo;Brier dominated&amp;rdquo; by some probabilistic one. That is, there is always a probabilistic assignment that is closer, in terms of Brier distance, to every possible truth-value assignment.&lt;/p&gt;

&lt;p&gt;In fact we&amp;rsquo;ll show something a bit more general. We&amp;rsquo;ll show that there&amp;rsquo;s a probability assignment that&amp;rsquo;s closer to all the possible &lt;em&gt;probability&lt;/em&gt; assignments. But truth-value assignments are probability assignments, just extreme ones. So the result we really want follows straight away as a special case.$
\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\x}{\vec{x}}
\newcommand{\y}{\vec{y}}
\newcommand{\z}{\vec{z}}
\newcommand{\v}{\vec{v}}
\newcommand{\p}{\vec{p}}
\newcommand{\q}{\vec{q}}
\newcommand{\B}{B}
\newcommand{\R}{\mathbb{R}}
\newcommand{\EIpq}{EI_{\p}(\q)}\newcommand{\EIpp}{EI_{\p}(\p)}
$&lt;/p&gt;

&lt;h1 id=&#34;recap&#34;&gt;Recap&lt;/h1&gt;

&lt;p&gt;For reference, let&amp;rsquo;s collect our notation, terminology, and previous results, so that we have everything in one place.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;re using $n$ for the number of possibilities under consideration. And we use bold letters like $\x$ and $\p$ to represent $n$-tuples of real numbers. So $\p = (p_1, \ldots, p_n)$ is a point in $n$-dimensional space: a member of $\R^n$.&lt;/p&gt;

&lt;p&gt;We call $\p$ a &lt;em&gt;probability assignment&lt;/em&gt; if its coordinates are (a) all nonnegative, and (b) they sum to $1$. And we write $P$ for the set of all probability assignments.&lt;/p&gt;

&lt;p&gt;We call $\v$ a &lt;em&gt;truth-value assignment&lt;/em&gt; if its coordinates are all zeros except for a single $1$. And we write $V$ for the set of all truth-value assignments.&lt;/p&gt;

&lt;p&gt;A point $\y$ is a &lt;em&gt;mixture&lt;/em&gt; of the points $\x_1, \ldots, \x_n$ if there are real numbers $\lambda_1, \ldots, \lambda_n$ such that:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$\lambda_i \geq 0$ for all $i$,&lt;/li&gt;
&lt;li&gt;$\lambda_1 + \ldots + \lambda_n = 1$, and&lt;/li&gt;
&lt;li&gt;$\y = \lambda_1 \x_1 + \ldots + \lambda_n \x_n$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We say that a set is &lt;em&gt;convex&lt;/em&gt; if it is closed under mixing, i.e. any mixture of elements in the set is also in the set.&lt;/p&gt;

&lt;p&gt;The difference between two points, $\x - \y$, is defined coordinate-wise:
  $$ \x - \y = (x_1 - y_1, \ldots, x_n - y_n). $$
The &lt;em&gt;dot product&lt;/em&gt; of two points $\x$ and $\y$ is written $\x \cdot \y$, and is defined:
  $$ \x \cdot \y = x_1 y_1 + \ldots + x_n y_n. $$
As a reminder, the dot product returns a single, real number (not another $n$-dimensional point as one might expect). And the sign of the dot product reflects the angle between $\x$ and $\y$ when viewed as vectors/arrows. In particular, $\x \cdot \y \leq 0$ corresponds to a right-or-obtuse angle.&lt;/p&gt;

&lt;p&gt;Finally, $\B(\x,\y)$ is the Brier distance between $\x$ and $\y$, which can be defined:
  $$
  \begin{align}
    \B(\x,\y) &amp;amp;= (\x - \y)^2\\&lt;br /&gt;
              &amp;amp;= (\x - \y) \cdot (\x - \y).
  \end{align}
  $$&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s restate the two key theorems we&amp;rsquo;ll be relying on.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem (Convexity).&lt;/strong&gt;&amp;nbsp;
The set of probability functions $P$ is convex.&lt;/p&gt;

&lt;p&gt;We established this in &lt;a href=&#34;http://jonathanweisberg.org/post/Accuracy for Dummies - Part 5 - Convexity/&#34;&gt;Part 5&lt;/a&gt; of this series. In particular, we showed that $P$ is the &amp;ldquo;convex hull&amp;rdquo; of $V$: the set of all mixtures of points in $V$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lemma (Obtusity).&lt;/strong&gt;&amp;nbsp;
If $S$ is convex, $\x \not \in S$, and $\y \in S$ minimizes $\B(\y,\x)$ as a function of $\y$ on the domain $S$, then for any $\z \in S$, $(\x - \y) \cdot (\z - \y) \leq 0$.&lt;/p&gt;

&lt;p&gt;The intuitive idea behind this lemma, which we proved last time in &lt;a href=&#34;(/post/Accuracy for Dummies - Part 6 - Obtusity/)&#34; target=&#34;_blank&#34;&gt;Part 6&lt;/a&gt;, can be illustrated with a diagram:
&lt;img src=&#34;http://jonathanweisberg.org/img/accuracy/ObtusityLemma3.png&#34; alt=&#34;&#34; /&gt;
Given a point outside a convex set, we can find a point inside (the closest point) that forms a right-or-obtuse angle with all other points in the set.&lt;/p&gt;

&lt;p&gt;What we&amp;rsquo;ll show next is the natural and intuitive consequence: that point $\y$ is thus closer to any point $\z$ of $S$ than $\x$ is.&lt;/p&gt;

&lt;h1 id=&#34;the-brier-dominance-theorem&#34;&gt;The Brier Dominance Theorem&lt;/h1&gt;

&lt;p&gt;Intuitively, we want to show that if the angle formed at point $\y$ with the points $\x$ and $\z$ is right-or-obtuse, then $\y$ must be closer to $\z$ than $\x$ is (in Brier distance).&lt;/p&gt;

&lt;p&gt;Formally, a right-or-obtuse angle corresponds to a dot product less than or equal to zero: $(\x - \y) \cdot (\z - \y) \leq 0$. But if $\x = \y$, then the dot product will be zero trivially. So the precise statement of our theorem is:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt;&amp;nbsp;
If $(\x - \y) \cdot (\z - \y) \leq 0$ and $\x \neq \y$, then $\B(\x,\z) &amp;gt; \B(\y,\z)$.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt;  To start, we establish a general identity via algebra:
  $$
  \begin{align}
  \B(\x, \z) - \B(\x, \y) - \B(\y,\z)
    &amp;amp;= (\x - \z)^2 - (\x - \y)^2 - (\y - \z)^2\\&lt;br /&gt;
    &amp;amp;= -2\y^2 - 2 \x \cdot \z + 2 \x \cdot \y + 2 \y \cdot \z\\&lt;br /&gt;
    &amp;amp;= -2 (\x - \y) \cdot (\z - \y).
  \end{align}
  $$
Now suppose $ (\x - \y) \cdot (\z - \y) \leq 0$. Then, given the negative sign on the $-2$ in the established identity,
  $$ \B(\x, \z) - \B(\x, \y) - \B(\y,\z) \geq 0, $$
from which we derive
  $$ \B(\x, \z)  \geq \B(\x, \y) + \B(\y,\z). $$
Now, since $\x \neq \y$ by hypothesis, $\B(\x,\y) &amp;gt; 0$. Thus $\B(\x,\z) &amp;gt; \B(\y,\z)$, as desired.
&lt;span class=&#34;floatright&#34;&gt;$\Box$&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;It follows now that if $\x$ isn&amp;rsquo;t a probability assignment, there&amp;rsquo;s a probability assignment that&amp;rsquo;s closer to every truth-value assignment than $\x$ is.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Corollary (Brier Dominance).&lt;/strong&gt; If $\x \not \in P$ then there is a $\p \in P$ such that $\B(\p,\v) &amp;lt; \B(\x, \v)$ for all $\v \in V$.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt;  Fix $\x \not \in P$, and let $\p$ be the member of $P$ that minimizes $B(\y,\x)$ as a function of $\y$. The Convexity theorem tells us that $P$ is convex, so the Obtusity lemma implies $(\x - \p) \cdot (\v - \p) \leq 0$ for every $\v \in V$. And since $\x \neq \p$ (because $\x \not \in P$), the last theorem entails $\B(\p,\v) &amp;lt; \B(\x, \v)$, as desired.
&lt;span class=&#34;floatright&#34;&gt;$\Box$&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;This is the core of the main result we&amp;rsquo;ve been working towards. Hooray! But, we still have one piece of unfinished business. For what if $\p$ is itself dominated??&lt;/p&gt;

&lt;h1 id=&#34;undominated-dominance&#34;&gt;Undominated Dominance&lt;/h1&gt;

&lt;p&gt;We&amp;rsquo;ve shown that credences which violate the probability axioms are always &amp;ldquo;accuracy dominated&amp;rdquo; by some assignment of credences that obeys those axioms. But what if those dominating, probabilistic credences are themselves dominated? &lt;em&gt;What if they&amp;rsquo;re dominated by non-probabilistic credences??&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;For all we&amp;rsquo;ve said, that&amp;rsquo;s a real possibility. And if it actually obtains, then there&amp;rsquo;s nothing especially accuracy-conducive about the laws of probability. So we had better rule this possibility out. Luckily, that&amp;rsquo;s pretty easy to do.&lt;/p&gt;

&lt;p&gt;In fact, the reals work here was already done back in &lt;a href=&#34;http://jonathanweisberg.org/post/Accuracy for Dummies - Part 3/&#34;&gt;Part 3&lt;/a&gt; of the series. There we showed that Brier distance is a &amp;ldquo;proper&amp;rdquo; measure of inaccuracy: each probability assignment expects itself to do best with respect to accuracy, if inaccuracy is measured by Brier distance.&lt;/p&gt;

&lt;p&gt;As a reminder, we wrote $\EIpq$ for the expected inaccuracy of probability assignment $\q$ according to assignment $\p$. When inaccuracy is measured in terms of Brier distance:
$$ \EIpq = p_1 \B(\q,\v_1) + p_2 \B(\q,\v_2) + \ldots + p_n \B(\q,\v_n). $$
Here $\v_i$ is the truth-value assignment with a $1$ in the $i$-th coordinate, and $0$ everywhere else. What we showed in Part 3 was:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt;&amp;nbsp;
$\EIpq$ is uniquely minimized when $\q = \p$.&lt;/p&gt;

&lt;p&gt;And notice, this would be impossible if there were some $\q$ such that $\B(\q,\v_i) \leq \B(\p,\v_i)$ for all $i$. For then the weighted average $\EIpq$ would have to be no larger than $\EIpp$. And this contradicts the theorem, which says that $\EIpq &amp;gt; \EIpp$ for all $\q \neq \p$.&lt;/p&gt;

&lt;p&gt;So, at long last, we have the full result we want:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Corollary (Undominated Brier Dominance).&lt;/strong&gt; If $\x \not \in P$ then there is a $\p \in P$ such that $\B(\p,\v) &amp;lt; \B(\x, \v)$ for all $\v \in V$. Moreover, there is no $\q \in P$ such that $\B(\q,\v) \leq \B(\p, \v)$ for all $\v \in V$.&lt;/p&gt;

&lt;p&gt;So the laws of probability really are specially conducive to accuracy, as measured using Brier distance. Only probabilistic credence assignments are undominated.&lt;/p&gt;

&lt;h1 id=&#34;where-to-next&#34;&gt;Where to Next?&lt;/h1&gt;

&lt;p&gt;That&amp;rsquo;s a pretty sweet result. And it raises plenty of fun and interesting questions we could look at next. Here are three:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;What about other ways of measuring inaccuracy besides Brier? Are there reasonable alternatives, and if so, do similar results apply to them?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;What about other probabilistic principles, like Conditionalization, the Principal Principle, or the Principle of Indifference? Can we take this approach beyond the probability axioms?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Speaking of the probability axioms, we&amp;rsquo;ve been working with a pretty paired down conception of a &amp;ldquo;probability assignment&amp;rdquo;. Usually we assign probabilities not just to atomic possibilities, but to disjunctions/sets of possibilities: e.g. &amp;ldquo;the prize is behind either door #1 or door #2&amp;rdquo;. Can we extend this result to such &amp;ldquo;super-atomic&amp;rdquo; probability assignments?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We&amp;rsquo;ll tackle some or all of these questions in future posts. But I haven&amp;rsquo;t yet decided which ones or in what order.&lt;/p&gt;

&lt;p&gt;So for now let&amp;rsquo;s just stop and appreciate the work we&amp;rsquo;ve already done. Because not only have we proved one of the most central and interesting results of the accuracy framework. But also, in a lot of ways the hardest work is already behind us. If you&amp;rsquo;ve come this far, I think you deserve a nice pat on the back.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://i1145.photobucket.com/albums/o503/KimmieRocks/tumblr_liqmv89ru51qb2dn6.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Journal Submission Rates by Gender: A Look at the APA/BPA Data</title>
      <link>http://jonathanweisberg.org/post/A%20Look%20at%20the%20APA-BPA%20Data/</link>
      <pubDate>Tue, 06 Jun 2017 11:45:04 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/A%20Look%20at%20the%20APA-BPA%20Data/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; &lt;em&gt;editors at CJP and Phil Quarterly have kindly shared some important, additional information. See the edit below for details.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A &lt;a href=&#34;https://link.springer.com/article/10.1007/s11098-017-0919-0&#34; target=&#34;_blank&#34;&gt;new paper&lt;/a&gt; on the representation of women in philosophy journals prompted some debate in the philosophy blogosphere last week. The paper found women to be underrepresented across a range of prominent journals, yet overrepresented in the two journals studied where review was non-anonymous.&lt;/p&gt;

&lt;p&gt;Commenters &lt;a href=&#34;http://dailynous.com/2017/05/26/women-philosophy-journals-new-data/&#34; target=&#34;_blank&#34;&gt;over at Daily Nous&lt;/a&gt; complained about the lack of base-rate data. How many of the submissions to these journals were from women? In some respects, it&amp;rsquo;s hard to know what to make of these findings without such data.&lt;/p&gt;

&lt;p&gt;A few commenters linked to &lt;a href=&#34;http://www.apaonline.org/resource/resmgr/journal_surveys_2014/apa_bpa_survey_data_2014.xlsx&#34; target=&#34;_blank&#34;&gt;a survey&lt;/a&gt; conducted by the APA and BPA a while back, which supplies some numbers along these lines. I was surprised, because I&amp;rsquo;ve wondered about these numbers, but I didn&amp;rsquo;t recall seeing this data-set before. I was excited too because the data-set is huge, in a way: it covers more than 30,000 submissions at 40+ journals over a span of three years!&lt;/p&gt;

&lt;p&gt;So I was keen to give it a closer look. This post walks through that process. But I should warn you up front that the result is kinda disappointing.&lt;/p&gt;

&lt;h1 id=&#34;initial-reservations&#34;&gt;Initial Reservations&lt;/h1&gt;

&lt;p&gt;Right away some conspicuous omissions stand out.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; A good number of the usual suspects aren&amp;rsquo;t included, like &lt;em&gt;Philosophical Studies&lt;/em&gt;, &lt;em&gt;Analysis&lt;/em&gt;, and &lt;em&gt;Australasian Journal of Philosophy&lt;/em&gt;. So the usual worries about response rates and selection bias apply.&lt;/p&gt;

&lt;p&gt;The data are also a bit haphazard and incomplete. Fewer than half of the journals that responded included gender data. And some of those numbers are suspiciously round.&lt;/p&gt;

&lt;p&gt;Still, there&amp;rsquo;s hope. We have data on over ten thousand submissions even after we exclude journals that didn&amp;rsquo;t submit any gender data. As long as they paint a reasonably consistent picture, we stand to learn a lot.&lt;/p&gt;

&lt;h1 id=&#34;first-pass&#34;&gt;First Pass&lt;/h1&gt;

&lt;p&gt;For starters we&amp;rsquo;ll just do some minimal cleaning. We&amp;rsquo;ll exclude data from 2014, since almost no journals supplied it. And we&amp;rsquo;ll lump together the submissions from the remaining three years, 2011&amp;ndash;13, since the gender data isn&amp;rsquo;t broken down by year.&lt;/p&gt;

&lt;p&gt;We can then calculate the following cross-journal tallies for 2011&amp;ndash;13:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Accepted submissions&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Rejected submissions&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Men&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;792&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9104&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Women&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;213&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1893&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The difference here looks notable at first: 17.5% of submitted papers came from women compared with  21.2% of accepted papers, a statistically significant difference (&lt;em&gt;p&lt;/em&gt; = 0.002).&lt;/p&gt;

&lt;p&gt;But if we plot the data by journal, the picture becomes much less clear:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/apa_bpa_data_files/unnamed-chunk-3-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;The dashed line&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; indicates parity: where submission and acceptance rate would be equal. At journals above the line, women make up a larger portion of published authors than they do submitting authors. At journals below the line, it&amp;rsquo;s the reverse.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s pretty striking how much variation there is between journals. For example, &lt;em&gt;BJPS&lt;/em&gt; is 12 points above the parity line while &lt;em&gt;Phil Quarterly&lt;/em&gt; is 9 points below it.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s also notable that it&amp;rsquo;s the largest journals which diverge the most from parity: &lt;em&gt;BJPS&lt;/em&gt;, &lt;em&gt;EJP&lt;/em&gt;, &lt;em&gt;MIND&lt;/em&gt;, and &lt;em&gt;Phil Quarterly&lt;/em&gt;. (Note: &lt;em&gt;Hume Studies&lt;/em&gt; is actually the most extreme by far. But I&amp;rsquo;ve excluded it from the plot because it&amp;rsquo;s very small, and as an extreme outlier it badly skews the &lt;em&gt;y&lt;/em&gt;-axis.)&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s hard to see all the details in the plot, so here&amp;rsquo;s the same data in a table.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;Journal&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;submissions&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;accepted&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;% submissions women&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;% accepted women&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Ancient Philosophy&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;346&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;63&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;24&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;British Journal for the Philosophy of Science&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1267&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;117&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;27&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Canadian Journal of Philosophy&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;792&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;132&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;21&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Dialectica&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;826&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;74&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;12.05&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;15.48&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;European Journal for Philosophy&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1554&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;98&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;11.84&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;25&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Hume Studies&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;152&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;30&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;23.7&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;58.1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Journal of Applied Philosophy&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;510&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;47&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;20&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Journal of Political Philosophy&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1143&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;53&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;35&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;30&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;MIND&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1498&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;74&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Oxford Studies in Ancient Philosophy&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;290&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;43&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;21&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;20.3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Philosophy East and West&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;320&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;66&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;15&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Phronesis&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;388&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;38&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;24&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;25&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;The Journal of Aesthetics and Art Criticism&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;611&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;93&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;29&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;27&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;The Philosophical Quarterly&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2305&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;77&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;5&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&#34;rounders-removed&#34;&gt;Rounders Removed&lt;/h1&gt;

&lt;p&gt;I mentioned that some of the numbers look suspiciously round. Maybe 10% of submissions to &lt;em&gt;MIND&lt;/em&gt; really were from women, compared with 5% of accepted papers. But some of these cases probably involve non-trivial rounding, maybe even eyeballing or guesstimating. So let&amp;rsquo;s see how things look without them.&lt;/p&gt;

&lt;p&gt;If we omit journals where both percentages are round (integer multiples of 5), that leaves ten journals. And the gap from before is even more pronounced: 16.3% of submissions from women compared with  22.9% of accepted papers (&lt;em&gt;p&lt;/em&gt; = 0.0000003).&lt;/p&gt;

&lt;p&gt;But it&amp;rsquo;s still a few, high-volume journals driving the result: &lt;em&gt;BJPS&lt;/em&gt; and &lt;em&gt;EJP&lt;/em&gt; do a ton of business, and each has a large gap. So much so that they&amp;rsquo;re able to overcome the opposite contribution of &lt;em&gt;Phil Quarterly&lt;/em&gt; (which does a mind-boggling amount of business!).&lt;/p&gt;

&lt;h1 id=&#34;editors-anonymous&#34;&gt;Editors Anonymous&lt;/h1&gt;

&lt;p&gt;Naturally I fell to wondering how these big journals differ in their editorial practices. What are they doing differently that leads to such divergent results?&lt;/p&gt;

&lt;p&gt;One thing the data tell us is which journals practice fully anonymous review, with even the editors ignorant of the author&amp;rsquo;s identity. That narrows it down to just three journals: &lt;em&gt;CJP&lt;/em&gt;, &lt;em&gt;Dialectica&lt;/em&gt;, and &lt;em&gt;Phil Quarterly&lt;/em&gt;.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; The tallies then are:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Accepted submissions&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Rejected submissions&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Men&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;240&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3103&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Women&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;43&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;537&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;And now the gap is gone: 14.8% of submissions from women, compared with 15.2% of accepted papers&amp;mdash;not a statistically significant difference (&lt;em&gt;p&lt;/em&gt; = 0.91). That makes it look like the gap is down to editors&amp;rsquo; decisions being influenced by knowledge of the author&amp;rsquo;s gender (whether deliberately or unconsciously).&lt;/p&gt;

&lt;p&gt;But notice again, &lt;em&gt;Phil Quarterly&lt;/em&gt; is still a huge part of this story. It&amp;rsquo;s their high volume and unusually negative differential that compensates for the more modest, positive differentials at &lt;em&gt;CJP&lt;/em&gt; and &lt;em&gt;Dialectica&lt;/em&gt;. So I still want to know more about &lt;em&gt;Phil Quarterly&lt;/em&gt;, and what might explain their unusually negative differential.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: editors at &lt;em&gt;CJP&lt;/em&gt; and &lt;em&gt;Phil Quarterly&lt;/em&gt; kindly wrote with the following, additional information.&lt;/p&gt;

&lt;p&gt;At &lt;em&gt;CJP&lt;/em&gt;, the author&amp;rsquo;s identity is withheld from the editors while they decide whether to send the paper for external review, but then their identity is revealed (presumably to avoid inviting referees who are unacceptably close to the author&amp;mdash;e.g. those identical to the author).&lt;/p&gt;

&lt;p&gt;And chairman of &lt;em&gt;Phil Quarterly&lt;/em&gt;&amp;rsquo;s editorial board, Jessica Brown, writes:&lt;/p&gt;

&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;the PQ is very aware of issues about the representation of women, unsurprisingly given that the editorial board consists of myself, Sarah Broadie and Sophie-Grace Chappell. We monitor data on submissions by women and papers accepted in the journal every year.&lt;/li&gt;
&lt;li&gt;the PQ has for many years had fully anonymised processing including the point at which decisions on papers are made (i.e. accept, reject, R and R etc). So, when we make such decisions we have no idea of the identity of the author.&lt;/li&gt;

&lt;li&gt;&lt;p&gt;While in some years the data has concerned us, more recently the figures do look better which is encouraging:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;16-17: 25% declared female authored papers accepted; 16% submissions&lt;/li&gt;
&lt;li&gt;15-16: 14% accepted; 15% submissions&lt;/li&gt;
&lt;li&gt;14-15: 16% accepted; 16% submissions&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;a-gruesome-conclusion&#34;&gt;A Gruesome Conclusion&lt;/h1&gt;

&lt;p&gt;In the end, I don&amp;rsquo;t see a clear lesson here. Before drawing any conclusions from the aggregated, cross-journal tallies, it seems we&amp;rsquo;d need to know more about the policies and practices of the journals driving them. Otherwise we&amp;rsquo;re liable to be misled to a false generalization about a heterogeneous group.&lt;/p&gt;

&lt;p&gt;Some of that policy-and-practice information is probably publicly available; I haven&amp;rsquo;t had a chance to look. And I bet a lot of it is available informally, if you just talk to the right people. So this data-set could still be informative on our base-rate question. But sadly, I don&amp;rsquo;t think I&amp;rsquo;m currently in a position to make informative use of it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://i.imgur.com/ojvPBaY.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;technical-note&#34;&gt;Technical Note&lt;/h1&gt;

&lt;p&gt;This post was written in R Markdown and the source is &lt;a href=&#34;https://github.com/jweisber/rgo/blob/master/apa bpa data/apa_bpa_data.Rmd&#34; target=&#34;_blank&#34;&gt;available on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;No, I don&amp;rsquo;t mean &lt;em&gt;Ergo&lt;/em&gt;! We published our first issue in 2014 while the survey covers mainly 2011&amp;ndash;13.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;&lt;strong&gt;Edit&lt;/strong&gt;: the parity line was solid blue originally. But that misled some people into reading it as a fitted line. For reference and posterity, &lt;a href=&#34;http://jonathanweisberg.org/img/apa_bpa_data_files/unnamed-chunk-3-2.png&#34;&gt;the original image is here&lt;/a&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;That&amp;rsquo;s if we continue to exclude journals with very round numbers. Adding these journals back in doesn&amp;rsquo;t change the following result, though.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Accuracy for Dummies, Part 6: Obtusity</title>
      <link>http://jonathanweisberg.org/post/Accuracy%20for%20Dummies%20-%20Part%206%20-%20Obtusity/</link>
      <pubDate>Wed, 24 May 2017 00:00:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/Accuracy%20for%20Dummies%20-%20Part%206%20-%20Obtusity/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;http://jonathanweisberg.org/post/Accuracy for Dummies - Part 5 - Convexity/&#34;&gt;Last time&lt;/a&gt; we saw that the set of probability assignments is &lt;em&gt;convex&lt;/em&gt;. Today we&amp;rsquo;re going to show that convex sets have a special sort of &amp;ldquo;obtuse&amp;rdquo; relationship with outsiders. Given a point &lt;em&gt;outside&lt;/em&gt; a convex set, there is always a point &lt;em&gt;in&lt;/em&gt; the set that forms a right-or-obtuse angle with it.&lt;/p&gt;

&lt;p&gt;Recall our 2D diagram from the first post. The convex set of interest here is the diagonal line segment from $(0,1)$ to $(1,0)$:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/accuracy/2D Dominance Diagram - 400px.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For any point outside the diagonal, like $c^* $, there is a point like $c&amp;rsquo;$ on it that forms a right angle with all other points on the diagonal. As a result, $c&amp;rsquo;$ is closer to all other points on the diagonal than $c^* $ is. In particular, $c&amp;rsquo;$ is closer to both vertices, so it&amp;rsquo;s always more accurate than $c^*$. It&amp;rsquo;s &amp;ldquo;closer to the truth&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;The insider point $c&amp;rsquo;$ that we used in this case is the closest point on the diagonal to $c^*$. That&amp;rsquo;s what licenses the right-triangle reasoning here. Today we&amp;rsquo;re generalizing this strategy to $n$ dimensions.&lt;/p&gt;

&lt;p&gt;To do that, we need some tools for reasoning about $n$-dimensional geometry.$
\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\x}{\vec{x}}
\newcommand{\y}{\vec{y}}
\newcommand{\z}{\vec{z}}
\newcommand{\B}{B}
$&lt;/p&gt;

&lt;h1 id=&#34;arithmetic-with-arrows&#34;&gt;Arithmetic with Arrows&lt;/h1&gt;

&lt;p&gt;You&amp;rsquo;re familiar with arithmetic in one dimension: adding, subtracting, and multiplying single numbers. What about points in $n$ dimensions?&lt;/p&gt;

&lt;p&gt;We introduced two ideas for arithmetic with points last time. We&amp;rsquo;ll add a few more today, and also talk about what they mean geometrically.&lt;/p&gt;

&lt;p&gt;Suppose you have two points $\x$ and $\y$ in $n$ dimensions:
  $$
  \begin{align}
    \x &amp;amp;= (x_1, \ldots, x_n),\\&lt;br /&gt;
    \y &amp;amp;= (y_1, \ldots, y_n).
  \end{align}
  $$
Their sum $\x + \y$, as we saw last time, is defined as follows:
  $$ \x + \y = (x_1 + y_1, \ldots, x_n + y_n). $$
In other words, points are added coordinate-wise.&lt;/p&gt;

&lt;p&gt;This definition has a natural, geometric meaning we didn&amp;rsquo;t mention last time. Start by thinking of $\x$ and $\y$ as &lt;em&gt;vectors&lt;/em&gt;&amp;mdash;as arrows pointing from the origin to the points $\x$ and $\y$. Then $\x + \y$ just amounts to putting the two arrows end-to-point and taking the point at the end:
&lt;img src=&#34;http://jonathanweisberg.org/img/accuracy/VectorAddition.png&#34; alt=&#34;&#34; /&gt;
(Notice that we&amp;rsquo;re continuing our usual practice of bold letters for points/vectors like $\x$ and $\y$, and italics for single numbers like $x_1$ and $y_3$.)&lt;/p&gt;

&lt;p&gt;You can also multiply a vector $\x$ by a single number, $a$. The definition is once again coordinate-wise:
  $$ a \x = (a x_1, \ldots, a x_n). $$
And again there&amp;rsquo;s a natural, geometric meaning. We&amp;rsquo;ve lengthened the vector $\x$ by a factor of $a$.
&lt;img src=&#34;http://jonathanweisberg.org/img/accuracy/VectorMultiplication.png&#34; alt=&#34;&#34; /&gt;
Notice that if $a$ is between $0$ and $1$, then &amp;ldquo;lengthening&amp;rdquo; is actually shortening. For example, multiplying a vector by $a = 1/ 2$ makes it half as long.&lt;/p&gt;

&lt;p&gt;If $a$ is negative, then multiplying by $a$ reverses the direction of the arrow. For example, multiplying the northeasterly arrow $(1,1)$ by $-1$ yields the southwesterly arrow pointing to $(-1,-1)$.&lt;/p&gt;

&lt;p&gt;That means we can define subtraction in terms of addition and multiplication by negative one (just as with single numbers):
  $$
  \begin{align}
    \x - \y &amp;amp;= \x + (-1 \times \y)\\&lt;br /&gt;
            &amp;amp;= (x_1 - y_1, \ldots, x_n - y_n).
  \end{align}
  $$
So vector subtraction amounts to coordinate-wise subtraction.&lt;/p&gt;

&lt;p&gt;But what about multiplying two vectors? That&amp;rsquo;s actually different from what you might expect! We don&amp;rsquo;t just multiply coordinate-wise. We do that &lt;strong&gt;and then add up the results&lt;/strong&gt;:
  $$ \x \cdot \y = x_1 y_1 + \ldots + x_n y_n. $$
So the product of two vectors is &lt;strong&gt;not a vector&lt;/strong&gt;, but a number. That number is called the &lt;em&gt;dot product&lt;/em&gt;, $\x \cdot \y$.&lt;/p&gt;

&lt;p&gt;Why are dot products defined this way? Why do we add up the results of coordinate-wise multiplication to get a single number? Because it yields a more useful extension of the concept of multiplication from single numbers to vectors. We&amp;rsquo;ll see part of that in a moment, in the geometric meaning of the dot product.&lt;/p&gt;

&lt;p&gt;(There&amp;rsquo;s an algebraic side to the story too, having to do with the axioms that characterize the real numbers&amp;mdash;&lt;a href=&#34;https://en.wikipedia.org/wiki/Field_(mathematics)&#34; target=&#34;_blank&#34;&gt;the field axioms&lt;/a&gt;. We won&amp;rsquo;t go into that, but it comes out in &lt;a href=&#34;http://www.youtube.com/watch?v=63HpaUFEtXY&amp;amp;t=8m28s&#34; target=&#34;_blank&#34;&gt;this bit&lt;/a&gt; of a beautiful lecture by Francis Su, especially around &lt;a href=&#34;http://www.youtube.com/watch?v=63HpaUFEtXY&amp;amp;t=11m45s&#34; target=&#34;_blank&#34;&gt;the 11:45 mark&lt;/a&gt;.)&lt;/p&gt;

&lt;h1 id=&#34;signs-and-their-significance&#34;&gt;Signs and Their Significance&lt;/h1&gt;

&lt;p&gt;In two dimensions, a right angle has a special algebraic property: the dot-product of two arrows making the angle is always zero.&lt;/p&gt;

&lt;p&gt;Imagine a right triangle at the origin, with one leg going up to the point $(0,1)$ and the other leg going out to $(1,0)$:
&lt;img src=&#34;http://jonathanweisberg.org/img/accuracy/VectorRightAngle.png&#34; alt=&#34;&#34; /&gt;
The dot product of those two vectors is $(1,0) \cdot (0,1) = 1 \times 0 + 0 \times 1 = 0$. One more example: consider the right angle formed by the vectors $(-3,3)$ and $(1,1)$.
&lt;img src=&#34;http://jonathanweisberg.org/img/accuracy/VectorRightAngle2.png&#34; alt=&#34;&#34; /&gt;
Again, the dot product is $(-3,3) \cdot (1,1) = -3 \times 1 + 3 \times 1 = 0.$&lt;/p&gt;

&lt;p&gt;Going a bit further: the dot product is always positive for acute angles, and negative for obtuse angles. Take the vectors $(5,0)$ and $(-1,1)$:
&lt;img src=&#34;http://jonathanweisberg.org/img/accuracy/VectorObtuseAngle.png&#34; alt=&#34;&#34; /&gt;
Then we have $(5,0) \cdot (-1,1) = -5$. Whereas for $(5,0)$ and $(1,1)$:
&lt;img src=&#34;http://jonathanweisberg.org/img/accuracy/VectorAcuteAngle.png&#34; alt=&#34;&#34; /&gt;
we find $(5,0) \cdot (1,1) = 5$.&lt;/p&gt;

&lt;p&gt;So the sign of the dot-product reflects the angle formed by the vectors $\x$ and $\y$:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;acute angle: $\x \cdot \y &amp;gt; 0$,&lt;/li&gt;
&lt;li&gt;right angle: $\x \cdot \y = 0$,&lt;/li&gt;
&lt;li&gt;obtuse angle: $\x \cdot \y &amp;lt; 0$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That&amp;rsquo;s going to be key in generalizing to $n$ dimensions, where reasoning with diagrams breaks down. But first, one last bit of groundwork.&lt;/p&gt;

&lt;h1 id=&#34;algebra-with-arrows&#34;&gt;Algebra with Arrows&lt;/h1&gt;

&lt;p&gt;You can check pretty easily that vector addition and multiplication behave a lot like ordinary addition and multiplication. The usual laws of commutativity, associativity, and distribution hold:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$\x + \y = \y + \x$.&lt;/li&gt;
&lt;li&gt;$\x + (\y + \z) = (\x + \y) + \z$.&lt;/li&gt;
&lt;li&gt;$a ( \x + \y) = a\x + a\y$.&lt;/li&gt;
&lt;li&gt;$\x \cdot \y = \y \cdot \x$.&lt;/li&gt;
&lt;li&gt;$\x \cdot (\y + \z) = \x\y + \x\z$.&lt;/li&gt;
&lt;li&gt;$a (\x \cdot \y) = a \x \cdot \y = \x \cdot a \y$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One notable consequence, which we&amp;rsquo;ll use below, is the analogue of the familiar &lt;a href=&#34;https://en.wikipedia.org/wiki/FOIL_method&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;FOIL method&amp;rdquo;&lt;/a&gt; from high school algebra:
  $$
  \begin{align}
    (\x - \y)^2 &amp;amp;= (\x - \y) \cdot (\x - \y)\\&lt;br /&gt;
                &amp;amp;= \x^2 - 2 \x \cdot \y + \y^2.
  \end{align}
  $$
We&amp;rsquo;ll also make use of the fact that the Brier distance between $\x$ and $\y$ can be written $(\x - \y)^2$. Why?&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s write $\B(\x,\y)$ for the Brier distance between points $\x$ and $\y$. Recall the definition of Brier distance, which is just the square of Euclidean distance:
  $$ \B(\x,\y) = (x_1 - y_1)^2 + (x_2 - y_2)^2 + \ldots + (x_n - y_n)^2. $$
Now consider that, thanks to our definition of vector subtraction:
  $$ \x - \y = (x_1 - y_1, x_2 - y_2, \ldots, x_n - y_n). $$
And thanks to the definition of the dot product:
  $$ (\x - \y) \cdot (\x - \y) = (x_1 - y_1)^2 + (x_2 - y_2)^2 + \ldots (x_n - y_n)^2. $$
So $\B(\x, \y) = (\x - \y) \cdot (\x - \y)$, in other words:
  $$ \B(\x, \y) = (\x - y)^2. $$&lt;/p&gt;

&lt;h1 id=&#34;a-cute-lemma&#34;&gt;A Cute Lemma&lt;/h1&gt;

&lt;p&gt;Now we can prove the lemma that&amp;rsquo;s the aim of this post. For the intuitive idea, picture a convex set $S$ in the plane, like a pentagon. Then choose an arbitrary point $\x$ outside that set:
&lt;img src=&#34;http://jonathanweisberg.org/img/accuracy/ObtusityLemma.png&#34; alt=&#34;&#34; /&gt;
Now trace a straight line from $\x$ to the closest point of the convex region, $\y$:
&lt;img src=&#34;http://jonathanweisberg.org/img/accuracy/ObtusityLemma2.png&#34; alt=&#34;&#34; /&gt;
Finally, trace another straight line to any other point $\z$ of $S$:
&lt;img src=&#34;http://jonathanweisberg.org/img/accuracy/ObtusityLemma3.png&#34; alt=&#34;&#34; /&gt;
No matter what point we choose for $\z$, the angle formed will either be right or obtuse. It cannot be acute.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lemma.&lt;/strong&gt; Let $S$ be a convex set of points in $\mathbb{R}^n$. Let $\x \not \in S$, and let $\y \in S$ minimize $\B(\y, \x)$ as a function of $\y$ on the domain $S$. Then for any $\z \in S$,
  $$ (\x - \y) \cdot (\z - \y) \leq 0. $$&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s pause to understand what the Lemma is saying before we dive into the proof.&lt;/p&gt;

&lt;p&gt;Focus on the centered inequality. It&amp;rsquo;s about the vectors $\x - \y$ and $\z - \y$. These are the arrows pointing from $\y$ to $\x$, and from $\y$ to $\z$. So in terms of our original two dimensional diagram with the triangle:
&lt;img src=&#34;http://jonathanweisberg.org/img/accuracy/2D Dominance Diagram - 400px.png&#34; alt=&#34;&#34; /&gt;
we&amp;rsquo;re looking at the angle between  $c^*$, $c&amp;rsquo;$, and any point on the diagonal you like&amp;hellip; which includes the ones we&amp;rsquo;re especially interested in, the vertices. What the lemma tells us is that this angle is always at least a right angle.&lt;/p&gt;

&lt;p&gt;Of course, it&amp;rsquo;s exactly a right angle in this case, not an obtuse one. That&amp;rsquo;s because our convex region is just the diagonal line. But the Lemma could also be applied to the whole triangular region in the diagram. That&amp;rsquo;s a convex set too. And if we took a point inside the triangle as our third point, the angle formed would be obtuse. (This is actually important if you want to generalize the dominance theorem beyond what we&amp;rsquo;ll prove next time. But for us it&amp;rsquo;s just a mathematical extra.)&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s prove the Lemma.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt; Because $S$ is convex and $\y$ and $\z$ are in $S$, any mixture of $\y$ and $\z$ must also be in $S$. That is, every point $\lambda \z + (1-\lambda) \y$ is in $S$, given $0 \leq \lambda \leq 1$.&lt;/p&gt;

&lt;p&gt;Notice that we can rewrite $\lambda \z + (1-\lambda) \y$ as follows:
  $$ \lambda \z + (1-\lambda) \y = \y + \lambda(\z - \y). $$
We&amp;rsquo;ll use this fact momentarily.&lt;/p&gt;

&lt;p&gt;Now, by hypothesis $\y$ is at least as close to $\x$ as any other point of $S$ is. So, in particular, $\y$ is at least as close to $\x$ as the mixtures of $\y$ and $\z$ are. Thus, for any given $\lambda \in [0,1]$:
  $$ \B(\y,\x) \leq \B(\lambda \z + (1-\lambda) \y, \x). $$
Using algebra, we can transform the right-hand side as follows:
  $$
  \begin{align}
    \B(\lambda \z + (1-\lambda) \y, \x) &amp;amp;= \B(\x, \lambda \z + (1-\lambda) \y)\\&lt;br /&gt;
      &amp;amp;= \B(\x, \y + \lambda(\z - \y))\\&lt;br /&gt;
      &amp;amp;= (\x - (\y + \lambda(\z - \y)))^2\\&lt;br /&gt;
      &amp;amp;= ((\x - \y) - \lambda(\z - \y))^2\\&lt;br /&gt;
      &amp;amp;= (\x - \y)^2 + \lambda^2(\z - \y)^2 - 2\lambda(\x - \y) \cdot (\z - \y)\\&lt;br /&gt;
      &amp;amp;= \B(\x,\y) + \lambda^2\B(\z,\y) - 2\lambda(\x - \y) \cdot (\z - \y).
  \end{align}
  $$
Combining this equation with the previous inequality, we have:
  $$ \B(\y,\x) \leq \B(\x,\y) + \lambda^2\B(\z,\y) - 2\lambda(\x - \y) \cdot (\z - \y). $$
And because $\B(\y, \x) = \B(\x, \y)$, this becomes:&lt;br /&gt;
  $$ 0 \leq \lambda^2\B(\z,\y) - 2\lambda(\x - \y) \cdot (\z - \y). $$
If we then restrict our attention to $\lambda &amp;gt; 0$, we can divide and rearrange terms to get:
  $$ (\x - \y) \cdot (\z - \y) \leq \frac{\lambda\B(\z,\y)}{2}. $$
And since this inequality holds no matter how small $\lambda$ is, it follows that
  $$ (\x - \y) \cdot (\z - \y) \leq 0, $$
as desired.
&lt;span class=&#34;floatright&#34;&gt;$\Box$&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&#34;taking-stock&#34;&gt;Taking Stock&lt;/h1&gt;

&lt;p&gt;Here&amp;rsquo;s what we&amp;rsquo;ve got from this post and the last one:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Last time: the set of probability functions $P$ is convex.&lt;/li&gt;
&lt;li&gt;This time: given a point $\x$ outside $P$, there&amp;rsquo;s a point $\y$ inside $P$ that forms a right-or-obtuse angle with every other point $\z$ in $P$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Intuitively, it should follow that:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$\y$ is closer to every $\z$ in $P$ than $\x$ is.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And indeed, that&amp;rsquo;s what we&amp;rsquo;ll show in the next post!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Accuracy for Dummies, Part 5: Convexity</title>
      <link>http://jonathanweisberg.org/post/Accuracy%20for%20Dummies%20-%20Part%205%20-%20Convexity/</link>
      <pubDate>Thu, 18 May 2017 10:35:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/Accuracy%20for%20Dummies%20-%20Part%205%20-%20Convexity/</guid>
      <description>

&lt;p&gt;In this and the next two posts we&amp;rsquo;ll establish the central theorem of the accuracy framework. We&amp;rsquo;ll show that the laws of probability are specially suited to the pursuit of accuracy, measured in Brier distance.&lt;/p&gt;

&lt;p&gt;We showed this for cases with two possible outcomes, like a coin toss,  way back in &lt;a href=&#34;http://jonathanweisberg.org/post/Accuracy for Dummies - Part 1/&#34;&gt;the first post of this series&lt;/a&gt;. A simple, &lt;a href=&#34;http://jonathanweisberg.org/img/accuracy/2D Dominance Diagram - 400px.png&#34;&gt;two-dimensional diagram&lt;/a&gt; was all we really needed for that argument. To see how the same idea extends to any number of dimensions, we need to generalize the key ingredients of that reasoning to $n$ dimensions.&lt;/p&gt;

&lt;p&gt;This post supplies the first ingredient: the convexity theorem.&lt;/p&gt;

&lt;h1 id=&#34;convex-shapes&#34;&gt;Convex Shapes&lt;/h1&gt;

&lt;p&gt;Convex shapes are central to the accuracy framework because, in a way, the laws of probability have a convex shape. Hopefully that mystical pronouncement will make sense by the end of this post.&lt;/p&gt;

&lt;p&gt;You probably know a convex shape when you see one. Circles, triangles, and octagons are convex; pentagrams and the state of Texas are not.&lt;/p&gt;

&lt;p&gt;But what makes a convex shape convex? Roughly: &lt;em&gt;it contains all its connecting lines&lt;/em&gt;. If you take any two points in a convex region and draw a line connecting them, the line will lie entirely inside that region.&lt;/p&gt;

&lt;p&gt;But on a non-convex figure, you can find points whose connecting line leaves the figure&amp;rsquo;s boundary:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/accuracy/TexasLine.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We want to take this idea beyond two dimensions, though. And for that, we need  to generalize the idea of connecting lines. We need the concept of a &amp;ldquo;mixture&amp;rdquo;.&lt;/p&gt;

&lt;h2 id=&#34;pointy-arithmetic&#34;&gt;Pointy Arithmetic&lt;/h2&gt;

&lt;p&gt;In two dimensions it&amp;rsquo;s pretty easy to see that if you take some percentage of one point, and a complementary percentage of another point, you get a third point on the line between them.$
\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\p}{\vec{p}}
\newcommand{\q}{\vec{q}}
\newcommand{\r}{\vec{r}}
\newcommand{\v}{\vec{v}}
\newcommand{\R}{\mathbb{R}}
$&lt;/p&gt;

&lt;p&gt;For example, if you take $1/ 2$ of $(0,0)$ and add it to $1/ 2$ of $(1,1)$, you get the point halfway between: $(1/ 2,1/ 2)$. That&amp;rsquo;s pretty intuitive geometrically:
&lt;img src=&#34;http://jonathanweisberg.org/img/accuracy/Fig1.png&#34; alt=&#34;&#34; /&gt;
But we can capture the idea algebraically too:
$$
  \begin{align}
    1/ 2 \times (0,0) + 1/ 2 \times (1,1)
      &amp;amp;= (0,0) + (1/ 2, 1/ 2)\\&lt;br /&gt;
      &amp;amp;= (1/ 2, 1/ 2).
  \end{align}
$$&lt;/p&gt;

&lt;p&gt;Likewise, if you add $3/10$ of $(0,0)$ to $7/10$ of $(1, 1)$, you get the point seven-tenths of the way in between, namely $(7/10, 7/10)$:
&lt;img src=&#34;http://jonathanweisberg.org/img/accuracy/Fig2.png&#34; alt=&#34;&#34; /&gt;
In algebraic terms:
$$
  \begin{align}
    3/10 \times (0,0) + 7/10 \times (1,1)
      &amp;amp;= (0,0) + (7/10, 7/10)\\&lt;br /&gt;
      &amp;amp;= (7/10, 7/10).
  \end{align}
$$&lt;/p&gt;

&lt;p&gt;Notice that we just introduced two rules for doing arithmetic with points. When multiplying a point $\p = (p_1, p_2)$ by a number $a$, we get:
$$ a \p = (a p_1, a p_2). $$
And when adding two points $\p = (p_1, p_2)$ and $\q = (q_1, q_2)$ together:
$$ \p + \q = (p_1 + q_1, p_2 + q_2). $$
In other words, multiplying a point by a single number works element-wise, and so does adding two points together.&lt;/p&gt;

&lt;p&gt;We can generalize these ideas straightforwardly to any number of dimensions $n$. Given points $\p = (p_1, p_2, \ldots, p_n)$ and $\q = (q_1, q_2, \ldots, q_n)$, we can define:
$$ a \p = (a p_1, a p_2, \ldots, a p_n), $$
and
$$ \p + \q = (p_1 + q_1, p_2 + q_2, \ldots, p_n + q_n).$$
We&amp;rsquo;ll talk more about arithmetic with points next time. For now, these two definitions will do.&lt;/p&gt;

&lt;h2 id=&#34;mixtures&#34;&gt;Mixtures&lt;/h2&gt;

&lt;p&gt;Now back to connecting lines between points. The idea is that the straight line between $\p$ and $\q$ is the set of points we get by &amp;ldquo;mixing&amp;rdquo; some portion of $\p$ with some portion of $\q$.&lt;/p&gt;

&lt;p&gt;We take some number $\lambda$ between $0$ and $1$, we multiply $\p$ by $\lambda$ and $\q$ by $1 - \lambda$, and we sum the results: $\lambda \p + (1-\lambda) \q$. The set of points you can obtain this way is the straight line between $\p$ and $\q$.&lt;/p&gt;

&lt;p&gt;In fact, you can mix any number of points together. Given $m$ points $\q_1, \ldots, \q_m$, we can define their &lt;em&gt;mixture&lt;/em&gt; as follows. Let $\lambda_1, \ldots \lambda_m$ be positive real numbers that sum to one. That is:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$\lambda_i \geq 0$ for all $i$, and&lt;/li&gt;
&lt;li&gt;$\lambda_1 + \lambda_2 + \ldots + \lambda_m = 1$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Then we multiply each $\q_i$ by the corresponding $\lambda_i$ and sum up:
  $$ \p = \lambda_1 \q_1 + \ldots + \lambda_m \q_m. $$
The resulting point $\p$ is a &lt;em&gt;mixture&lt;/em&gt; of the $\q_i$&amp;rsquo;s.&lt;/p&gt;

&lt;p&gt;Now we can define the general notion of a &lt;em&gt;convex set&lt;/em&gt; of points. A convex set is one where the mixture of any points in the set is also contained in the set. (A convex set is &amp;ldquo;closed under mixing&amp;rdquo;, you might say.)&lt;/p&gt;

&lt;h1 id=&#34;convex-hulls&#34;&gt;Convex Hulls&lt;/h1&gt;

&lt;p&gt;It turns out that the set of possible probability assignments is convex.&lt;/p&gt;

&lt;p&gt;More than that, it&amp;rsquo;s the convex set generated by the possible truth-value assignments, in a certain way. It&amp;rsquo;s the &amp;ldquo;convex hull&amp;rdquo; of the possible truth-value assignments.&lt;/p&gt;

&lt;p&gt;What in the world is a &amp;ldquo;convex hull&amp;rdquo;?&lt;/p&gt;

&lt;p&gt;Imagine some points in the plane&amp;mdash;the corners of a square, for example. Now imagine stretching a rubber band around those points and letting it snap tight. The shape you get is the square with those points as corners. And the set of points enclosed by the rubber band is a convex set. Take any two points inside the square, or on its boundary, and draw the straight line between them. The line will not leave the square.&lt;/p&gt;

&lt;p&gt;Intuitively, the convex hull of a set of points in the plane is the set enclosed by the rubber band exercise. Formally, the convex hull of a set of points is the set of points that can be obtained from them as a mixture. (And this definition works in any number of dimensions.)&lt;/p&gt;

&lt;p&gt;For example, any of the points in our square example can be obtained by taking a mixture of the vertices. Take the center of the square: it&amp;rsquo;s halfway between the bottom left and top right corners. To get something to the left of that we can mix in some of the top left corner (and correspondingly less of the top right). And so on.&lt;/p&gt;

&lt;p&gt;Now imagine the rubber band exercise using the possible truth-value assignments, instead of the corners of a square. In two dimensions, those are the points $(0,1)$ and $(1,0)$. And when you let the band snap tight, you get the diagonal line connecting them. As we saw way back in &lt;a href=&#34;http://jonathanweisberg.org/post/Accuracy for Dummies - Part 1/&#34;&gt;our first post&lt;/a&gt;, the points on that diagonal line are the possible probability assignments.&lt;/p&gt;

&lt;h1 id=&#34;peeking-ahead&#34;&gt;Peeking Ahead&lt;/h1&gt;

&lt;p&gt;We also saw that if you take any point &lt;em&gt;not&lt;/em&gt; on that diagonal, the closest point on the diagonal forms a right angle. That&amp;rsquo;s what lets us do some basic geometric reasoning to see that there&amp;rsquo;s a point on the line that&amp;rsquo;s closer to both vertices than the point off the line:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/accuracy/2D Dominance Diagram - 400px.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;That fact about closest points and right angles is what&amp;rsquo;s going to enable us to generalize the argument beyond two dimensions. If you take any point not on a convex hull, there&amp;rsquo;s a point on the convex hull (namely the closest point) which forms a right (or obtuse) angle with the other points on the hull.&lt;/p&gt;

&lt;p&gt;Consider the three dimensional case. The possible truth-value assignments are $(1,0,0)$, $(0,1,0)$, and $(0,0,1)$:
&lt;img src=&#34;http://jonathanweisberg.org/img/accuracy/Three Vertices.png&#34; alt=&#34;&#34; /&gt;
And when you let a rubber band snap tight around them, it encloses the triangular surface connecting them:
&lt;img src=&#34;http://jonathanweisberg.org/img/accuracy/Three Vertices with Hull.png&#34; alt=&#34;&#34; /&gt;
That&amp;rsquo;s the set of probability assignments for three outcomes.&lt;/p&gt;

&lt;p&gt;Now take any point that&amp;rsquo;s not on that triangular surface. Drop a straight line to the closest point on the surface. Then draw another straight line from there to one of the triangle&amp;rsquo;s vertices. These two straight lines will form a right or obtuse angle. So the distance from the first, off-hull point to the vertex is further than the distance from the second, on-hull point to the vertex.&lt;/p&gt;

&lt;p&gt;Essentially the same reasoning works in any number of dimensions. But to make it work, we need to do three things.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Prove that the probability assignments always form a convex hull around the possible truth-value assignments.&lt;/li&gt;
&lt;li&gt;Prove that any point outside a convex hull forms a right angle (or an obtuse angle) with any point on the hull.&lt;/li&gt;
&lt;li&gt;Prove that the point off the hull is further from all the vertices than the closest point on the hull.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This post is dedicated to the first item.&lt;/p&gt;

&lt;h1 id=&#34;the-convexity-theorem&#34;&gt;The Convexity Theorem&lt;/h1&gt;

&lt;p&gt;We&amp;rsquo;re going to prove that the set of possible probability assignments is the same as the convex hull of the possible truth-value assignments. First let&amp;rsquo;s get some notation in place.&lt;/p&gt;

&lt;h2 id=&#34;notation&#34;&gt;Notation&lt;/h2&gt;

&lt;p&gt;As usual $n$ is the number of possible outcomes under consideration. So each possible truth-value assignment is a point of $n$ coordinates, with a single $1$ and $0$ everywhere else. For example, if $n = 4$ then $(0, 0, 1, 0)$ represents the case where the third possibility obtains.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll write $V$ for the set of all possible truth value assignments. And we&amp;rsquo;ll write $\v_1, \ldots, \v_n$ for the elements of $V$. The first element $\v_1$ has its $1$ in the first coordinate, $\v_2$ has its $1$ in the second coordinate, etc.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll use a superscript $^+$ for the convex hull of a set. So $V^+$ is the convex hull of $V$. It&amp;rsquo;s the set of all points that can be obtained by mixing members of $V$.&lt;/p&gt;

&lt;p&gt;Recall, a mixture is a point obtained by taking nonnegative real numbers $\lambda_1, \ldots, \lambda_n$ that sum to one, and multiplying each one against the corresponding $\v_i$ and then summing up:
  $$ \lambda_1 \v_1 + \lambda_2 \v_2 + \ldots + \lambda_n \v_n. $$
So $V^+$ is the set of all points that can be obtained by this method. Each choice of values $\lambda_1, \ldots, \lambda_n$ generates a member of $V^+$. (To exclude one of the $\v_i$&amp;rsquo;s from a mixture, just set $\lambda_i = 0$.)&lt;/p&gt;

&lt;p&gt;Finally, we&amp;rsquo;ll use $P$ for the set of all probability assignments. Recall: a probability assignment is a point of $n$ coordinates, where each coordinate is nonnegative, and all the coordinates together add up to one. That is, $\p = (p_1,\ldots,p_n)$ is a probability assignment just in case:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$p_i \geq 0$ for all $i$, and&lt;/li&gt;
&lt;li&gt;$p_1 + p_2 + \ldots + p_n = 1$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The set $P$ contains just those points $\p$ satisfying these two conditions.&lt;/p&gt;

&lt;h2 id=&#34;statement-and-proof&#34;&gt;Statement and Proof&lt;/h2&gt;

&lt;p&gt;In the notation just established, what we&amp;rsquo;re trying to show is that $V^+ = P$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; $V^+ = P$. That is, the convex hull of the possible truth-value assignments just is the set of possible probability assignments.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt; Let&amp;rsquo;s first show that $V^+ \subseteq P$.&lt;/p&gt;

&lt;p&gt;Notice that a truth-value assignment is also probability assignment. Its coordinates are always $1$ or $0$, so all coordinates are nonnegative. And since it has only a single coordinate with value $1$, its coordinates add up to $1$.&lt;/p&gt;

&lt;p&gt;But we have to show that any mixture of truth-value assignments is also a probability assignment. So let $\lambda_1, \ldots, \lambda_n$ be nonnegative numbers that sum to $1$. If we multiply $\lambda_i$ against a truth-value assignment $\v_i$, we get a point with $0$ in every coordinate except the $i$-th coordinate, which has value $\lambda_i$. For example, $\lambda_3 \times (0, 0, 1, 0) = (0, 0, \lambda_3, 0)$. So the mixture that results from $\lambda_1, \ldots, \lambda_n$ is:
  $$
    \lambda_1 \v_1 + \lambda_2 \v_2 + \ldots \lambda_n \v_n = (\lambda_1, \lambda_2, \ldots, \lambda_n).
  $$
And this mixture has coordinates that are all nonnegative and sum to $1$, by hypothesis. In other words, it is a probability assignment.&lt;/p&gt;

&lt;p&gt;So we turn to showing that $P \subseteq V^+$. In other words, we want to show that every probability assignment can be obtained as a mixture of the $\v_i$&amp;rsquo;s.&lt;/p&gt;

&lt;p&gt;So take an arbitrary probability assignment $\p \in P$, where $\p = (p_1, \ldots, p_n)$. Let the $\lambda_i$&amp;rsquo;s be the probabilities that $\p$ assigns to each $i$: $\lambda_1 = p_1$, $\lambda_2 = p_2$, and so on. Then, by the same logic as in the first part of the proof:
  $$ \lambda_1 \v_1 + \ldots + \lambda_n \v_n = (p_1, \ldots, p_n). $$
In other words, $\p$ is a mixture of the possible truth-value assignments, where the weights in the mixture are just the probability values assigned by $\p$. &lt;span style=&#34;float: right;&#34;&gt;$\Box$&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&#34;up-next&#34;&gt;Up Next&lt;/h1&gt;

&lt;p&gt;We&amp;rsquo;ve established the first of the three items listed earlier. Next time we&amp;rsquo;ll establish the second: given a point outside a convex set, there&amp;rsquo;s always a point inside that forms a right or obtuse angle with any other point of the set. Then we&amp;rsquo;ll be just a few lines of algebra from the main result: the Brier dominance theorem!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Journals as Ratings Agencies</title>
      <link>http://jonathanweisberg.org/post/Journals%20as%20Ratings%20Agencies/</link>
      <pubDate>Thu, 30 Mar 2017 15:27:04 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/Journals%20as%20Ratings%20Agencies/</guid>
      <description>

&lt;p&gt;Starting in July, philosophy&amp;rsquo;s two most prestigious journals won&amp;rsquo;t reject submitted papers anymore. Instead they&amp;rsquo;ll &amp;ldquo;grade&amp;rdquo; every submission, assigning a rating on the familiar letter-grade scale (A+, A, A-, B+, B, B-, etc.).&lt;/p&gt;

&lt;p&gt;They will, in effect, become ratings agencies.&lt;/p&gt;

&lt;p&gt;They&amp;rsquo;ll still publish papers. Those rated A- or higher can be published in the journal, if the authors want. Or they can seek another venue, if they think they can do better.&lt;/p&gt;

&lt;p&gt;I just made that up. But imagine if it were true&amp;mdash;especially if a bunch of journals did this. How would it change philosophy&amp;rsquo;s publication game?&lt;/p&gt;

&lt;p&gt;Well we&amp;rsquo;d save a lot of wasted labour, for one thing. And we&amp;rsquo;d discourage frivolous submissions, for another.&lt;/p&gt;

&lt;h1 id=&#34;the-bad&#34;&gt;The Bad&lt;/h1&gt;

&lt;p&gt;Under the current arrangement, the system is sagging low under the weight of premature, mediocre, even low-quality submissions. (I&amp;rsquo;d say it&amp;rsquo;s even creaking and cracking.) Editors scrounge miserably for referees, and referees frantically churn out reports and recommendations, mostly for naught.&lt;/p&gt;

&lt;p&gt;In a typical case, the editor rejects the submission and the referees&amp;rsquo; reports are filed away in a database, never to be read again. Maybe the author makes substantial revisions, but very likely they don&amp;rsquo;t&amp;mdash;especially if the paper&amp;rsquo;s main idea is the real limiting factor. The process repeats at another journal, often at several more journals. And in the end all the philosophical public sees is: accepted at &lt;em&gt;International Journal of Such &amp;amp; Such Studies&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Of all the people who&amp;rsquo;ve read and assessed the paper by that point, only two have their assessments directly broadcast to the public. And even then, only the &amp;ldquo;two thumbs more-or-less up&amp;rdquo; part of the signal gets out.&lt;/p&gt;

&lt;p&gt;Yet five, eight, or even ten people have weighed in on the paper by then. They&amp;rsquo;ve thought about its strengths and weaknesses, and they&amp;rsquo;ve generated valuable insights and assessments that could save others time and trouble. Yet only the handling editors and the authors get the direct benefit of that labour.&lt;/p&gt;

&lt;p&gt;The current system even encourages authors to waste editors&amp;rsquo; and referees&amp;rsquo; time. Unless they&amp;rsquo;re in a rush, authors can start at the top of the journal-prestige hierarchy and work their way down. You don&amp;rsquo;t even have to perfect your paper before starting this incredibly inefficient process. With so many journals to try, you&amp;rsquo;ll basically get unlimited kicks at the can. So you might as well let the referees do your homework for you.&lt;/p&gt;

&lt;p&gt;(This doesn&amp;rsquo;t apply to all authors, obviously. Some work in areas that severely limit their can-kicking. And many &lt;em&gt;are&lt;/em&gt; in a rush, to get jobs and tenure.)&lt;/p&gt;

&lt;h1 id=&#34;the-good&#34;&gt;The Good&lt;/h1&gt;

&lt;p&gt;But, if a paper were publicly assigned a grade every place it was submitted, authors might be more realistic in deciding where to submit. They might also wait until their paper is truly ready for public consumption before imposing on editors and referees.&lt;/p&gt;

&lt;p&gt;Readers would also benefit from seeing a paper&amp;rsquo;s transcript. Not only could it inform their decision about whether to read the paper, it could aid their sense of how its contribution is received by peers and experts.&lt;/p&gt;

&lt;p&gt;Referees would also have better incentives, to take on referee work and to be more diligent about it. They would know that their labour would have a greater impact, and that their assessment would have a more lasting effect.&lt;/p&gt;

&lt;p&gt;Editors could even limit submissions based on their grade-history, e.g. &amp;ldquo;no submissions already graded  by two other journals&amp;rdquo;, or &amp;ldquo;no submissions with an average grade less than a B&amp;rdquo;. (Ideally, different journals would have different policies here, to allow some variety.)&lt;/p&gt;

&lt;h1 id=&#34;the-ugly&#34;&gt;The Ugly&lt;/h1&gt;

&lt;p&gt;Of course, several high-profile journals would have to take the lead to make this kind of thing happen. And there would have to be strong norms within the discipline about publicizing grades: requiring they be listed alongside the paper on CVs and websites, for example&lt;/p&gt;

&lt;p&gt;And there would be costs.&lt;/p&gt;

&lt;p&gt;Everybody has their favourite story about the groundbreaking paper that got rejected five times, but was finally published in &lt;em&gt;The Posh Journal of Philosophy Review&lt;/em&gt;, and has since been cited a gajillion times. Such papers could be weighed down by having their grade-transcripts publicized. (On the plus side, we could have a new genre of great paper: the cult classic!)&lt;/p&gt;

&lt;p&gt;Also, some authors have to rely on referee feedback more than others, because of their limited philosophical networks. They&amp;rsquo;d likely find their papers with longer, more checkered grade-transcripts, exacerbating an existing injustice.&lt;/p&gt;

&lt;p&gt;And, in the end, the present proposal might only be a band-aid. If there really is an oversubmission problem in academic philosophy (as I suspect there is), it&amp;rsquo;s probably caused by increased pressure to publish&amp;mdash;because jobs are scarce, and administrators demand it, for example. Turning journals into ratings agencies wouldn&amp;rsquo;t relieve that pressure, even if it would help to manage some of its bad effects.&lt;/p&gt;

&lt;h1 id=&#34;decision-r-r&#34;&gt;Decision: R&amp;amp;R&lt;/h1&gt;

&lt;p&gt;In the end, I&amp;rsquo;m undecided about this proposal. I think it has some very attractive features, but the costs give me pause (much the same as the alternatives I&amp;rsquo;m aware of, like &lt;a href=&#34;http://davidfaraci.com/populus&#34; target=&#34;_blank&#34;&gt;Populus&lt;/a&gt;). I&amp;rsquo;m only certain that we can&amp;rsquo;t keep going as we have been; it won&amp;rsquo;t end well.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Accuracy for Dummies, Part 4: Euclid in the Round</title>
      <link>http://jonathanweisberg.org/post/Accuracy%20for%20Dummies%20-%20Part%204/</link>
      <pubDate>Thu, 23 Feb 2017 00:00:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/Accuracy%20for%20Dummies%20-%20Part%204/</guid>
      <description>

&lt;p&gt;Last time we took Brier distance beyond two dimensions. &lt;a href=&#34;http://jonathanweisberg.org/post/Accuracy for Dummies - Part 3/&#34;&gt;We showed&lt;/a&gt; that it&amp;rsquo;s &amp;ldquo;proper&amp;rdquo; in any finite number of dimensions. Today we&amp;rsquo;ll show that Euclidean distance is &amp;ldquo;improper&amp;rdquo; in any finite number dimensions.&lt;/p&gt;

&lt;p&gt;When I first sat down to write this post, I had in mind a straightforward generalization of &lt;a href=&#34;http://jonathanweisberg.org/post/Accuracy for Dummies - Part 1/&#34;&gt;our previous result&lt;/a&gt; for Euclidean distance in two dimensions. And I figured it would be easy to prove.&lt;/p&gt;

&lt;p&gt;Not so.&lt;/p&gt;

&lt;p&gt;My initial conjecture was false, and worse, when I asked my accuracy-guru friends for the truth, nobody seemed to know. (They did offer lots of helpful suggestions, though.)&lt;/p&gt;

&lt;p&gt;So today we&amp;rsquo;re muddling through on our own even more than usual. Here goes.&lt;/p&gt;

&lt;h1 id=&#34;background&#34;&gt;Background&lt;/h1&gt;

&lt;p&gt;Let&amp;rsquo;s recall where we are. We&amp;rsquo;ve been considering different ways of measuring the inaccuracy of a probability assignment given a possibility, or a &amp;ldquo;possible world&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s start today by regimenting our terminology. We&amp;rsquo;ve used these terms semi-formally for a while now. But let&amp;rsquo;s gather them here for reference, and to make them a little more precise.$
\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\p}{\vec{p}}
\newcommand{\q}{\vec{q}}
\newcommand{\u}{\vec{u}}
\newcommand{\EIpq}{EI_{\p}(\q)}
\newcommand{\EIpp}{EI_{\p}(\p)}
$&lt;/p&gt;

&lt;p&gt;Given a number of dimensions $n$:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A &lt;em&gt;probability assignment&lt;/em&gt; $\p = (p_1, \ldots, p_n)$ is a vector of positive real numbers that sum to $1$.&lt;/li&gt;
&lt;li&gt;A &lt;em&gt;possible world&lt;/em&gt; is a vector $\u$ of length $n$ containing all zeros except for a single $1$. (A &lt;a href=&#34;https://en.wikipedia.org/wiki/Unit_vector&#34; target=&#34;_blank&#34;&gt;unit vector&lt;/a&gt; of length $n$, in other words.)&lt;/li&gt;
&lt;li&gt;A &lt;em&gt;measure of inaccuracy&lt;/em&gt; $D(\p, \u)$ is a function that takes a probability assignment and a possible world and returns a real number.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We&amp;rsquo;ve been considering two measures of inaccuracy. The first is the familiar Euclidean distance between $\p$ and $\u$. For example, when $\u = (1, 0, \ldots, 0)$ we have:
$$ \sqrt{(p_1 - 1)^2 + (p_2 - 0)^2 + \ldots + (p_n - 0)^2}.$$
The second way of measuring inaccuracy is less familiar, Brier distance, which is just the square of Euclidean distance:
$$ (p_1 - 1)^2 + (p_2 - 0)^2 + \ldots + (p_n - 0)^2.$$&lt;/p&gt;

&lt;p&gt;What we found in $n = 2$ dimensions is that Euclidean distance is &amp;ldquo;unstable&amp;rdquo; in a way that Brier is not. If we measure inaccuracy using Euclidean distance, a probability assignment can expect some &lt;em&gt;other&lt;/em&gt; probability assignment to do better accuracy-wise, i.e. to have lower inaccuracy.&lt;/p&gt;

&lt;p&gt;In fact, given almost any probability assignment, the way to minimize expected inaccuracy is to leap to certainty in the most likely possibility. Given $(2/3, 1/3)$, for example, the way to minimize expected inaccuracy is to move to $(1,0)$.&lt;/p&gt;

&lt;p&gt;Because Euclidean distance is unstable in this way, it&amp;rsquo;s called an &amp;ldquo;improper&amp;rdquo; measure of inaccuracy. So, two more bits of terminology:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Given a probability assignment $\p$ and a measure of inaccuracy $D$, the &lt;em&gt;expected inaccuracy&lt;/em&gt; of probability assignment $\q$, written $\EIpq$, is the weighted sum:
$$
\EIpq = p_1 D(\q,\u_1) + \ldots + p_n D(\q,\u_n),
$$
where $\u_i$ is the possible world with a $1$ at index $i$.&lt;/li&gt;
&lt;li&gt;A measure of inaccuracy $D$ is &lt;em&gt;improper&lt;/em&gt; if there is a probability assignment $\p$ such that for some assignment $\q \neq \p$, $\EIpq &amp;lt; \EIpp$ when inaccuracy is measured according to $D$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Last time we showed that Brier is &lt;em&gt;proper&lt;/em&gt; in any finite number of dimensions $n$. Today our main task is to show that Euclidean distance is &lt;em&gt;&lt;strong&gt;im&lt;/strong&gt;proper&lt;/em&gt; in any finite number of dimensions $n$.&lt;/p&gt;

&lt;p&gt;But first, let&amp;rsquo;s get a tempting mistake out of the way.&lt;/p&gt;

&lt;h1 id=&#34;a-conjecture-and-its-refutation&#34;&gt;A Conjecture and Its Refutation&lt;/h1&gt;

&lt;p&gt;In &lt;a href=&#34;http://jonathanweisberg.org/post/Accuracy for Dummies - Part 1/&#34;&gt;our first post&lt;/a&gt;, we saw that Euclidean distance isn&amp;rsquo;t just improper in two dimensions. It&amp;rsquo;s also &lt;em&gt;extremizing&lt;/em&gt;: the assignment $(2/3, 1/3)$ doesn&amp;rsquo;t just expect &lt;em&gt;some&lt;/em&gt; other assignment to do better accuracy-wise. It expects the assignment $(1,0)$ to do best!&lt;/p&gt;

&lt;p&gt;At first I thought we&amp;rsquo;d be proving a straightforward generalization of that result today:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Conjecture 1 (False).&lt;/strong&gt; Let $(p_1, \ldots, p_n)$ be a probability assignment with a unique largest element $p_i$. If we measure inaccuracy by Euclidean distance, then $\EIpq$ is minimized when $\q = \u_i$.&lt;/p&gt;

&lt;p&gt;Intuitively: expected inaccuracy is minimized by leaping to certainty in the most probable possibility. Turns out this is false in three dimensions. Here&amp;rsquo;s a&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Counterexample.&lt;/strong&gt; Let&amp;rsquo;s define:
$$
\begin{align}
\p &amp;amp;= (5/12, 4/12, 3/12),\\&lt;br /&gt;
\p&amp;rsquo; &amp;amp;= (6/12, 4/12, 2/12),\\&lt;br /&gt;
\u_1 &amp;amp;= (1, 0, 0).
\end{align}
$$&lt;/p&gt;

&lt;p&gt;Then we can calculate (or better, &lt;a href=&#34;https://github.com/jweisber/a4d/blob/master/Euclid%20in%20the%20Round.nb&#34; target=&#34;_blank&#34;&gt;have &lt;em&gt;Mathematica&lt;/em&gt; calculate&lt;/a&gt;):
$$
\begin{align}
\EIpp &amp;amp;\approx .804,\\&lt;br /&gt;
EI_{\p}(\p&amp;rsquo;) &amp;amp;\approx .800,\\&lt;br /&gt;
EI_{\p}(\u_1) &amp;amp;\approx .825.
\end{align}
$$
In this case $\EIpp &amp;lt; EI_{\p}(\u_1)$. So leaping to certainty doesn&amp;rsquo;t minimize expected inaccuracy (as measured by Euclidean distance).&lt;/p&gt;

&lt;p&gt;Of course, staying put doesn&amp;rsquo;t minimize it either, since $EI_{\p}(\p&amp;rsquo;) &amp;lt; \EIpp$.&lt;/p&gt;

&lt;p&gt;So what &lt;em&gt;does&lt;/em&gt; minimize it in this example? I asked &lt;em&gt;Mathematica&lt;/em&gt; to minimize $\EIpq$ and got&amp;hellip; nothing for days. Eventually I gave up waiting and asked instead for &lt;a href=&#34;https://github.com/jweisber/a4d/blob/master/Euclid%20in%20the%20Round.nb&#34; target=&#34;_blank&#34;&gt;a numerical approximation of the minimum&lt;/a&gt;. One second later I got:&lt;/p&gt;

&lt;p&gt;$$EI_{\p}(0.575661, 0.250392, 0.173947) \approx 0.797432.$$&lt;/p&gt;

&lt;p&gt;I have no idea what that is in more meaningful terms, I&amp;rsquo;m sorry to say. But at least we know it&amp;rsquo;s not anywhere near the extreme point $\u_1$ I conjectured at the outset. (See the &lt;strong&gt;Update&lt;/strong&gt; at the end for a little more.)&lt;/p&gt;

&lt;h1 id=&#34;a-shortcut-and-its-shortcomings&#34;&gt;A Shortcut and Its Shortcomings&lt;/h1&gt;

&lt;p&gt;So I asked friends who do this kind of thing for a living how they handle the $n$-dimensional case.  A couple of them suggested taking a shortcut around it!&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Look, you&amp;rsquo;ve already handled the two-dimensional case. And that&amp;rsquo;s just an instance of higher dimensional cases.&lt;/p&gt;

&lt;p&gt;Take a probability assignment like (2/3, 1/3). We can also think of it as (2/3, 1/3, 0), or as (2/3, 0, 1/3, 0), etc.&lt;/p&gt;

&lt;p&gt;No matter how many zeros we sprinkle around in there, the same thing is going to happen as in the two-dimensional case. Leaping to certainty in the 2/3 possibility will minimize expected inaccuracy. (Because possibilities with no probability make no difference to expected value calculations.)&lt;/p&gt;

&lt;p&gt;So no matter how many dimensions we&amp;rsquo;re working in, there will always be &lt;em&gt;some&lt;/em&gt; probability assignment where leaping to certainty minimizes expected inaccuracy. It just might have lots of zeros in it.&lt;/p&gt;

&lt;p&gt;So Euclidean distance is, technically, improper in any finite number of dimensions.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;At first I thought that was good enough for philosophy. Though I still wanted to know how to handle &amp;ldquo;no zeros&amp;rdquo; cases for the mathematical clarity.&lt;/p&gt;

&lt;p&gt;Then I realized there may be a philosophical reason to be dissatisfied with this shortcut. A lot of people endorse the &lt;a href=&#34;http://philosophy.anu.edu.au/sites/default/files/Staying%20Regular.December%2028.2012.pdf&#34; target=&#34;_blank&#34;&gt;Regularity principle&lt;/a&gt;: you should never assign zero probability to any possibility. For these people, the shortcut might be a dead end.&lt;/p&gt;

&lt;p&gt;(Of course, maybe we shouldn&amp;rsquo;t embrace Regularity if we&amp;rsquo;re working in the accuracy framework. I won&amp;rsquo;t stop for that question here.)&lt;/p&gt;

&lt;h1 id=&#34;a-theorem-and-its-corollary&#34;&gt;A Theorem and Its Corollary&lt;/h1&gt;

&lt;p&gt;So let&amp;rsquo;s take the problem head on. We want to show that Euclidean distance is improper in $n &amp;gt; 2$ dimensions, even when there are &amp;ldquo;no zeros&amp;rdquo;. Two last bits of terminology:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A probability assignment $(p_1, \ldots, p_n)$ is &lt;em&gt;regular&lt;/em&gt; if $p_i &amp;gt; 0$ for all $i$.&lt;/li&gt;
&lt;li&gt;A probability assignment $(p_1, \ldots, p_n)$ is &lt;em&gt;uniform&lt;/em&gt; if $p_i = p_j$ for all $i,j$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So, for example, the assignment $(1/3, 1/3, 1/3)$ is both regular and uniform. Whereas the assignment $(2/5, 2/5, 1/5)$ is regular, but not uniform.&lt;/p&gt;

&lt;p&gt;What we&amp;rsquo;ll show is that assignments like $(2/5, 2/5, 1/5)$ make Euclidean distance &amp;ldquo;unstable&amp;rdquo;: they expect some other assignment to do better, accuracy-wise. (Exactly which other assignment they&amp;rsquo;ll expect to do best isn&amp;rsquo;t always easy to say.)&lt;/p&gt;

&lt;p&gt;(Though I try to keep the math in these posts as elementary as possible, this proof will use calculus. If you know a bit about derivatives, you should be fine. Technically we&amp;rsquo;ll use multi-variable calculus. But if you&amp;rsquo;ve worked with derivatives in single-variable calculus, that should be enough for the main ideas.)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt;&amp;nbsp;
Let $\p = (p_1, \ldots, p_n)$ be a regular, non-uniform probability assignment. If accuracy is measured by Euclidean distance, then $EI_{\p}(\q)$ is not minimized when $\q = \p$.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt;&amp;nbsp;
Let $\p = (p_1, \ldots, p_n)$ be a regular and non-uniform probability assignment, and measure inaccuracy using Euclidean distance. Then:
$$
\begin{align}
EI_{\p}(\q) &amp;amp;= p_1 \sqrt{(q_1 - 1)^2 + \ldots + (q_n - 0)^2} + \ldots + p_n \sqrt{(q_1 - 0)^2 + \ldots + (q_n - 1)^2}\\&lt;br /&gt;
&amp;amp;= p_1 \sqrt{(q_1 - 1)^2 + \ldots + q_n^2} + \ldots + p_n \sqrt{q_1^2 + \ldots + (q_n - 1)^2}
\end{align}
$$&lt;/p&gt;

&lt;p&gt;The crux of our proof will be that the derivatives of this function are non-zero at the point $\q = \p$. Since the minimum of a function is always a &lt;a href=&#34;https://en.wikipedia.org/wiki/Critical_point_(mathematics)&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;critical point&amp;rdquo;&lt;/a&gt;, that suffices to show that $\q = \p$ is not a minimum of $\EIpq$.&lt;/p&gt;

&lt;p&gt;To start, we calculate the partial derivative of $\EIpq$ for an arbitrary $q_i$:
$$
\begin{align}
\frac{\partial}{\partial q_i} \EIpq
&amp;amp;=
\frac{\partial}{\partial q_i} \left( p_1 \sqrt{(q_1 - 1)^2 + \ldots + q_n^2} + \ldots + p_n \sqrt{q_1^2 + \ldots + (q_n - 1)^2} \right)\\&lt;br /&gt;
&amp;amp;=
p_1 \frac{\partial}{\partial q_i} \sqrt{(q_1 - 1)^2 + \ldots + q_n^2} + \ldots + p_n \frac{\partial}{\partial q_i} \sqrt{q_1^2 + \ldots + (q_n - 1)^2}\\&lt;br /&gt;
&amp;amp;= \quad
p_i \frac{q_i - 1}{\sqrt{(q_i - 1)^2 + \sum_{j \neq i} q_j^2}} + \sum_{j \neq i} p_j \frac{q_i}{\sqrt{(q_j - 1)^2 + \sum_{k \neq j} q_k^2}}\\&lt;br /&gt;
&amp;amp;= \quad
\sum_{j \neq i} \frac{p_j q_i}{\sqrt{(q_j - 1)^2 + \sum_{k \neq j} q_k^2}} - \sum_{j \neq i} \frac{p_i q_j}{\sqrt{(q_i - 1)^2 + \sum_{j \neq i} q_j^2}}.
\end{align}
$$&lt;/p&gt;

&lt;p&gt;Then we evaluate at $\q = \p$:
$$
\begin{align}
\frac{\partial}{\partial q_i} \EIpp
&amp;amp;= \sum_{j \neq i} \frac{p_i p_j}{\sqrt{(p_j - 1)^2 + \sum_{k \neq j} p_k^2}} - \sum_{j \neq i} \frac{p_i p_j}{\sqrt{(p_i - 1)^2 + \sum_{j \neq i} p_j^2}}
\end{align}
$$&lt;/p&gt;

&lt;p&gt;Now, because $\p$ is not uniform, some of its elements are larger than others. And because it is finite, there is at least one largest element. When $p_i$ is one of these largest elements, then $\partial / \partial q_i \EIpp$ is negative.&lt;/p&gt;

&lt;p&gt;Why?&lt;/p&gt;

&lt;p&gt;In our equation for $\partial / \partial q_i \EIpp$, each positive term has a corresponding negative term whose numerator is identical. And when $p_i$ is a largest element of $\p$, the denominator of each negative term will never be larger, but will sometimes be smaller, than the denominator of its corresponding positive term. Subtracting $1$ from $p_i$ before squaring does more to reduce the sum of squares $p_i^2 + \sum_{j \neq i} p_j^2$ than subtracting $1$ from any smaller term would. It effectively removes the/a largest square from the sum and substitutes the smallest replacement. So the negative terms are never smaller, but are sometimes larger, than their positive counterparts.&lt;/p&gt;

&lt;p&gt;If, on the other hand, $p_i$ is the one of the smallest elements, then $\partial / \partial q_i \EIpp$ is positive. For then the reverse argument applies: the denominator of each negative term will never be smaller and will sometimes be larger than the denominator of the corresponding positive term. So the negatives terms are never larger, but are sometimes smaller, than their positive counterparts.&lt;/p&gt;

&lt;p&gt;We have shown that the partial derivates of $\EIpq$ are non-zero at the point $\q = \p$. Thus $\p$ is not a critical point of $\EIpq$, and hence cannot be a minimum of $\EIpq$.  &lt;span class=&#34;floatright&#34;&gt;$\Box$&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Corollary.&lt;/strong&gt; Euclidean distance is improper in any finite number of dimensions.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt;&amp;nbsp; This is just a slight restatement of our theorem. If $\q = \p$ is not a minimum of $\EIpq$, then there is some $\q \neq \p$ such that $\EIpq &amp;lt; \EIpp$.  &lt;span class=&#34;floatright&#34;&gt;$\Box$&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&#34;conjectures-awaiting-refutations&#34;&gt;Conjectures Awaiting Refutations&lt;/h1&gt;

&lt;p&gt;Notice, we&amp;rsquo;ve also shown something a bit stronger. We showed that the slope of $\EIpq$ at the point $\q = \p$ is always negative in the direction of $\p$&amp;rsquo;s largest element(s), and positive in the direction of its smallest element(s). That means we can always reduce expected inaccuracy by taking some small quantity away from the/a smallest element of $\p$ and adding it to the/a largest element. In other words, we can always reduce expected inaccuracy by moving &lt;em&gt;some&lt;/em&gt; way towards perfect certainty in the/a possibility that $\p$ rates most probable.&lt;/p&gt;

&lt;p&gt;However, we &lt;em&gt;haven&amp;rsquo;t&lt;/em&gt; shown that repeatedly minimizing expected inaccuracy will, eventually, lead to certainty in the/a possibility that was most probable to begin with. For one thing, we haven&amp;rsquo;t shown that moving towards certainty in this direction minimizes expected inaccuracy at each step. We&amp;rsquo;ve only shown that moving in this direction reduces it.&lt;/p&gt;

&lt;p&gt;Still, I&amp;rsquo;m pretty sure a result along these lines holds. Tinkering in &lt;em&gt;Mathematica&lt;/em&gt; strongly suggests that the following Conjectures are true in any finite number of dimensions $n$:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Conjecture 2.&lt;/strong&gt; If a probability assignment gives greater than $1/ 2$ probability to some possibility, then expected inaccuracy is minimized by assigning probability 1 to that possibility. (But see the &lt;strong&gt;Update&lt;/strong&gt; below.)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Conjecture 3.&lt;/strong&gt; Given a non-uniform probability assignment, repeatedly minimizing expected inaccuracy will, within a finite number of steps, increase the probability of the/a possibility that was most probable initially beyond $1/ 2$.&lt;/p&gt;

&lt;p&gt;If these conjectures hold, then there&amp;rsquo;s still a weak-ish sense in which Euclidean distance is &amp;ldquo;extremizing&amp;rdquo; in $n &amp;gt; 2$ dimensions. Given a non-uniform probability assignment, repeatedly minimizing expected inaccuracy will eventually lead to greater than $1/ 2$ probability in the/a possibility that was most probable to begin with. Then, minimizing inaccuracy will lead in a single step to certainty in that possibility.&lt;/p&gt;

&lt;p&gt;Proving these conjectures would close much of the gap between the theorem we proved and the false conjecture I started with. If you&amp;rsquo;re interested, you can use &lt;a href=&#34;https://github.com/jweisber/a4d/blob/master/Euclid%20in%20the%20Round.nb&#34; target=&#34;_blank&#34;&gt;this &lt;em&gt;Mathematica&lt;/em&gt; notebook&lt;/a&gt; to test them.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update: Mar. 6, 2017.&lt;/strong&gt; Thanks to some excellent help from &lt;a href=&#34;https://mathematics.stanford.edu/people/department-directory/name/jonathan-love/&#34; target=&#34;_blank&#34;&gt;Jonathan Love&lt;/a&gt;, I&amp;rsquo;ve tweaked this post (and greatly simplified &lt;a href=&#34;http://jonathanweisberg.org/post/Accuracy%20for%20Dummies%20-%20Part%203/&#34;&gt;the previous one&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;I changed the counterexample to the false Conjecture 1, which used to be $\p = (3/7, 2/7, 2/7)$ and $\p&amp;rsquo; = (4/7, 2/7, 1/7)$. That works fine, but it&amp;rsquo;s potentially misleading.&lt;/p&gt;

&lt;p&gt;As Jonathan kindly pointed out, the minimum point then is something quite nice. It&amp;rsquo;s obtained by moving in the $x$-dimension from $3/7$ to $\sqrt{3/7}$, and correspondingly reducing the probability in the $y$ and $z$ dimensions in equal parts.&lt;/p&gt;

&lt;p&gt;But, in general, moving to the square root of the largest $p_i$ (when there is one) doesn&amp;rsquo;t minimize $\EIpq$. Even in the special case where all the other elements in the vector are equal, this doesn&amp;rsquo;t generally work.&lt;/p&gt;

&lt;p&gt;Jonathan did solve that special case, though, and he found at least one interesting result connected with Conjecture 2. There appear to be cases where $p_i &amp;lt; 1/ 2$ for all $i$, and yet $\EIpq$ is still minimized by going directly to the extreme. For example, $\p = (.465, .2675, .2675)$.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Accuracy for Dummies, Part 3: Beyond the Second Dimension</title>
      <link>http://jonathanweisberg.org/post/Accuracy%20for%20Dummies%20-%20Part%203/</link>
      <pubDate>Fri, 27 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/Accuracy%20for%20Dummies%20-%20Part%203/</guid>
      <description>

&lt;p&gt;Last time we saw why accuracy-mavens prefer Brier distance to Euclidean distance. But we did everything in two dimensions. That&amp;rsquo;s fine for a coin toss, with only two possibilities. But what if there are three doors and one of them has a prize behind it??&lt;/p&gt;

&lt;p&gt;Don&amp;rsquo;t panic! Today we&amp;rsquo;re going to verify that Brier distance is still a proper way of measuring inaccuracy, even when there are more than two possibilities. (Next time we&amp;rsquo;ll talk about Euclidean distance with more than two possibilitie.)&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s start small, with just three possibilities. $\renewcommand{\vec}[1]{\mathbf{#1}}\newcommand{\p}{\vec{p}}\newcommand{\q}{\vec{q}}\newcommand{\v}{\vec{v}}\newcommand{\EIpq}{EI_{\p}(\q)}\newcommand{\EIpp}{EI_{\p}(\p)}$&lt;/p&gt;

&lt;h1 id=&#34;three-possibilities&#34;&gt;Three Possibilities&lt;/h1&gt;

&lt;p&gt;You&amp;rsquo;re on a game show; there are three doors; one has a prize behind it. The three possibilities are represented by the vertices $(1,0,0)$, $(0,1,0)$, and $(0,0,1)$:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/accuracy/Three Vertices.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Your credences are given by some probability assignment $(p_1, p_2, p_3)$. It might be $(1/ 3, 1/ 3, 1/ 3)$ but it could be anything&amp;hellip; $(7/ 10, 2/ 10, 1/ 10)$, for example.&lt;/p&gt;

&lt;p&gt;In case you&amp;rsquo;re curious, here&amp;rsquo;s what the range of possible probability assignments looks like in graphical terms:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/accuracy/Three Vertices with Hull.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The triangular surface is the three-dimensional analogue of the diagonal line in &lt;a href=&#34;http://jonathanweisberg.org/img/accuracy/2D Dominance Diagram.png&#34;&gt;the two-dimensional diagram&lt;/a&gt; from our &lt;a href=&#34;http://jonathanweisberg.org/post/Accuracy for Dummies - Part 1/&#34;&gt;first post&lt;/a&gt; in this series.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;ll be handy to refer to points on this surface using single letters, like $\p$ for $(p_1, p_2, p_3)$. We&amp;rsquo;ll write these letters in bold, to distinguish a sequence of numbers like $\p$ from a single number like $p_1$. (In math-speak, $\p$ is a &lt;em&gt;vector&lt;/em&gt; and $p_1$ is a &lt;em&gt;scalar&lt;/em&gt;.)&lt;/p&gt;

&lt;p&gt;Our job is to show that Brier distance is &amp;ldquo;proper&amp;rdquo; in three dimensions. Let&amp;rsquo;s recall what that means: given a point $\p$, the expected Brier distance (according to $\p$) of a point $\q = (q_1, q_2, q_3)$ from the three vertices is always smallest when $\q = \p$.&lt;/p&gt;

&lt;p&gt;What does &lt;em&gt;that&lt;/em&gt; mean?&lt;/p&gt;

&lt;p&gt;Recall, the Brier distance from $\q$ to the vertex $(1, 0, 0)$ is:
$$
(q_1 - 1)^2 + (q_2 - 0)^2 + (q_3 - 0)^2
$$
Or, more succinctly:
$$
(q_1 - 1)^2 + q_2^2 + q_3^2
$$
So the &lt;em&gt;expected&lt;/em&gt; Brier distance of $\q$ according to $\p$ weights each such sum by the probability $\p$ assigns to the corresponding vertex.
$$
\begin{align}
&amp;amp;\quad\quad p_1 \left( (q_1 - 1)^2 + q_2^2 + q_3^2 \right)\\&lt;br /&gt;
  &amp;amp;\quad + p_2 \left( q_1^2 + (q_2 - 1)^2 + q_3^2 \right)\\&lt;br /&gt;
  &amp;amp;\quad + p_3 \left( q_1^2 + q_2^2 + (q_3 - 1)^2 \right)
\end{align}
$$
We need to show that this quantity is smallest when $\q = \p$, i.e. when $q_1 = p_1$, $q_2 = p_2$, and $q_3 = p_3$.&lt;/p&gt;

&lt;h2 id=&#34;visualizing-expected-inaccuracy&#34;&gt;Visualizing Expected Inaccuracy&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s do some visualization. We&amp;rsquo;ll take a few examples of $\p$, and graph the expected inaccuracy of other possible points $\q$, using Brier distance to measure inaccuracy.&lt;/p&gt;

&lt;p&gt;For example, suppose $\p = (1/ 3, 1/ 3, 1/ 3)$. Then the expected inaccuracy of each point $\q$ looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/accuracy/BrierEI3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The horizontal axes represent $q_1$ and $q_2$. The vertical axis represents expected inaccuracy.&lt;/p&gt;

&lt;p&gt;Where&amp;rsquo;s $q_3$?? Not pictured! If we used all three visible dimensions for the elements of $\q$, we&amp;rsquo;d have nothing left to visualize expected inaccuracy. But $q_3$ is there implicitly. You can always get $q_3$ by calculating $1 - (q_1 + q_2)$, because $\q$ is a probability assignment. So we don&amp;rsquo;t actually need $q_3$ in the graph!&lt;/p&gt;

&lt;p&gt;Now, the red dot is the lowest point on the surface: the smallest possible expected inaccuracy, according to $\p$. But where is that in terms of $q_1$ and $q_2$? Let&amp;rsquo;s look at the same graph from directly above:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/accuracy/BrierEI3-above.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Hey! Looks like the red dot is located at $q_1 = 1/ 3$ and $q_2 = 1/ 3$, i.e. at $\q = (1/ 3, 1/ 3, 1/ 3)$. Also known as $\p$. So that&amp;rsquo;s promising: looks like expected inaccuracy is minimized when $\q = \p$, at least in this example.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s do one more example, $\p = (6/ 10, 3/ 10, 1/ 10)$. Then the expected Brier distance of each point $\q$ looks like this:
&lt;img src=&#34;http://jonathanweisberg.org/img/accuracy/BrierEI3-2.png&#34; alt=&#34;&#34; /&gt;
Or, taking the aerial view again:
&lt;img src=&#34;http://jonathanweisberg.org/img/accuracy/BrierEI3-2above.png&#34; alt=&#34;&#34; /&gt;
Yep, looks like the red dot is located at $q_1 = 6/ 10$ and $q_2 = 3/ 10$, i.e. at $\q = (6/ 10, 3/ 10, 1/ 10)$, also known as $\p$. So, once again, it seems expected inaccuracy is minimized when $\q = \p$.&lt;/p&gt;

&lt;p&gt;So let&amp;rsquo;s prove that that&amp;rsquo;s how it always is.&lt;/p&gt;

&lt;h2 id=&#34;a-proof&#34;&gt;A Proof&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;ll need a little notation: I&amp;rsquo;m going to write $\EIpq$ for the expected inaccuracy of point $\q$, according to $\p$.&lt;/p&gt;

&lt;p&gt;Now recall our formula for expected inaccuracy:
$$
\begin{align}
\EIpq
&amp;amp;= \quad p_1 \left( (q_1 - 1)^2 + q_2^2 + q_3^2 \right)\\&lt;br /&gt;
  &amp;amp;\quad + p_2 \left( q_1^2 + (q_2 - 1)^2 + q_3^2 \right)\\&lt;br /&gt;
  &amp;amp;\quad + p_3 \left( q_1^2 + q_2^2 + (q_3 - 1)^2 \right).
\end{align}
$$
How do we find the point $\q$ that minimizes this mess?&lt;/p&gt;

&lt;p&gt;Originally this post used some pretty tedious calculus. But thanks to a hot tip from &lt;a href=&#34;https://mathematics.stanford.edu/people/department-directory/name/jonathan-love/&#34; target=&#34;_blank&#34;&gt;Jonathan Love&lt;/a&gt;, we can get by just with algebra.&lt;/p&gt;

&lt;p&gt;First we need to expand the squares in our big ugly sum:
$$
\begin{align}
\EIpq
&amp;amp;= \quad p_1 \left( q_1^2 - 2q_1 + 1 + q_2^2 + q_3^2 \right)\\&lt;br /&gt;
  &amp;amp;\quad + p_2 \left( q_1^2 + q_2^2 - 2q_2 + 1 + q_3^2 \right)\\&lt;br /&gt;
  &amp;amp;\quad + p_3 \left( q_1^2 + q_2^2 + q_3^2 - 2q_3 + 1 \right).
\end{align}
$$
Then we&amp;rsquo;ll gather some common terms and rearrange things:
$$
\begin{align}
\EIpq &amp;amp;= (p_1 + p_2 + p_3)\left(q_1^2 + q_2^2 + q_3^2 + 1 \right) - 2p_1q_1 - 2p_2q_2 - 2p_3q_3.\\&lt;br /&gt;
\end{align}
$$
Since $p_1 + p_2 + p_3 = 1$, that simplifies to:
$$
\begin{align}
\EIpq &amp;amp;= q_1^2 + q_2^2 + q_3^2 + 1 - 2p_1q_1 - 2p_2q_2 - 2p_3q_3.\\&lt;br /&gt;
\end{align}
$$&lt;/p&gt;

&lt;p&gt;Now we&amp;rsquo;ll use &lt;a href=&#34;https://mathematics.stanford.edu/people/department-directory/name/jonathan-love/&#34; target=&#34;_blank&#34;&gt;Jonathan&lt;/a&gt;&amp;rsquo;s ingenious trick. We&amp;rsquo;re going to add $p_1^2 + p_2^2 + p_3^2 - 1$ to this expression, &lt;em&gt;which doesn&amp;rsquo;t change where the minimum occurs&lt;/em&gt;. If you shift every point on a graph upwards by the same amount, the minimum is still in the same place. (Imagine everybody in the world grows by an inch overnight; the shortest person in the world is still the shortest, despite being an inch taller.)&lt;/p&gt;

&lt;p&gt;Then, magically, we get an expression that factors into something tidy:
$$
\begin{align}
&amp;amp;\phantom{=}\phantom{=} p_1^2 + p_2^2 + p_3^2 + q_1^2 + q_2^2 + q_3^2 - 2p_1q_1 - 2p_2q_2 - 2p_3q_3\\&lt;br /&gt;
  &amp;amp;= (p_1 - q_1)^2 + (p_2 - q_2)^2 + (p_3 - q_3)^2.
\end{align}
$$
And not just tidy, but easy to minimize. It&amp;rsquo;s a sum of squares, and squares are never negative. So the smallest possible value  is $0$, which occurs when all the squares are $0$, i.e. when $q_1 = p_1$, $q_2 = p_2$, and $q_3 = p_3$.&lt;/p&gt;

&lt;p&gt;So, the minimum of $\EIpq$ occurs in the same place, namely when $\q = \p$!&lt;/p&gt;

&lt;h2 id=&#34;the-nth-dimension&#34;&gt;The Nth Dimension&lt;/h2&gt;

&lt;p&gt;Now we can use the same idea to generalize to any number of dimensions. Since the steps are essentially identical, I&amp;rsquo;ll keep it short and (I hope) sweet.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt;&amp;nbsp;
Given a probability assignment $\p = (p_1, \ldots, p_n)$, if inaccuracy is measured using Brier distance, then $\EIpq$ is uniquely minimized when $\q = \p$.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt;&amp;nbsp;
Let $\p = (p_1, \ldots, p_n)$ be a probability assignment, and let $\EIpq$ be the expected inaccuracy according to $\p$ of probability assignment $\q = (q_1, \ldots, q_n)$, measured using Brier distance.&lt;/p&gt;

&lt;p&gt;First we simplify our expression for $\EIpq$ using algebra:
$$
\begin{align}
\EIpq
&amp;amp;= \quad p_1 \left( (q_1 - 1)^2 + q_2^2 + \ldots + q_n^2 \right)\\&lt;br /&gt;
  &amp;amp;\quad + p_2 \left( q_1^2 + (q_2 - 1)^2 + \ldots + q_n^2 \right)\\&lt;br /&gt;
  &amp;amp;\quad\quad \vdots\\&lt;br /&gt;
  &amp;amp;\quad + p_n \left( q_1^2 + q_2^2 + \ldots + q_{n-1}^2 + (q_n - 1)^2 \right)\\&lt;br /&gt;
&amp;amp;= (p_1 + \ldots + p_n)\left( q_1^2 + \ldots + q_n^2 + 1\right) - 2 p_1 q_1 - \ldots - 2 p_n q_n\\&lt;br /&gt;
&amp;amp;= q_1^2 + \ldots + q_n^2 + 1 - 2 p_1 q_1 - \ldots - 2 p_n q_n.
\end{align}
$$
Now, because $p_1^2 + \ldots + p_n^2 - 1$ is a constant, adding it to $\EIpq$ doesn&amp;rsquo;t change where the minimum occurs. So we can minimize instead:
$$
\begin{align}
&amp;amp;\phantom{=}\phantom{=} p_1^2 + \ldots + p_n^2 + q_1^2 + \ldots + q_n^2 - 2 p_1 q_1 - \ldots - 2 p_n q_n\\&lt;br /&gt;
&amp;amp;= (p_1 - q_1)^2 + \ldots + (p_n - q_n)^2.
\end{align}
$$
Being a sum of squares, the minimum value here cannot be less than $0$, which occurs when $\q = \p$. &lt;span style=&#34;float: right;&#34;&gt;$\Box$&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;div class=&#34;text-center&#34;&gt;
&lt;object data=&#34;http://www.youtube.com/embed/MkmMxfCgewQ&#34;
   width=&#34;560&#34; height=&#34;315&#34; classboo=&#34;text-center&#34;&gt;&lt;/object&gt;
&lt;/div&gt;

&lt;p&gt;So what did we learn? That Brier distance isn&amp;rsquo;t just &amp;ldquo;stable&amp;rdquo; in toy cases like a coin-toss. It&amp;rsquo;s also stable in toy cases with any finite number of outcomes.&lt;/p&gt;

&lt;p&gt;No matter how many outcomes are under consideration, each probability assignment expects itself to do best at minimizing inaccuracy, if we use Brier distance to measure inaccuracy.&lt;/p&gt;

&lt;p&gt;To go beyond toy cases, we&amp;rsquo;d have to extend this result to cases with infinite numbers of possibilities. And I haven&amp;rsquo;t even begun to think about how to do that.&lt;/p&gt;

&lt;p&gt;Instead, next time we&amp;rsquo;ll look at what happens in $3+$ dimensions when we use Euclidean distance instead of Brier distance. And it&amp;rsquo;s actually kind of interesting! It turns out Euclidean distance is still improper in $3+$ dimensions, but not necessarily in the same way as in $2$ dimensions. More on that next time&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Accuracy for Dummies, Part 2: from Euclid to Brier</title>
      <link>http://jonathanweisberg.org/post/Accuracy%20for%20Dummies%20-%20Part%202/</link>
      <pubDate>Wed, 18 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/Accuracy%20for%20Dummies%20-%20Part%202/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;http://jonathanweisberg.org/post/Accuracy%20for%20Dummies%20-%20Part%201/&#34;&gt;Last time&lt;/a&gt; we saw that Euclidean distance is an &amp;ldquo;unstable&amp;rdquo; way of measuring inaccuracy. Given one assignment of probabilities, you&amp;rsquo;ll expect some other assignment to be more accurate (unless the first assignment is either perfectly certain or perfectly uncertain).&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s why accuraticians don&amp;rsquo;t use good ol&amp;rsquo; Euclidean distance.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://crookedrunbrewing.files.wordpress.com/2014/05/scientician.png?w=240&#34; alt=&#34;Just ask this accuratician&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Instead they use&amp;hellip; well, there are lots of alternatives. But the closest thing to a standard one is &lt;em&gt;Brier distance&lt;/em&gt;: the square of Euclidean distance.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s Euclid&amp;rsquo;s formula for the distance between two points $(a, b)$ and $(c, d)$ in the plane:
$$ \sqrt{ (a - c)^2 + (b - d)^2 }. $$
And here&amp;rsquo;s Brier&amp;rsquo;s:
$$ (a - c)^2 + (b - d)^2. $$
So, to get from Euclid to Brier, you just take away the square root.&lt;/p&gt;

&lt;p&gt;That makes a world of difference, it turns out. Brier distance isn&amp;rsquo;t unstable the way Euclidean distance is. But we&amp;rsquo;ll see that it&amp;rsquo;s enough like Euclidean distance to vindicate the argument for the laws of probability we began with last time.&lt;/p&gt;

&lt;p&gt;But first, a fun fact.&lt;/p&gt;

&lt;h1 id=&#34;fun-fact&#34;&gt;Fun Fact&lt;/h1&gt;

&lt;p&gt;Brier distance comes from the world of weather forecasting. Glenn W. Brier worked for the U. S. Weather Bureau, and in &lt;a href=&#34;http://docs.lib.noaa.gov/rescue/mwr/078/mwr-078-01-0001.pdf&#34; target=&#34;_blank&#34;&gt;a 1950 paper&lt;/a&gt; he proposed his formula as a way of measuring how well a weather forecaster is doing at predicting the weather.&lt;/p&gt;

&lt;p&gt;Suppose you say there&amp;rsquo;s a 70% chance of rain. If it does rain, you&amp;rsquo;re hardly wrong, but you&amp;rsquo;re not exactly right either. Brier suggested assessing a forecaster&amp;rsquo;s probabilities by taking the square of the difference from $1$ when it rains, and from $0$ when it doesn&amp;rsquo;t.&lt;/p&gt;

&lt;p&gt;Well, actually, he proposed taking the &lt;em&gt;average&lt;/em&gt; of those squares. But we&amp;rsquo;ll follow the recent philosophical literature and keep it simple: we&amp;rsquo;ll just use the sum of squares rather than its average.&lt;/p&gt;

&lt;p&gt;Now on to the substance. Two facts about Brier distance make it useful as a replacement for Euclidean distance.&lt;/p&gt;

&lt;h1 id=&#34;euclid-and-brier-are-ordinally-equivalent&#34;&gt;Euclid and Brier are Ordinally Equivalent&lt;/h1&gt;

&lt;p&gt;First, Brier distance is &lt;em&gt;ordinally equivalent&lt;/em&gt; to Euclidean distance. Meaning: whenever a distance is larger according to Euclid, it&amp;rsquo;s larger according to Brier too. And vice versa.&lt;/p&gt;

&lt;p&gt;How do we know that? Because Brier is just Euclid squared, and squaring a larger number always results in a larger number (for positive numbers like distances, anyway). If $D$ is the distance from Toronto to the sun, and $d$ is the distance from Toronto to the moon, then $D^2 &amp;gt; d^2$. It&amp;rsquo;s further to the sun than to the moon, both in terms of Brier distance and Euclidean distance.&lt;/p&gt;

&lt;p&gt;So, when we&amp;rsquo;re comparing distances from the truth, Brier distance behaves a lot like Euclidean distance. In particular, what we learned from our opening diagram about Euclidean distance holds for Brier distance, too.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/accuracy/2D%20Dominance%20Diagram%20-%20400px.png&#34; alt=&#34;Opening diagram&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Not only is $c&amp;rsquo;$ closer to both vertices than $c^*$ in Euclidean terms, it&amp;rsquo;s also closer in terms of Brier distance.&lt;/p&gt;

&lt;h1 id=&#34;brier-is-stable&#34;&gt;Brier is Stable&lt;/h1&gt;

&lt;p&gt;Second, Brier distance doesn&amp;rsquo;t lead to the kind of instability that made Euclidean distance problematic. To see why, let&amp;rsquo;s rerun our expected inaccuracy calculations from last time, but using Brier distance instead of Euclid.&lt;/p&gt;

&lt;p&gt;Suppose your credences in Heads and Tails are $p$ and $1-p$. What&amp;rsquo;s the expected inaccuracy of having some credence $q$ in Heads, and $1-q$ in Tails?&lt;/p&gt;

&lt;p&gt;Well, the Brier distance between $(q, 1-q)$ and $(1,0)$ is:
$$(q - 1)^2 + ((1-q) - 0)^2.$$
And the Brier distance between $(q, 1-q)$ and $(0,1)$ is:
$$(q - 0)^2 + ((1-q) - 1)^2.$$
We don&amp;rsquo;t know which of $(1,0)$ or $(0,1)$ is the &amp;ldquo;true&amp;rdquo; one. But we have assigned them the probabilities $p$ and $1-p$, respectively. So we can calculate the expected inaccuracy of $(q, 1-q)$, written $EI(q, 1-q)$:
$$
\begin{align}
EI(q, 1-q) &amp;amp;= p \left( (q - 1)^2 + ((1-q) - 0)^2 \right)\\&lt;br /&gt;
  &amp;amp;\quad + (1-p) \left( (q - 0)^2 + ((1-q) - 1)^2 \right)\\&lt;br /&gt;
  &amp;amp;= 2 p (1 - q)^2 + 2(1-p) q^2\\&lt;br /&gt;
  &amp;amp;= 2 p q^2 - 4pq + 2p + 2q^2 - 2pq^2\\&lt;br /&gt;
  &amp;amp;= 2q^2 - 4pq + 2p
\end{align}
$$
Now that last line might look like a mess. But it&amp;rsquo;s really just a quadratic equation, where the variable is $q$. Remember: we&amp;rsquo;re treating $p$ as a constant since that&amp;rsquo;s the credence you hold. And we&amp;rsquo;re looking at potential values of $q$ to see which ones minimize the quantity $EI(q, 1-q)$, given a fixed credence of $p$ in heads.&lt;/p&gt;

&lt;p&gt;So which value of $q$ minimizes this quadratic formula? You might remember from algebra class that a quadratic equation of the form:
$$
ax^2 + bx + c
$$
is a parabola, with the bottom of the bowl located at $x = -b/2a$. (Or, if you know some calculus, you can take the derivative and set it equal to $0$. Since the derivative here is $2ax + b$, setting it equal to $0$ yields, again, $x = -b/2a$.)&lt;/p&gt;

&lt;p&gt;In the case of our formula, we have $a = 2$ and $b = -4p$. So the minimum happens when $q = 4p/4 = p$. In other words, given credence $p$ in heads, expected inaccuracy is minimized by sticking with that same credence, i.e. assigning $q = p$.&lt;/p&gt;

&lt;p&gt;So, to complement our result about Euclidean distance from last time, we have a&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt;&amp;nbsp; Suppose $p \in [0,1]$. Then, according to the probability assignment $(p, 1-p)$, the expected Brier distance of any alternative assignment $(q, 1-q)$ from the points $(1,0)$ and $(0,1)$ is uniquely minimized when $p = q$.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt;&amp;nbsp; Scroll up! &lt;span style=&#34;float: right;&#34;&gt;$\Box$&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&#34;proper-scoring-rules&#34;&gt;Proper Scoring Rules&lt;/h1&gt;

&lt;p&gt;When a measure of inaccuracy is stable like this, it&amp;rsquo;s called &lt;em&gt;proper&lt;/em&gt; (or sometimes: &lt;em&gt;immodest&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;There are lots of other proper ways of measuring inaccuracy besides Brier. But Brier tends to be the default among philosophers writing in the accuracy framework, at least as a working example. Why?&lt;/p&gt;

&lt;p&gt;My impression (though I&amp;rsquo;m no guru) is that it&amp;rsquo;s the default because:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Brier is a lot like Euclidean distance, as we saw. So it&amp;rsquo;s easier and more intuitive to work with than some of the alternatives.&lt;/li&gt;
&lt;li&gt;Brier tends to be representative of other proper/immodest rules. If you discover something philosophically interesting using Brier, there&amp;rsquo;s a good chance it holds for many other proper scoring rules.&lt;/li&gt;
&lt;li&gt;Brier has other nice mathematical properties which, according to authors like Richard Pettigrew, make it The One True Measure of Inaccuracy. (It may have some odd features too, though: see &lt;a href=&#34;http://m-phi.blogspot.ca/2015/03/a-strange-thing-about-brier-score.html&#34; target=&#34;_blank&#34;&gt;this post&lt;/a&gt; by Brian Knab and Miriam Schoenfield, for example.)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;How does our starting argument for the laws of total probability fare if we use other proper scoring rules, besides Brier? Really well, it turns out!&lt;/p&gt;

&lt;p&gt;The key fact our diagram illustrates doesn&amp;rsquo;t just hold for Euclidean distance and Brier distance. Speaking &lt;em&gt;very&lt;/em&gt; loosely: it holds on any proper way of measuring distance (but do see sections 8 and 9 of &lt;a href=&#34;https://philpapers.org/rec/JOYAAC&#34; target=&#34;_blank&#34;&gt;Joyce&amp;rsquo;s 2009&lt;/a&gt; for the details before getting carried away with this generalization; or see Theorem 4.3.5 of &lt;a href=&#34;https://global.oup.com/academic/product/accuracy-and-the-laws-of-credence-9780198732716&#34; target=&#34;_blank&#34;&gt;Pettigrew 2016&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Proving that requires grinding through a good deal of math, though. So in these posts we&amp;rsquo;re going to stick with Brier distance, at least for a while.&lt;/p&gt;

&lt;h1 id=&#34;begging-the-question&#34;&gt;Begging the Question?&lt;/h1&gt;

&lt;p&gt;We started these posts with an illustration of an influential argument for the laws of probability. But we quickly switched to &lt;em&gt;assuming&lt;/em&gt; those very same laws in the arguments that followed.&lt;/p&gt;

&lt;p&gt;For example, to illustrate the instability of Euclidean distance, I chose a point on the diagonal of our diagram, $(.6, .4)$. And in the theorem that generalized that example, I assumed probabilistic assignments like $(p, 1-p)$ and $(q, 1-q)$, which add up to $1$.&lt;/p&gt;

&lt;p&gt;So didn&amp;rsquo;t we beg the question when we motivated switching from Euclid to Brier?&lt;/p&gt;

&lt;p&gt;To some extent: yes. We are assuming that reasonable ways of measuring inaccuracy can&amp;rsquo;t be so hostile to the laws of probability that they make almost all probability assignments unstable.&lt;/p&gt;

&lt;p&gt;But also: no. We aren&amp;rsquo;t assuming that the laws of probability are absolute and inviolable, just that they&amp;rsquo;re reasonable &lt;em&gt;sometimes&lt;/em&gt;. Euclidean distance would rule out probabilistic credences on pretty much all occasions. So it conflicts with the very modest thought that following the laws of probability is &lt;em&gt;occasionally&lt;/em&gt; reasonable. So, even if you&amp;rsquo;re just a little bit open to the idea of probability theory, Euclidean distance will seem pretty unfriendly.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Perhaps most importantly, though: the motivation I&amp;rsquo;ve given you here for moving from Euclid to Brier isn&amp;rsquo;t the official one you&amp;rsquo;ll find in an actual, bottom-up argument for probability theory, like &lt;a href=&#34;https://richardpettigrew.wordpress.com/accuracy-book/&#34; target=&#34;_blank&#34;&gt;Richard Pettigrew&amp;rsquo;s&lt;/a&gt;. His argument starts from a much more abstract place. He starts with axioms that any measure of inaccuracy must obey, and then narrows things down to Brier.&lt;/p&gt;

&lt;p&gt;So there&amp;rsquo;s the official story and the unofficial story. This post gives you the unofficial story, to help you get started. Because the official story is often really hard to understand. Not only is the math way more abstract, but the philosophical motivations are often hard to suss out. Because&amp;mdash;and this is just between you and me now&amp;mdash;the people telling the official story actually started out with the unofficial story, and then worked backwards until they came up with an officially respectable story that doesn&amp;rsquo;t beg the question quite so obviously.&lt;/p&gt;

&lt;p&gt;Ok, that&amp;rsquo;s unfair. Here&amp;rsquo;s a more even-handed (and better-informed) way of putting it, from &lt;a href=&#34;http://ndpr.nd.edu/news/70705-accuracy-and-the-laws-of-credence/&#34; target=&#34;_blank&#34;&gt;Kenny Easwaran&amp;rsquo;s review&lt;/a&gt; of Pettigrew&amp;rsquo;s book:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Some philosophers have a vision of what they do as starting from unassailable premises, and giving an ironclad argument for a conclusion. However, I think we&amp;rsquo;ve all often seen cases where these arguments are weaker than they seem to the author, and with the benefit of a bit of distance, one can often recognize how the premises were in fact motivated by an attempt to justify the conclusion, which was chosen in advance. Pettigrew avoids the charade of pretending to have come up with the premises independently of recognizing that they lead to the conclusions of his arguments. Instead, he is open about having chosen target conclusions in advance [&amp;hellip;] and investigated what collection of potentially plausible principles about accuracy and epistemic decision theory will lead to those conclusions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;This argument is essentially drawn from &lt;a href=&#34;https://philpapers.org/rec/JOYAAC&#34; target=&#34;_blank&#34;&gt;(Joyce 2009)&lt;/a&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Accuracy for Dummies, Part 1: Euclid Improper</title>
      <link>http://jonathanweisberg.org/post/Accuracy%20for%20Dummies%20-%20Part%201/</link>
      <pubDate>Fri, 13 Jan 2017 09:53:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/Accuracy%20for%20Dummies%20-%20Part%201/</guid>
      <description>

&lt;p&gt;If you&amp;rsquo;ve bumped into &lt;a href=&#34;https://plato.stanford.edu/entries/epistemic-utility/#AccArg&#34; target=&#34;_blank&#34;&gt;the accuracy framework&lt;/a&gt; before, you&amp;rsquo;ve probably seen a diagram like this one:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/accuracy/2D Dominance Diagram - 400px.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The vertices $(1,0)$ and $(0,1)$ represent two possibilities, whether a coin lands heads or tails in this example.&lt;/p&gt;

&lt;p&gt;According to the laws of probability, the probability of heads and of tails must add up to $1$, like $.3 + .7$ or $.5 + .5$. So the diagonal line connecting the two vertices covers all the possible probability assignments&amp;hellip;  $(0,1)$, $(.3,.7)$, $(.5, .5)$, $(.9, .1)$, $(1,0)$, etc.$\newcommand{\vone}{(1,0)}$$\newcommand{\vtwo}{(0,1)}$&lt;/p&gt;

&lt;p&gt;The diagram illustrates a key fact of the accuracy framework. Assignments that obey the laws of probability are always &amp;ldquo;closer to the truth&amp;rdquo; than assignments that violate those laws&amp;mdash;&lt;em&gt;no matter what the truth turns out to be&lt;/em&gt;.  Given any point &lt;em&gt;not&lt;/em&gt; on the line, there is a point &lt;em&gt;on&lt;/em&gt; the line that is closer to &lt;em&gt;both&lt;/em&gt; vertices $\vone$ and $\vtwo$. So, whether the coin lands heads or tails, you&amp;rsquo;ll be closer to the truth if your degrees of belief (a.k.a. &amp;ldquo;credences&amp;rdquo;) obey the laws of probability.&lt;/p&gt;

&lt;p&gt;Take $c^*$, for example, which doesn&amp;rsquo;t lie on the diagonal line. Let&amp;rsquo;s assume $c^*$ is the point $(.7, .5)$, which violates the laws of probability: $.7 + .5 &amp;gt; 1$. Now compare that to $c&amp;rsquo;$, which does lie on the diagonal line. That&amp;rsquo;s the point $(.6, .4)$, which does obey the laws of probability: $.6 + .4 = 1$.&lt;/p&gt;

&lt;p&gt;Well, $c&amp;rsquo;$ is closer to $\vone$ than $c^*$ is. Just look at the right-triangle connecting all three points: to get to $\vone$ from $c^*$ you have to travel along the hypotenuse. But you only have to travel the distance of one of the legs to get there from $c&amp;rsquo;$. And the same thinking applies to the other vertex, $\vtwo$. So $c&amp;rsquo;$ is closer to both vertices than $c^*$ is.&lt;/p&gt;

&lt;p&gt;The same idea applies to &lt;em&gt;any&lt;/em&gt; point in the unit square. If it&amp;rsquo;s not on the diagonal line, there&amp;rsquo;s a point on the line that will be closer to both vertices&amp;mdash;because Pythagoras. Just go from whatever $c^*$ you start with to the closest point $c&amp;rsquo;$ on the diagonal. You&amp;rsquo;ll have two right-triangles, one for  each vertex. So the $c&amp;rsquo;$ point will be closer to both vertices than the $c^*$ point you started with.&lt;/p&gt;

&lt;p&gt;What&amp;rsquo;s more, no point on the line is closer to both vertices than any other. For example, if you move from $(.5, .5)$ to some other point on the line, you&amp;rsquo;ll move towards one vertex but away from the other. So if you&amp;rsquo;re off the line, you can always get closer to both vertices by moving onto the line. But once you&amp;rsquo;re on the line, there&amp;rsquo;s no way that guarantees you&amp;rsquo;ll be closer to the vertex representing the true outcome of the coin toss.&lt;/p&gt;

&lt;p&gt;And that&amp;rsquo;s why you should obey the laws of probability, according to advocates of the accuracy framework. Violating the laws of probability takes you away from the truth, no matter what the truth turns out to be. Whereas if you obey the laws of probability, that doesn&amp;rsquo;t happen.&lt;/p&gt;

&lt;h1 id=&#34;hey-dummy&#34;&gt;Hey, Dummy&lt;/h1&gt;

&lt;p&gt;If you&amp;rsquo;re like me, the first time you see this argument you think to yourself: &amp;ldquo;Cool! The diagram gives me the key idea, I&amp;rsquo;ll worry about mathematical technicalities later (like, what if there are more than two possibilities?). For now let me just see where you&amp;rsquo;re going with this, epistemology-wise&amp;hellip;&amp;rdquo;&lt;/p&gt;

&lt;p&gt;But when I finally did sit down to work through the math, I found it much harder than I expected to answer some elementary questions. The answers to these questions were usually taken for granted in published work, or they went by so fast I wasn&amp;rsquo;t sure about the details.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m still working on filling in a lot of these gaps, as I work through &lt;a href=&#34;https://global.oup.com/academic/product/accuracy-and-the-laws-of-credence-9780198732716?cc=ca&amp;amp;lang=en&amp;amp;&#34; target=&#34;_blank&#34;&gt;Richard Pettigrew&amp;rsquo;s excellent new book&lt;/a&gt; and get up to speed (I hope!) with the latest research. I&amp;rsquo;m writing these posts to help me get clear on the basics, and hopefully help you do the same.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s-media-cache-ak0.pinimg.com/736x/2e/46/00/2e4600f7eab945f936f00548b5498ba4.jpg&#34; alt=&#34;Dennis Duffy: Hey Dummy&#34; /&gt;&lt;/p&gt;

&lt;p&gt;(Warning: since I&amp;rsquo;m learning this stuff as I go, my solutions and proofs won&amp;rsquo;t always be the best. In fact they&amp;rsquo;re bound to have errors. So I encourage you to contact me with corrections, and help improve these posts for others.)&lt;/p&gt;

&lt;p&gt;Now on to today&amp;rsquo;s topic: Euclidean distance as a measure of accuracy.&lt;/p&gt;

&lt;h1 id=&#34;fear-of-a-euclidean-plane&#34;&gt;Fear of a Euclidean Plane&lt;/h1&gt;

&lt;p&gt;We just saw that the laws of probability keep you close to the truth in our coin-toss example, whatever the truth turns out to be. And by &amp;ldquo;close&amp;rdquo; we meant Euclidean distance, the kind of spatial distance familiar from grade-school geometry.&lt;/p&gt;

&lt;p&gt;But people writing in the accuracy framework never use Euclidean distance. Why not? Because, it turns out, Euclidean distance is unstable!&lt;/p&gt;

&lt;p&gt;&amp;ldquo;Unstable&amp;rdquo; how?&lt;/p&gt;

&lt;p&gt;Well, if your aim is to be as close to the truth as possible in terms of Euclidean distance, then you will almost always be driven to change your opinion to something extreme: either $(1,0)$ or $(0,1)$. And not because you get some definite information about how the coin-flip turns out. But just because of the way Euclidean distance interacts with &lt;a href=&#34;https://plato.stanford.edu/entries/rationality-normative-utility/#DefExpUti&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;expected value&lt;/em&gt;&lt;/a&gt;. (I&amp;rsquo;m going to assume you&amp;rsquo;re familiar with the notion of expected value. If not, you can read &lt;a href=&#34;(https://plato.stanford.edu/entries/rationality-normative-utility/#DefExpUti)&#34; target=&#34;_blank&#34;&gt;the linked section&lt;/a&gt; of the &lt;em&gt;SEP&lt;/em&gt; article or do a bit of googling.)&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s how that happens. Suppose your credences in heads/tails are $(.6, .4)$: you&amp;rsquo;re $60\%$ confident the coin will land heads, and $40\%$ confident it&amp;rsquo;ll land tails. What&amp;rsquo;s your &lt;em&gt;expected inaccuracy&lt;/em&gt;, then? If we think of accuracy as utility, and thus inaccuracy as disutility, how well can you expect to do by holding your current state of opinion?&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s run the calculation. We&amp;rsquo;ll write $EI(x, 1-x)$ for the expected inaccuracy of having credence $x$ in heads and $1-x$ in tails.
$$
\begin{align}
EI(.6, .4) &amp;amp;= .6 \sqrt{(.6 - 1)^2 + (.4 - 0)^2} + .4 \sqrt{(.6 - 0)^2 + (.4 - 1)^2}\\&lt;br /&gt;
&amp;amp;= .6 \sqrt{(-.4)^2 + .4^2} + .4 \sqrt{.6^2 + (-.6)^2}\\&lt;br /&gt;
&amp;amp;= .678823
\end{align}
$$
Ok, not bad. But now let&amp;rsquo;s compare that to how you can expect to do if you change your opinion to the extreme state $(1,0)$:
$$
\begin{align}
EI(1, 0) &amp;amp;= .6 \sqrt{(1 - 1)^2 + (0 - 0)^2} + .4 \sqrt{(1 - 0)^2 + (0 - 1)^2}\\&lt;br /&gt;
&amp;amp;= .565685
\end{align}
$$
Some things to keep in mind here:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The numbers outside the square root symbols are $.6$ and $.4$ because those are your current beliefs, and we&amp;rsquo;re asking how well you expect to do &lt;em&gt;according to your current beliefs&lt;/em&gt;.

&lt;ul&gt;
&lt;li&gt;The numbers inside the square roots are $1$ and $0$ because we&amp;rsquo;re asking how well you expect to do by adopting those extreme opinions. So those numbers describe the outcomes whose inaccuracy we want to evaluate and weigh.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Remember, &lt;strong&gt;smaller&lt;/strong&gt; numbers are &lt;strong&gt;better&lt;/strong&gt; because we&amp;rsquo;re talking about &lt;strong&gt;in&lt;/strong&gt;accuracy.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;And look: the extreme opinion $(1, 0)$ does &lt;em&gt;better&lt;/em&gt; than the more moderate opinion you actually hold, $(.6, .4)$. The extreme opinion has lower expected inaccuracy (think: higher expected accuracy).&lt;/p&gt;

&lt;p&gt;In fact, the extreme assignment does better than the moderate one &lt;em&gt;according to the moderate assignment itself&lt;/em&gt;. So your moderate opinions end up undermining themselves. They drive you to hold more extreme opinions than you initially do, in the name of accuracy.&lt;/p&gt;

&lt;p&gt;This isn&amp;rsquo;t an artifact of the particular example $(.6, .4)$. We can prove that an extreme state of opinion always does best in terms of expected inaccuracy&amp;mdash;unless you are completely uncertain about the outcome, i.e. $(.5,.5)$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt;&amp;nbsp;
Suppose $p \in [0, 1]$ and $p \neq .5$. Then, according to the probability assignment $(p, 1-p)$, the expected Euclidean distance of any alternative assignment $(q, 1-q)$ from the points $(1,0)$ and $(0,1)$ is uniquely minimized by:
$$
q = \begin{cases}
0 &amp;amp; \mbox{ if } p &amp;lt; .5,\\&lt;br /&gt;
1 &amp;amp; \mbox{ if } p &amp;gt; .5.
\end{cases}
$$&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt;&amp;nbsp;
Suppose $0 \leq p \leq 1$ and $p \neq .5$. According to the probability assignment $(p, 1-p)$, the expected Euclidean distance from $(1,0)$ and $(0,1)$ of any alternative assignment $(q, 1-q)$ is:
$$
\begin{align}
EI(q, 1-q) &amp;amp;= p \sqrt{(q - 1)^2 + ((1-q) - 0)^2}\\&lt;br /&gt;
&amp;amp;\quad + (1-p) \sqrt{(q - 0)^2 + ((1-q) - 1)^2}\\&lt;br /&gt;
&amp;amp;= p \sqrt{(q - 1)^2 + (1 - q)^2} + (1-p) \sqrt{q^2 + q^2}\\&lt;br /&gt;
&amp;amp;= p \sqrt{2} (1 - q) + (1-p) \sqrt{2} q\\&lt;br /&gt;
&amp;amp;= \sqrt{2} \left( p (1 - q) + (1-p) q \right).
\end{align}
$$
We are looking for the value of $q$ that minimizes the quantity on the last line, which is the same if we drop the $\sqrt{2}$ and just seek to minimize:
$$ p (1 - q) + (1-p) q. $$
This quantity is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Weighted_arithmetic_mean&#34; target=&#34;_blank&#34;&gt;weighted average&lt;/a&gt; of the two values $p$ and $(1-p)$, with the weights being $1-q$ and $q$, respectively. So the minimum possible value is just whichever of $p$ or $1-p$ is smaller. And this minimum is achieved when all the weight is given to the smaller value.&lt;/p&gt;

&lt;p&gt;So, if $p &amp;lt; .5$, then the minimum possible value is $p$, and it is achieved when $1 - q = 1$, and thus $q = 0$. If instead $p &amp;gt; .5$, the minimum possible value is $1 - p$ and is achieved when $q = 1$.
&lt;span style=&#34;float: right;&#34;&gt;$\Box$&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Based on this proof you can also see what happens when $p=.5$. It doesn&amp;rsquo;t matter what value $q$ takes: any value $0 \leq q \leq 1$ will result in the same expected inaccuracy, namely $.5\sqrt{2}$.&lt;/p&gt;

&lt;p&gt;So here&amp;rsquo;s the problem with Euclidean distance as a way of measuring inaccuracy. As soon as you find yourself leaning one way or another on heads-vs.-tails, you&amp;rsquo;re driven to extremes. If you get information that makes heads slightly more likely, say $.51$ for example, your expected inaccuracy is minimized by leaping to the conclusion that the coin will certainly come up heads.&lt;/p&gt;

&lt;p&gt;So any probability assignment to heads/tails besides $(.5, .5)$ is self-undermining. It gives you cause to adopt some other assignment&amp;mdash;an extreme one, at that.&lt;/p&gt;

&lt;p&gt;Even at $(.5, .5)$ things aren&amp;rsquo;t so happy, btw. Any other assignment of probabilities is just as good as far as minimizing inaccuracy goes. So even if the pursuit of accuracy doesn&amp;rsquo;t &lt;em&gt;require&lt;/em&gt; you to change your opinion, it still &lt;em&gt;permits&lt;/em&gt; you to do so. As far as accuracy goes, being indifferent about the coin toss also makes you indifferent about what opinion to hold. Which is pretty strange in itself.&lt;/p&gt;

&lt;h1 id=&#34;where-this-leaves-us&#34;&gt;Where This Leaves Us&lt;/h1&gt;

&lt;p&gt;Wait a minute: if Euclidean distance is a bad way to measure inaccuracy, then what&amp;rsquo;s the use of the diagram we started with?? And what&amp;rsquo;s the right way to measure inaccuracy?&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll tackle these questions in the next post. But here&amp;rsquo;s the short answer.&lt;/p&gt;

&lt;p&gt;One common way of measuring inaccuracy is a variation on Euclidean distance called &lt;em&gt;Brier&lt;/em&gt; distance. Brier distance is just enough like Euclidean distance to vindicate the reasoning we did with our opening diagram. But it&amp;rsquo;s different enough from Euclidean distance to avoid the instability problem we ended up with.&lt;/p&gt;

&lt;p&gt;So what is Brier distance? It&amp;rsquo;s just the square of Euclidean distance. Just take the square root symbol off Euclid&amp;rsquo;s formula and you&amp;rsquo;ve got the formula for Brier distance. Next time we&amp;rsquo;ll see how that one change makes all the right differences.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>