<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Philosophy on Jonathan Weisberg</title>
    <link>http://jonathanweisberg.org/tags/philosophy/index.xml</link>
    <description>Recent content in Philosophy on Jonathan Weisberg</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="http://jonathanweisberg.org/tags/philosophy/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Crash Course in Inductive Logic</title>
      <link>http://jonathanweisberg.org/post/inductive-logic/</link>
      <pubDate>Tue, 19 Nov 2019 00:00:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/inductive-logic/</guid>
      <description>

&lt;p&gt;There are four ways things can turn out with two flips of a coin:
$$ HH, \quad HT, \quad TH, \quad TT.$$
If we know nothing about the coin&amp;rsquo;s tendencies, we might assign equal probability to each of
these four possible outcomes:
$$ Pr(HH) = Pr(HT) = Pr(TH) = Pr(TT) = 1/ 4. $$
But from another point of view, there are primarily three possibilities. If we ignore order,
the possible outcomes are $0$ heads, $1$ head, or $2$ heads. So we might
instead assign equal probability to these three outcomes, then divide
the middle $1/ 3$ evenly between $HT$ and $TH$: $$
  Pr(HH) = 1/3  \qquad  Pr(HT) = Pr(TH) = 1/6  \qquad  Pr(TT) = 1/ 3.
$$&lt;/p&gt;

&lt;p&gt;This two-stage approach may seem odd. But it&amp;rsquo;s actually friendlier
from the point of view of inductive reasoning. On the first
scheme, a heads on the first toss doesn&amp;rsquo;t increase the probability of
another heads. It stays fixed at $1/ 2$:
$$
  \newcommand{\p}{Pr}
  \newcommand{\given}{\mid}
  \renewcommand{\neg}{\mathbin{\sim}}
  \renewcommand{\wedge}{\mathbin{\text{&amp;amp;}}}
  \p(HH \given H) = \frac{1/ 4}{1/ 4 + 1/ 4} = \frac{1}{2}.
$$
Whereas it does increase on the second strategy, from $1/ 2$ to $2/ 3$:
$$ \p(HH \given H) = \frac{1/ 3}{1/ 3 + 1/ 6} = \frac{2}{3}. $$
The two-stage approach thus learns from experience, where the
single-step division is skeptical about induction.&lt;/p&gt;

&lt;p&gt;This holds true as we increase the number of flips. If we do three tosses
for example, we&amp;rsquo;ll find that $\p(HHH \given HH) = 3/ 4$ on the
two-stage analysis. Whereas this probability stays stubbornly fixed at
$1/ 2$ on the first approach. It won&amp;rsquo;t budge no matter how many heads
we observe, so we can&amp;rsquo;t learn anything about the coin&amp;rsquo;s bias this way.&lt;/p&gt;

&lt;p&gt;This is the difference between Carnap&amp;rsquo;s famous account of induction, from
his 1950 book &lt;em&gt;Logical Foundations of Probability&lt;/em&gt;, and the account
he finds &lt;a href=&#34;http://www.kfs.org/jonathan/witt/t515en.html&#34; target=&#34;_blank&#34;&gt;in Wittgenstein&amp;rsquo;s
&lt;em&gt;Tractatus&lt;/em&gt;&lt;/a&gt;. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:peirce&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:peirce&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; Although
Carnap had actually been scooped by W. E. Johnson, who worked out a similar
analysis about $25$ years earlier.&lt;/p&gt;

&lt;p&gt;This is a short explainer on some key elements of inductive logic worked out by
Johnson and Carnap and the place of those ideas in the story of
inductive logic.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://jonathanweisberg.org/pdf/inductive-logic.pdf&#34;&gt;PDF version here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;states-structures&#34;&gt;States &amp;amp; Structures&lt;/h1&gt;

&lt;p&gt;Carnap calls a fine-grained specification like $TH$ a &lt;em&gt;state-description&lt;/em&gt;.
The coarser grained &amp;ldquo;$1$ head&amp;rdquo; is a &lt;em&gt;structure-description&lt;/em&gt;. A
state-description specifies which flips land heads, and which tails.
While a structure-description specifies &lt;em&gt;how many&lt;/em&gt; land heads and tails,
without necessarily saying which.&lt;/p&gt;

&lt;p&gt;It needn&amp;rsquo;t be coin flips landing heads or tails, of course. The same
ideas apply to any set of objects or events, and any feature they might
have or lack.&lt;/p&gt;

&lt;p&gt;Suppose we have two objects $a$ and $b$, each of which might have some
property $F$. Working for a moment as Carnap did, in
first-order logic, here is an example of a structure-description:
$$ (Fa \wedge \neg Fb) \vee (\neg Fa \wedge Fb). $$ But this isn&amp;rsquo;t a
state-description, since it doesn&amp;rsquo;t specify which object has $F$. It
only says how many objects have $F$, namely $1$. One of the disjuncts alone would be a
state-description though:
$$ Fa \wedge \neg Fb. $$&lt;/p&gt;

&lt;p&gt;Carnap&amp;rsquo;s initial idea was that all structure-descriptions start out with the
same probability. These probabilities are then divided equally among
the state-descriptions that make up a structure-description.&lt;/p&gt;

&lt;p&gt;For example, if we do three flips, there are four
structure-descriptions: $0$ heads, $1$ head, $2$ heads, and $3$ heads.
Some of these have only one state-description. For example, there&amp;rsquo;s only
one way to get $0$ heads, namely $TTT$. So $$ \p(TTT) = 1/ 4. $$ But
others have multiple state-descriptions. There are three ways to get $1$
head for example, so we divide $1/ 4$ between them:
$$ \p(HTT) = \p(THT) = \p(TTH) = 1/ 12. $$&lt;/p&gt;

&lt;p&gt;The effect is that more homogeneous sequences start out more probable.
There&amp;rsquo;s only one way to get all heads, so the $HH$ state-description
inherits the full probability of the corresponding &amp;ldquo;$2$ heads&amp;rdquo;
structure-description. But a $50$-$50$ split has multiple permutations,
each of which inherits only a portion of the same quantum of
probability. A heterogeneous sequence of heads and tails thus starts out
less probable than a homogeneous one.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s why the two-stage analysis is induction-friendly. It effectively
builds Hume&amp;rsquo;s &amp;ldquo;uniformity of nature&amp;rdquo; assumption into the prior
probabilities.&lt;/p&gt;

&lt;h1 id=&#34;the-rule-of-succession&#34;&gt;The Rule of Succession&lt;/h1&gt;

&lt;p&gt;The two-stage assignment also yields a very simple formula for induction:
Laplace&amp;rsquo;s famous Rule of Succession. (Derivation in the Appendix.)&lt;/p&gt;

&lt;dl&gt;
&lt;dt&gt;The Rule of Succession&lt;/dt&gt;
&lt;dd&gt;&lt;p&gt;Given $k$ heads out of $n$ observed flips, the probability of heads
on a subsequent toss is $$\frac{k+1}{n+2}.$$&lt;/p&gt;&lt;/dd&gt;
&lt;/dl&gt;

&lt;p&gt;Laplace arrived at this rule about $150$ years earlier by somewhat
different means. But there is a strong similarity.&lt;/p&gt;

&lt;p&gt;Laplace supposed that our coin has some fixed, but unknown, chance $p$
of landing heads on each toss. Suppose we regard all possible values
$0 \leq p \leq 1$ as equally likely.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; If we then update our beliefs
about the true value of $p$ using Bayes&amp;rsquo; theorem, we arrive at the Rule
of Succession. (Proving this is a bit involved. Maybe I&amp;rsquo;ll go over it
another time.)&lt;/p&gt;

&lt;p&gt;The two-stage way of assigning prior probabilities is essentially the same
idea, just applied in a discrete setting. By treating all
structure-descriptions as equiprobable, we make all possible frequencies
of heads equiprobable. This is a discrete analogue of treating all
possible values of $p$ as equiprobable.&lt;/p&gt;

&lt;h1 id=&#34;the-continuum-of-inductive-methods&#34;&gt;The Continuum of Inductive Methods&lt;/h1&gt;

&lt;p&gt;Both Johnson and Carnap eventually realized that the two methods of assigning priors we&amp;rsquo;ve
considered are just two points on a larger continuum.&lt;/p&gt;

&lt;dl&gt;
&lt;dt&gt;The $\lambda$ Continuum&lt;/dt&gt;
&lt;dd&gt;&lt;p&gt;Given $k$ heads out of $n$ observed flips, the probability of heads
on a subsequent toss is $$\frac{k + \lambda/2}{n + \lambda},$$ for
some $\lambda$ in the range $0 \leq \lambda \leq \infty$.&lt;/p&gt;&lt;/dd&gt;
&lt;/dl&gt;

&lt;p&gt;What value should $\lambda$ take here? Notice we get the Rule of
Succession if $\lambda = 2$. And we get inductive skepticism if we let
$\lambda$ approach $\infty$. For then $k$ and $n$ fall away and the
ratio converges to $1/ 2$, no matter what $k$ and $n$ are.&lt;/p&gt;

&lt;p&gt;If we set $\lambda = 0$, we get a formula we haven&amp;rsquo;t discussed yet:
$k/n$. Reichenbach called this the Straight Rule. (In modern statistical
parlance it&amp;rsquo;s the &amp;ldquo;maximum likelihood estimate.&amp;rdquo;)&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;The overall pattern is: the higher $\lambda$, the more &amp;ldquo;cautious&amp;rdquo; our
inductive inferences will be. A larger $\lambda$ means less influence
from $k$ and $n$: the probability of another heads stays closer to the
initial value of $1/ 2$. In the extreme case where $\lambda = \infty$,
it stays stuck at exactly $1/ 2$ forever.&lt;/p&gt;

&lt;p&gt;A low value of $\lambda$, on the other hand, will make our inferences
more ambitious. In the extreme case $\lambda = 0$, we jump immediately
to the observed frequency. Our expectation about the next toss is just
$k/n$, the frequency we&amp;rsquo;ve observed so far. If we&amp;rsquo;ve observed only one
flip and it was heads ($k = n = 1$), we&amp;rsquo;ll be certain of heads on
the second toss! &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;We can illustrate this pattern in a plot. First let&amp;rsquo;s consider what
happens if the coin keeps coming up heads, i.e. $k = n$. As $n$
increases, various settings of $\lambda$ behave as follows.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/inductive-logic/lambda-continuum-k1-n1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now suppose the coin only lands heads every third time, so that
$k \approx n/3$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/inductive-logic/lambda-continuum-k1-n3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Notice how lower settings of $\lambda$ bounce around more here before
settling into roughly $1/ 3$. Higher settings approach $1/ 3$ more
steadily, but they take longer to get there.&lt;/p&gt;

&lt;h1 id=&#34;carnap-s-program&#34;&gt;Carnap&amp;rsquo;s Program&lt;/h1&gt;

&lt;p&gt;Johnson and Carnap went much further, and others since have gone further still. For
example, we can include more than one predicate, we can use relational
predicates, and much more.&lt;/p&gt;

&lt;p&gt;But philosophers aren&amp;rsquo;t too big on this research program nowadays. Why not?&lt;/p&gt;

&lt;p&gt;Choosing $\lambda$ is one issue. Once we see that it&amp;rsquo;s more than a
binary choice, between inductive optimism and skepticism, it&amp;rsquo;s hard to
see why we should plump for any particular value of $\lambda$. We could
set $\lambda = 2$, or $\pi$, or $42$. By what criterion could we make
this choice? No clear answer emerged from Carnap&amp;rsquo;s program.&lt;/p&gt;

&lt;p&gt;Another issue is Goodman&amp;rsquo;s famous &lt;a href=&#34;http://www.wi-phi.com/video/puzzle-grue&#34; target=&#34;_blank&#34;&gt;grue
puzzle&lt;/a&gt;. Suppose we trade our
coin flips for emeralds. We might replace the heads/tails dichotomy with
green/not-green then. But we could instead replace it with
grue/not-grue. The prescriptions of our inductive logic depend on
our choice of predicate&amp;mdash;on the underlying language to which we apply
our chosen value of $\lambda$.&lt;/p&gt;

&lt;p&gt;So the Johnson/Carnap system doesn&amp;rsquo;t provide us with rules for inductive
reasoning, more a framework for formulating such rules. We have to
decide which predicates should be projectible by choosing the underlying
language. And then we have to decide how projectible they should be by
choosing $\lambda$. Only then does the framework tell us what
conclusions to draw from a given set of observations.&lt;/p&gt;

&lt;p&gt;Personally, I still find the framework useful. It provides a
lovely way to express informal ideas more rigorously. In it we can frame
questions about induction, skepticism, and prior probabilities with
lucidity.&lt;/p&gt;

&lt;p&gt;I also like it as a source of toy models. For example, I might test when
a given claim about induction holds and when it doesn&amp;rsquo;t, by playing with
different incarnations of $\lambda$.&lt;/p&gt;

&lt;p&gt;The framework&amp;rsquo;s utility is thus a lot like that of its
deductive cousins. Compare Timothy Williamson&amp;rsquo;s use of modal logic to
create &lt;a href=&#34;https://philpapers.org/rec/WILANO-22&#34; target=&#34;_blank&#34;&gt;models of Gettier cases&lt;/a&gt;,
for example, or his model of &lt;a href=&#34;https://philpapers.org/rec/WILIKN&#34; target=&#34;_blank&#34;&gt;improbable
knowledge&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Even in deductive logic, we only get as much out as we put in. We have
to choose our connectives in propositional logic, our accessibility
relation in modal logic, etc. But a flexible system like possible-world
frames still has its uses. We can use it to explore
philosophical options and their interconnections.&lt;/p&gt;

&lt;h1 id=&#34;further-readings&#34;&gt;Further Readings&lt;/h1&gt;

&lt;p&gt;For more on this topic I suggest the following readings.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://fitelson.org/il.pdf&#34; target=&#34;_blank&#34;&gt;Inductive Logic&lt;/a&gt;, by Branden Fitelson&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cambridge.org/core/books/cambridge-companion-to-carnap/carnap-on-probability-and-induction/8AEBCBACE4A89B567B7508A1E065EB51&#34; target=&#34;_blank&#34;&gt;Carnap on Probability and Induction&lt;/a&gt;,
by Sandy Zabell&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.iep.utm.edu/conf-ind/&#34; target=&#34;_blank&#34;&gt;The IEP entry on Confirmation and
Induction&lt;/a&gt;, by Franz Huber&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://plato.stanford.edu/entries/logic-inductive/&#34; target=&#34;_blank&#34;&gt;The SEP entry on Inductive
Logic&lt;/a&gt;, by
James Hawthorne&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cambridge.org/core/books/pure-inductive-logic/2ED3E3EE53CC51A99C1DD94341CB7FA2&#34; target=&#34;_blank&#34;&gt;Pure Inductive
Logic&lt;/a&gt;,
by Jeffrey Paris and Alena Vencovská&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you want to see how Johnson and Carnap&amp;rsquo;s two-stage assignment of priors yields the
Rule of Succession, check out the Appendix.&lt;/p&gt;

&lt;h1 id=&#34;appendix-deriving-the-rule-of-succession&#34;&gt;Appendix: Deriving the Rule of Succession&lt;/h1&gt;

&lt;p&gt;To derive the Rule of Succession from the two-stage assignment of
priors, we need two key formulas.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The prior probability of a particular sequence with $k$ heads out of
$n$ flips.&lt;/li&gt;
&lt;li&gt;The prior probability of the same initial sequence, followed by one
more heads.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The first quantity is the probability of getting $k$ heads out of $n$
flips, regardless of order, divided by the number of ways to get $k$
heads out of $n$ flips. The number of ways to get $k$ heads out of $n$
flips is called the &lt;a href=&#34;https://en.wikipedia.org/wiki/Binomial_coefficient&#34; target=&#34;_blank&#34;&gt;binomial
coefficient&lt;/a&gt;. It&amp;rsquo;s
written $\binom{n}{k}$, and there&amp;rsquo;s a nice formula for calculating it:
$$ \binom{n}{k} = \frac{n!}{(n-k)!k!}. $$ Since a sequence of $n$ flips
can feature anywhere from $0$ to $n$ heads, there are $n+1$ structure
descriptions, each with probability $1/(n+1)$. Thus the probability of a
specific state-description with $k$ heads out of $n$ flips is
\begin{align}
  \frac{1}{(n+1)\binom{n}{k}}
    &amp;amp;= \frac{1}{(n+1) \frac{n!}{(n-k)!k!}}\\&lt;br /&gt;
    &amp;amp;= \frac{(n-k)!k!}{(n+1)!}.\tag{1}
\end{align}&lt;/p&gt;

&lt;p&gt;The second probability we need is for the same initial sequence, but
with an additional heads on the next toss. That&amp;rsquo;s a sequence with $k+1$
heads out of $n+1$ tosses. There are $n+2$ structure descriptions now,
each with probability $1/(n+2)$. So the probability in question is
\begin{align}
  \frac{1}{(n+2)\binom{n+1}{k+1}}
    &amp;amp;= \frac{1}{(n+2) \frac{(n+1)!}{(n-k)!(k+1)!}}\\&lt;br /&gt;
    &amp;amp;= \frac{(n-k)!(k+1)!}{(n+2)!}.\tag{2}
\end{align}&lt;/p&gt;

&lt;p&gt;Now, to get the conditional probability we&amp;rsquo;re after, we take the ratio
of the second probability $(2)$ over the first probability $(1)$: $$
\begin{aligned}
  \frac{ \frac{(n-k)!(k+1)!}{(n+2)!} }{ \frac{(n-k)!k!}{(n+1)!} }
    &amp;amp;= \frac{(n-k)!(k+1)!}{(n+2)!}  \frac{(n+1)!}{(n-k)!k!} \\&lt;br /&gt;
    &amp;amp;= \frac{k+1}{n+2}.
\end{aligned}
$$ This agrees with the rule of succession, as desired.&lt;/p&gt;

&lt;p&gt;So far though, we&amp;rsquo;ve only shown the rule of succession for a specific,
observed sequence. We&amp;rsquo;ve shown that $\p(HTHH \given HTH) = 3/ 4$, for example.
But what if we don&amp;rsquo;t know the particular sequence so far? Maybe we only
know there were $2$ heads out of $3$ tosses. Shouldn&amp;rsquo;t we still be able to
derive the same result?&lt;/p&gt;

&lt;p&gt;We can, with the help of a relevant theorem of probability: if
$\p(A \given B) = \p(A \given C)$, and $B$ and $C$ are mutually
exclusive, then
$$ \p(A \given B \vee C) = \p(A \given B) = \p(A \given C). $$ In our
case $A$ specifies heads on flip $n+1$, while $B$ and $C$ each specify
some sequence for flips $1$ through $n$. Although these sequences
feature the same number of heads and tails, $B$ and $C$ specify
different orderings. So they&amp;rsquo;re mutually exclusive.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ve already shown that
$$ \p(A \given B) = \frac{k+1}{n+1} = \p(A \given C). $$ So we just have
to verify the theorem:
$$
\begin{aligned}
  \p(A \given B \vee C)
    &amp;amp;= \frac{\p(A \wedge (B \vee C))}{\p(B \vee C)}\\&lt;br /&gt;
    &amp;amp;= \frac{\p(A \wedge B) + \p(A \wedge C)}{\p(B \vee C)}\\&lt;br /&gt;
    &amp;amp;= \frac{\p(A \given B)\p(B) + \p(A \given C)\p( C)}{\p(B \vee C)}\\&lt;br /&gt;
    &amp;amp;= \frac{\p(A \given B) \left( \p(B) + \p( C) \right)}{\p(B \vee C)}\\&lt;br /&gt;
    &amp;amp;= \p(A \given B).
\end{aligned}
$$
By applying this formula repeatedly to a disjunction of state-descriptions, we get the conditional probability on the structure description of interest.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:peirce&#34;&gt;Carnap also cites Keynes and Peirce as endorsing the Wittgensteinian approach. But thanks to Jonathan Livengood I learned this is actually a misattribution: Keynes mistakenly attributes the view to Peirce, and Carnap seems to have followed Keynes&amp;rsquo; error.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:peirce&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;

&lt;li id=&#34;fn:1&#34;&gt;&lt;p&gt;More precisely: we regard them as having the same probability
&lt;em&gt;density&lt;/em&gt;, namely $1$.&lt;/p&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;

&lt;li id=&#34;fn:2&#34;&gt;&lt;p&gt;For the $\lambda = 0$ case, we need probability axioms that permit
conditioning on zero-probability events. For example,
$\p(HH \given H) = 1$ so $\p(HT \given H) = 0$. Thus
$\p(HT) = 0$, and $\p(HTH \given HT)$ is undefined on the usual,
Kolmogorov axioms.&lt;/p&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;

&lt;li id=&#34;fn:3&#34;&gt;&lt;p&gt;When $n = 0$ we have to stipulate that the probability is $1/ 2$,
the limit as $\lambda \rightarrow 0$.&lt;/p&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Super-Humean Theory of Belief</title>
      <link>http://jonathanweisberg.org/post/super-humean-belief/</link>
      <pubDate>Wed, 21 Aug 2019 00:00:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/super-humean-belief/</guid>
      <description>&lt;p&gt;The classic &lt;a href=&#34;https://plato.stanford.edu/entries/formal-belief/#QuaBel&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;Lockean&amp;rdquo; thesis&lt;/a&gt; about full and partial belief says full belief is rational iff strong partial belief is rational. &lt;a href=&#34;https://philpapers.org/rec/LEIIHT&#34; target=&#34;_blank&#34;&gt;Hannes Leitgeb&amp;rsquo;s &amp;ldquo;Humean&amp;rdquo; thesis&lt;/a&gt; proposes a subtler connection.
$
\newcommand\p{Pr}
\newcommand{\B}{\mathbf{B}}
\newcommand{\given}{\mid}
$&lt;/p&gt;

&lt;dl&gt;
&lt;dt&gt;The Humean Thesis&lt;/dt&gt;
&lt;dd&gt;&lt;p&gt;For a rational agent whose full beliefs are given by the set $\mathbf{B}$, and whose credences by the probability function $\p$: $B \in \mathbf{B}$ iff $\p(B \given A) &amp;gt; t$ for all $A$ consistent with $\mathbf{B}$.&lt;/p&gt;&lt;/dd&gt;
&lt;/dl&gt;

&lt;p&gt;Notice that we can think of this as, instead, a coherentist theory of justification. Suppose we replace credence with &amp;ldquo;evidential&amp;rdquo; probability (think: Carnap, Williamson). Then we get a theory of justification where beliefs aren&amp;rsquo;t justified in isolation. It&amp;rsquo;s not enough for a belief to be highly probable in its own right, it has to be part of a larger body that underwrites that high probability.&lt;/p&gt;

&lt;p&gt;Flipping things around, the coherentist theory of justification from &lt;a href=&#34;http://jonathanweisberg.org/post/coherentism-1&#34;&gt;my last wacky post&lt;/a&gt; doubles as an even wackier theory of full belief. The Humean view is roughly that a belief is justified iff its fellows secure its high probability. Now the &amp;ldquo;Super-Humean&amp;rdquo; view says a belief is justified to the extent its fellows secure its high centrality.&lt;/p&gt;

&lt;p&gt;(Last time we explored one fun way of measuring centrality, drawing on coherentism for inspiration, and network theory for the math. But network theory offers &lt;a href=&#34;https://en.wikipedia.org/wiki/Centrality&#34; target=&#34;_blank&#34;&gt;many others ways of measuring centrality&lt;/a&gt;, which could be slotted in here to provide alternative theories of full and partial belief.)&lt;/p&gt;

&lt;p&gt;Like Leitgeb&amp;rsquo;s Humean view, the Super-Humean view has a holistic character. Instead of evaluating full beliefs just by looking at your credences, we also have to look at what else you believe.&lt;/p&gt;

&lt;p&gt;Another parallel: both theories have a permissive quality. Leitgeb presents examples where more than one set $\B$ fits with a given credence function $\p$, on the Humean view. And the same will be true on the Super-Humean view.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:coins&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:coins&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;But there are interesting differences. We can evaluate beliefs individually on the Super-Humean account, even though our method of evaluation is holistic. True, a belief&amp;rsquo;s justification depends on what else you believe. But your beliefs don&amp;rsquo;t all stand or fall together; some can come out justified even though others come out unjustified.&lt;/p&gt;

&lt;p&gt;Strictly speaking, some beliefs come out highly justified even though others come out hardly justified. Because, differing again from the Humean view, evaluations are graded on the Super-Humean view. Each belief is assigned a degree of justification.&lt;/p&gt;

&lt;p&gt;One nice thing about the Super-Humean view, then, is that it allows for &amp;ldquo;non-ideal&amp;rdquo; theorizing. We can study non-ideal agents, and discern more justified beliefs from lesser ones.&lt;/p&gt;

&lt;p&gt;&amp;ldquo;But does it handle the lottery and preface paradoxes?&amp;rdquo;, is the question we always ask about a theory of full belief. As is so often the case, the answer is &amp;ldquo;yes, but&amp;hellip;&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Consider a lottery of $100$ tickets with one to be selected at random as the winner. If you believe of each ticket that it will lose, we have a network of $101$ nodes: $L_1$ through $L_{100}$, plus the tautology node $\top$. How strong are the connections between these nodes? Assuming we take $L_3&amp;ndash;L_{100}$ as givens in determining the weight of the $L_2 \rightarrow L_1$ arrow, it gets weight $0$ since
$$\p(L_1 \given L_2 \wedge L_3 \wedge \ldots \wedge L_{100}) = 0.$$
And likewise for all the other arrows,&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:epsilon&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:epsilon&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; except those pointing to the $\top$ node (they always get weight $1$). All the $L_i$ beliefs thus come out with rock-bottom justification compared to $\top$, i.e. you aren&amp;rsquo;t justified in believing these lottery propositions.&lt;/p&gt;

&lt;p&gt;Contrast that with a preface case, where you believe each of $100$ claims you&amp;rsquo;ve researched, $C_1$ through $C_{100}$. These claims are positively correlated though, or at least independent.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:pollock&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:pollock&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; So
$$\p(C_1 \given C_2 \wedge C_3 \wedge \ldots \wedge C_{100}) \approx 1,$$
and likewise for the other $C_i$. The belief-graph here is thus tightly connected, and the $C_i$ nodes will score high on centrality compared to $\top$. So you&amp;rsquo;re highly justified in your beliefs in the preface case.&lt;/p&gt;

&lt;p&gt;So far so good, at least if you think&amp;mdash;as I tend to&amp;mdash;that lottery beliefs should come out unjustified, while preface beliefs should come out justified. What&amp;rsquo;s the &amp;ldquo;but&amp;hellip;&amp;rdquo; then? I see two issues (at least).&lt;/p&gt;

&lt;p&gt;First, we had to assume that all your remaining beliefs are taken as given in assessing the weight of a connection like $L_2 \rightarrow L_1$. That worked out well here. But as a general rule, it doesn&amp;rsquo;t always have great results, as &lt;a href=&#34;https://twitter.com/Juan/status/1161735087093776384?s=20&#34; target=&#34;_blank&#34;&gt;Juan Comesaña noted&lt;/a&gt; about our treatment of the Tweety case last time.&lt;/p&gt;

&lt;p&gt;We could go all the way to the other extreme of course, and just evaluate the $L_2 \rightarrow L_1$ connection in isolation by looking at $\p(L_1 \given L_2)$. But that seems too extreme, since it means ignoring the agent&amp;rsquo;s other beliefs altogether.&lt;/p&gt;

&lt;p&gt;What we want is something in between, it seems. We want the agent&amp;rsquo;s other beliefs to &amp;ldquo;get in the way&amp;rdquo; enough that they substantially weaken the connections in the lottery graph. But we don&amp;rsquo;t want them to be taken entirely for granted. Exactly how to achieve the right balance here is something I&amp;rsquo;m not sure about.&lt;/p&gt;

&lt;p&gt;Second issue: what if you only adopt a few lottery beliefs, just $L_1$ and $L_2$ for example? Then we can&amp;rsquo;t exploit the &amp;ldquo;collective defeat&amp;rdquo; that drove our treatment of the lottery.&lt;/p&gt;

&lt;p&gt;You might respond that this is a fine result, since isolated lottery beliefs are actually justified. It&amp;rsquo;s only when you apply the same logic to all the tickets that your justification is undercut. But I find this unsatisfying.&lt;/p&gt;

&lt;p&gt;Maybe a student encountering the paradox for the first time is justified in believing their ticket will lose. But it should be enough to defeat that justification that they merely realize they could believe the same thing about all the other tickets, for identical reasons. Even if they don&amp;rsquo;t go ahead to form those beliefs, they should drop the one belief they had about their own ticket.&lt;/p&gt;

&lt;p&gt;This is one way in which Leitgeb&amp;rsquo;s Humean theory seems superior to me. On the Humean view, which beliefs are rational depends on how the space of possibilities is partitioned (see &lt;a href=&#34;https://philpapers.org/rec/LEITST-2&#34; target=&#34;_blank&#34;&gt;Leitgeb 2014&lt;/a&gt;). And the partition is determined by the context&amp;mdash;how the subject frames the situation in their mind. (At least, that&amp;rsquo;s how I understand Leitgeb here.) So just realizing the symmetry of the lottery paradox is enough to defeat justification, on the Humean view.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:coins&#34;&gt;&lt;p&gt;Example: imagine we&amp;rsquo;ll flip a coin of unknown bias $10$ times. And suppose the probabilities obey Laplace&amp;rsquo;s &lt;a href=&#34;https://en.wikipedia.org/wiki/Rule_of_succession&#34; target=&#34;_blank&#34;&gt;Rule of Succession&lt;/a&gt; (a.k.a. Carnap&amp;rsquo;s  $\mathfrak{m}^*$ confirmation function). Then, if you believe each flip will land heads, your beliefs will all come out highly justified, i.e. highly central in your web of $10$ beliefs. But they&amp;rsquo;d have the same justification if you instead believed each flip will land tails.&lt;/p&gt;

&lt;p&gt;Permissivism aside, this might seem a pretty bad result on its own. Even if our theory fixed which way you should go, say heads instead of tails, that would be pretty weird. Shouldn&amp;rsquo;t you wait for at least a few flips before forming any such beliefs?&lt;/p&gt;

&lt;p&gt;The problem is that we haven&amp;rsquo;t required your beliefs to be inherently probable, only that they render one another probable. The Lockean and Humean theories have such a threshold requirement built-in, but we can build it into our theory too. We can just stipulate that a full belief should be highly probable, as well as being highly central in the network of all your beliefs.&lt;/p&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:coins&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:epsilon&#34;&gt;More carefully, each arrow gets the minimum possible weight. If we use the &amp;ldquo;Google hack&amp;rdquo; from last time, this is some small positive number $\epsilon$ instead of $0$.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:epsilon&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:pollock&#34;&gt;Notice we&amp;rsquo;re borrowing the crux of &lt;a href=&#34;https://pdfs.semanticscholar.org/b22f/a2029aad762cbeec8d03659b0f8b71dd6d91.pdf&#34; target=&#34;_blank&#34;&gt;Pollock&amp;rsquo;s (1994)&lt;/a&gt; classic treatment of the lottery and preface paradoxes. We&amp;rsquo;re just plugging his observation into a different formal framework.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:pollock&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Coherentism Without Coherence</title>
      <link>http://jonathanweisberg.org/post/coherentism-1/</link>
      <pubDate>Wed, 14 Aug 2019 00:00:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/coherentism-1/</guid>
      <description>

&lt;p&gt;If you look at the little network diagram below, you&amp;rsquo;ll probably
agree that $P$ is the most &amp;ldquo;central&amp;rdquo; node in some intuitive sense.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/coherentism-1/fig3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This post is about using a belief&amp;rsquo;s centrality in the web of belief to
give a coherentist account of its justification. The more central a
belief is, the more justified it is.&lt;/p&gt;

&lt;p&gt;But how do we quantify &amp;ldquo;centrality&amp;rdquo;? The rough idea: the more ways there
are to arrive at a proposition by following inferential pathways in the
web of belief, the more central it is.&lt;/p&gt;

&lt;p&gt;Since we&amp;rsquo;re coherentists today (for the next 10 minutes, anyway), cyclic
pathways are allowed here. If we travel
$P \rightarrow Q \rightarrow R \rightarrow P$, that counts as an
inferential path leading to $P$. And if we go around that cycle twice,
that counts as another such pathway.&lt;/p&gt;

&lt;p&gt;You might think this just wrecks the whole idea. Every node has
infinitely many such pathways leading to it, after all. By cycling
around and around we can come up with literally any number of pathways
ending at a given node.&lt;/p&gt;

&lt;p&gt;But, by examining how these pathways differ in the limit, we can
differentiate between more and less central nodes/beliefs. We can thus
clarify a sense in which $P$ is most central, and quantify that
centrality. We can even use that quantity to answer a classic objection
to coherentism leveled by &lt;a href=&#34;https://philpapers.org/rec/KLEWPC&#34; target=&#34;_blank&#34;&gt;Klein &amp;amp; Warfield
(1994)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As a bonus, we can do all this without ever giving an account of what
makes a corpus of beliefs &amp;ldquo;coherent.&amp;rdquo; This flips the script on a lot of
contemporary formal work on coherentism.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; Because coherentism is
holistic, you might think it has to evaluate the coherence of a whole
corpus first, before it can assess the individual members.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; But we&amp;rsquo;ll
see this isn&amp;rsquo;t so.
$$
\newcommand\T{\intercal}
\newcommand{\A}{\mathbf{A}}
\renewcommand{\v}{\mathbf{v}}
$$&lt;/p&gt;

&lt;h1 id=&#34;counting-pathways&#34;&gt;Counting Pathways&lt;/h1&gt;

&lt;p&gt;Our idea is to count how many paths there are leading to $P$ vs. other
nodes. We start with paths of length $1$, then count paths of length
$2$, then length $3$, and so on. As we count longer and longer paths,
each node&amp;rsquo;s count approaches infinity.&lt;/p&gt;

&lt;p&gt;But not their relative ratios! If, at each step, we divide the number of
paths ending at $P$ by the number of all paths, this ratio converges.&lt;/p&gt;

&lt;p&gt;To find its limit, we represent our graph numerically. A graph can be
represented in a table, where each node corresponds to a row and column.
The columns represent &amp;ldquo;sources&amp;rdquo; and the rows represent &amp;ldquo;targets.&amp;rdquo; We put
a $1$ where the column node points to the row node, otherwise we put a
$0$.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;$P$&lt;/th&gt;
&lt;th&gt;$Q$&lt;/th&gt;
&lt;th&gt;$R$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;$P$&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;$Q$&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;$R$&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Hiding the row and column names gives us a matrix we&amp;rsquo;ll call $\A$: $$
\A =
\left[
  \begin{matrix}
    0 &amp;amp; 1 &amp;amp; 1 \\&lt;br /&gt;
    1 &amp;amp; 0 &amp;amp; 0 \\&lt;br /&gt;
    0 &amp;amp; 1 &amp;amp; 0
  \end{matrix}
\right].
$$ Notice how each row records the length-$1$ paths leading to the
corresponding node. There are two such paths to $P$, and one each to $Q$
and $R$.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;The key to counting longer paths is to take powers of $\A$. If we
multiply $\A$ by itself to get $\A^2$, we get a record of the length-$2$
paths: $$
\A^2 = \A \times \A = \left[
  \begin{matrix}
    0 &amp;amp; 1 &amp;amp; 1 \\&lt;br /&gt;
    1 &amp;amp; 0 &amp;amp; 0 \\&lt;br /&gt;
    0 &amp;amp; 1 &amp;amp; 0
  \end{matrix}
\right] \left[
  \begin{matrix}
    0 &amp;amp; 1 &amp;amp; 1 \\&lt;br /&gt;
    1 &amp;amp; 0 &amp;amp; 0 \\&lt;br /&gt;
    0 &amp;amp; 1 &amp;amp; 0
  \end{matrix}
\right] =
\left[
  \begin{matrix}
    1 &amp;amp; 1 &amp;amp; 0 \\&lt;br /&gt;
    0 &amp;amp; 1 &amp;amp; 1 \\&lt;br /&gt;
    1 &amp;amp; 0 &amp;amp; 0
  \end{matrix}
\right].
$$ There are two such paths to $P$: $$
\begin{aligned}
  Q \rightarrow R \rightarrow P,\\&lt;br /&gt;
  P \rightarrow Q \rightarrow P.
\end{aligned}
$$ Similarly for $Q$: $$
\begin{aligned}
  Q \rightarrow P \rightarrow Q,\\&lt;br /&gt;
  R \rightarrow P \rightarrow Q.
\end{aligned}
$$ While $R$ has just one length-$2$ path:
$$ P \rightarrow Q \rightarrow R. $$ If we go on to examine $\A^3$, its
rows will tally the length-$3$ paths; in general, $\A^n$ tallies the
paths of length-$n$.&lt;/p&gt;

&lt;p&gt;But we want relative ratios, not raw counts. The trick to getting these
is to divide $\A$ at each step by a special number $\lambda$, known as
the &amp;ldquo;leading eigenvalue&amp;rdquo; of $\A$ (details &lt;a href=&#34;#tech&#34;&gt;below&lt;/a&gt;). If we take
the limit
$$ \lim_{n \rightarrow \infty} \left(\frac{\A}{\lambda}\right)^n $$ we
get a matrix whose columns all have a special property: $$
\left[
  \begin{matrix}
    0.41 &amp;amp; 0.55 &amp;amp; 0.31 \\&lt;br /&gt;
    0.31 &amp;amp; 0.41 &amp;amp; 0.23 \\&lt;br /&gt;
    0.23 &amp;amp; 0.31 &amp;amp; 0.18
  \end{matrix}
\right].
$$ They all have the same relative proportions. They&amp;rsquo;re multiples of the
same &amp;ldquo;frequency vector,&amp;rdquo; a vector of positive values that sum to $1$: $$
\left[
  \begin{matrix}
    0.43 \\&lt;br /&gt;
    0.32 \\&lt;br /&gt;
    0.25 \\&lt;br /&gt;
  \end{matrix}
\right].
$$ So as we tally longer and longer paths, we find that $43\%$ of those
paths lead to $P$, compared with $32\%$ for $Q$ and $25\%$ for $R$. Thus
$P$ is about $1.3$ times as justified as $Q$ ($.43/.32$), and about
$1.7$ times as justified as $R$ ($.43/.25$).&lt;/p&gt;

&lt;p&gt;We want absolute degrees of justification though, not just comparative
ones. So we borrow a trick from probability theory and use a tautology
for scale.&lt;/p&gt;

&lt;p&gt;We add a special node $\top$ to our graph, which every other node points
to, though $\top$ doesn&amp;rsquo;t point back.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/coherentism-1/fig4.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Updating our matrix $\A$ accordingly, we insert $\top$ in the first
row/column: $$
\A =
\left[
  \begin{matrix}
    0 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1 \\&lt;br /&gt;
    0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 \\&lt;br /&gt;
    0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \\&lt;br /&gt;
    0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0
  \end{matrix}
\right].
$$ Redoing our limit anlaysis gives us the vector
$(1.00, 0.57, 0.43, 0.33)$. But this isn&amp;rsquo;t our final answer, because
it&amp;rsquo;s actually not possible for the non-$\top$ nodes to get a value
higher than $2/3$ in a graph with just $3$ non-$\top$ nodes.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; So we
divide elementwise by $(1, 2/3, 2/3, 2/3)$ to scale things, giving us
our final result: $$
\left[
  \begin{matrix}
    1.00 \\&lt;br /&gt;
    0.85 \\&lt;br /&gt;
    0.65 \\&lt;br /&gt;
    0.49
  \end{matrix}
\right].
$$ The relative justifications are the same as before, e.g. $P$ is still
$1.3$ times as justified as $Q$. But now we can make absolute
assessments too. $R$ comes out looking pretty bad ($0.49$), as seems
right, while $Q$ looks a bit better ($0.65$). Of course $P$ looks best
($0.85$), though maybe not quite good enough to be justified &lt;em&gt;tout
court&lt;/em&gt;.&lt;/p&gt;

&lt;h1 id=&#34;the-klein-warfield-problem&#34;&gt;The Klein&amp;ndash;Warfield Problem&lt;/h1&gt;

&lt;p&gt;Ok that&amp;rsquo;s theoretically nifty and all, but does it work on actual cases?
Let&amp;rsquo;s try it out by looking at a notorious objection to coherentism.
&lt;a href=&#34;https://philpapers.org/rec/KLEWPC&#34; target=&#34;_blank&#34;&gt;Klein &amp;amp; Warfield (1994)&lt;/a&gt; argue that
coherentism flouts the laws of probability. How so?&lt;/p&gt;

&lt;p&gt;Making sense of things often means believing more: taking on new beliefs
to resolve the tensions in our existing ones. For example, if we think
Tweety is a bird who can&amp;rsquo;t fly, the tension is resolved if we also
believe they&amp;rsquo;re a penguin.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:5&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:5&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;But believing more means believing less probably. Increases in logical
strength bring decreases in probability (unless the stronger content was
already guaranteed with probability $1$). So increasing the coherence in
one&amp;rsquo;s web of belief will generally mean decreasing its probability. How
could increasing coherence increase justification, then?&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://philpapers.org/rec/MEROBO&#34; target=&#34;_blank&#34;&gt;Merricks (1995)&lt;/a&gt; points out that,
even though the probability of the whole corpus goes down, the
probabilities of individual beliefs go up in a way. After all, it&amp;rsquo;s more
likely Tweety can&amp;rsquo;t fly if they&amp;rsquo;re a penguin, than if they&amp;rsquo;re just a
bird of some unknown species.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s only the beginning of a satisfactory answer though. After all, we
might not be justified in believing Tweety&amp;rsquo;s a penguin in the first
place! Adding a new belief to support an existing belief doesn&amp;rsquo;t help if
the new belief has no support itself. We need a more global assessment,
which is where the present account shines.&lt;/p&gt;

&lt;p&gt;Suppose we add $P$ = &lt;em&gt;Tweety is a penguin&lt;/em&gt; to the network containing $B$
= &lt;em&gt;Tweety is a bird&lt;/em&gt; and $\neg F$ = &lt;em&gt;Tweety can&amp;rsquo;t fly&lt;/em&gt;. Will this
increase the centrality/justification of $B$ and of $\neg F$? Yes, but
we need to sort out the support relations to verify this.&lt;/p&gt;

&lt;p&gt;Presumably $P$ supports $B$, and $\neg F$ too. But what about the other
way around? If Tweety is a flightless bird, there&amp;rsquo;s a decent chance
they&amp;rsquo;re a penguin. But it&amp;rsquo;s hardly certain; they might be an emu or kiwi
instead. Come to think of it, isn&amp;rsquo;t support a matter of degree, so don&amp;rsquo;t
we need finer tools than just on/off arrows?&lt;/p&gt;

&lt;p&gt;Yes, and the refinement is easy. We accommodate degrees of support by
attaching weights to our arrows. Instead of just placing a $1$ in our
matrix $\A$ wherever the column-node points to the row-node, we put a
number from the $[0,1]$ interval that reflects the strength of support.
The same limit analysis as before still works, as it turns out.
We just think of our inferential links as &amp;ldquo;leaky pipes&amp;rdquo; now, where
weaker links make for leakier pipelines.&lt;/p&gt;

&lt;p&gt;We still need concrete numbers to analyze the Tweety example. But it&amp;rsquo;s a
toy example, so let&amp;rsquo;s just make up some plausible-ish numbers to get us
going. Let&amp;rsquo;s suppose $1\%$ of birds are flightless, and birds are an
even smaller percentage of the flightless things, say $0.1\%$. Let&amp;rsquo;s
also pretend that $20\%$ of flightless birds are penguins.&lt;/p&gt;

&lt;p&gt;Before believing Tweety is a penguin then, our web of belief looks like
this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/coherentism-1/fig5a.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Calculating the degrees of justification for $B$ and $\neg F$, both come
out very close to $0$ as you&amp;rsquo;d expect (with $B$ closer to $0$ than
$\neg F$). Now we add $P$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/coherentism-1/fig5b.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Recalculating degrees of justification, we find that they increase
drastically. $B$ and $F$ are now justified to degree $0.85$, while $P$
is justified to degree $0.26$. (All numbers approximate.)&lt;/p&gt;

&lt;p&gt;So our account vindicates Merricks. Not only does adding $P$ to the
corpus add &amp;ldquo;local&amp;rdquo; justification for $B$ and for $\neg F$. It also
improves their standing on a more global assessment.&lt;/p&gt;

&lt;p&gt;You might be worried though: did $P$ come out too weakly justified, at
just $0.26$? No: that&amp;rsquo;s either an artifact of oversimplification, or
else it&amp;rsquo;s actually the appropriate outcome. Notice that $B$ and $\neg F$
don&amp;rsquo;t really support Tweety being a penguin. They&amp;rsquo;re a flightless bird,
sure, but maybe they&amp;rsquo;re an emu, kiwi, or moa. We chose to believe
penguin, and maybe we have our reasons. If we do, then the graph is
missing background beliefs which would improve $P$&amp;rsquo;s standing once
added. But otherwise, we just fell prey to stereotyping or
comes-to-mind-bias, in which case it&amp;rsquo;s right that $P$ stand poorly.&lt;/p&gt;

&lt;h1 id=&#34;tech&#34;&gt;Technical Background&lt;/h1&gt;

&lt;p&gt;The notion of centrality used here is a common tool in network analysis,
where it&amp;rsquo;s known as &lt;a href=&#34;https://en.wikipedia.org/wiki/Eigenvector_centrality&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;eigenvector
centrality.&amp;rdquo;&lt;/a&gt;
Because the frequency vector we arrive at in the limit is an
&lt;a href=&#34;https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors&#34; target=&#34;_blank&#34;&gt;eigenvector&lt;/a&gt;
of the matrix $\A$. In fact it&amp;rsquo;s a special eigenvector, the only one
with all-positive values.&lt;/p&gt;

&lt;p&gt;Since we&amp;rsquo;re measuring justification on a $0$-to-$1$ scale, our account
depends on there always being such an eigenvector for $\A$. In fact we
need it to be unique, up to scaling (i.e. up to multiplication by a
constant).&lt;/p&gt;

&lt;p&gt;The theorem that guarantees this is actually quite old, going back to
work by Oskar Perron and Georg Frobenius published around 1910. Here&amp;rsquo;s one version of it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Perron&amp;ndash;Frobenius Theorem.&lt;/strong&gt; Let $\A$ be a square matrix whose entries
are all positive. Then all of the following hold.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;$\A$ has an eigenvalue $\lambda$ that is larger (in absolute value)
than $\A$&amp;rsquo;s other eigenvalues. We call $\lambda$ the &lt;em&gt;leading
eigenvalue&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;$\A$&amp;rsquo;s leading eigenvalue has an eigenvector $\v$ whose entries are
all positive. We call $\v$ the &lt;em&gt;leading eigenvector&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;$\A$ has no other positive eigenvectors, save multiples of $\v$.&lt;/li&gt;
&lt;li&gt;The powers $(\A/\lambda)^n$ as $n \rightarrow \infty$ approach a
matrix whose columns are all multiples of $\v$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now, our matrices had some zeros, so they weren&amp;rsquo;t positive in all their
entries. But it doesn&amp;rsquo;t really matter, as it turns out.&lt;/p&gt;

&lt;p&gt;Frobenius&amp;rsquo; contribution was to generalize this result to many cases that
feature zeros. But even in cases where Frobenius&amp;rsquo; weaker conditions
aren&amp;rsquo;t satisfied, we can just borrow a trick from Google.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:6&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:6&#34;&gt;6&lt;/a&gt;&lt;/sup&gt; Instead of
using a $0$-to-$1$ scale, we use $\epsilon$-to-$1$ for some very small
positive number $\epsilon$. Then all entries in $\A$ are guaranteed to
be positive, and we just rescale our results accordingly. (Choose
$\epsilon$ small enough and the difference is negligible in practice.)&lt;/p&gt;

&lt;h1 id=&#34;acknowledgments&#34;&gt;Acknowledgments&lt;/h1&gt;

&lt;p&gt;This post owes a lot to prior work by Elena Derksen and Selim Berker.
I&amp;rsquo;d never really thought much about how coherence and justification
relate prior to reading &lt;a href=&#34;http://www.philpeople.com/elenarabinoffderksen/research.html&#34; target=&#34;_blank&#34;&gt;Derksen&amp;rsquo;s
work&lt;/a&gt;. And
&lt;a href=&#34;https://philpapers.org/rec/BERCVG&#34; target=&#34;_blank&#34;&gt;Berker&amp;rsquo;s&lt;/a&gt; prompted me to take graphs
more seriously as a way of formalizing coherentism. I&amp;rsquo;m also grateful to
David Wallace for &lt;a href=&#34;http://jonathanweisberg.org/post/page-rank-1/&#34;&gt;introducing me to the Perron&amp;ndash;Frobenius theorem&amp;rsquo;s use
as a tool in network
analysis&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;&lt;p&gt;See &lt;a href=&#34;https://philpapers.org/rec/SHOICT-2&#34; target=&#34;_blank&#34;&gt;Shogenji (1999)&lt;/a&gt; and
&lt;a href=&#34;https://philpapers.org/rec/FITAPT&#34; target=&#34;_blank&#34;&gt;Fitelson (2003)&lt;/a&gt; for some early
accounts. See Section 6 of &lt;a href=&#34;https://plato.stanford.edu/entries/justep-coherence/#ProMeaCoh&#34; target=&#34;_blank&#34;&gt;Olsson&amp;rsquo;s SEP
entry&lt;/a&gt;
for a survey and more recent references.&lt;/p&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;

&lt;li id=&#34;fn:2&#34;&gt;&lt;p&gt;In his seminal book on coherentism, &lt;a href=&#34;https://philpapers.org/rec/BONTSO-4&#34; target=&#34;_blank&#34;&gt;Bonjour
(1985)&lt;/a&gt; writes: &amp;ldquo;the
justification of a particular empirical belief finally depends, not
on other particular beliefs as the linear conception of
justification would have it, but instead on the overall system and
its coherence.&amp;rdquo; This doesn&amp;rsquo;t commit us to
assessing overall coherence before individual
justification. But that&amp;rsquo;s a natural conclusion you might come away with.&lt;/p&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;

&lt;li id=&#34;fn:3&#34;&gt;&lt;p&gt;We could count every proposition as pointing to itself. This would
mean putting $1$&amp;rsquo;s down the diagonal, i.e. adding the identity
matrix $\mathbf{I}$ to $\A$. This can be useful as a way to ensure
the limits we&amp;rsquo;ll require exist. But we&amp;rsquo;ll solve that problem
differently in the &amp;ldquo;Technical Background&amp;rdquo; section. And otherwise it
doesn&amp;rsquo;t really affect our results. It increases the leading
eigenvalue by $1$, but doesn&amp;rsquo;t affect the leading eigenvector.&lt;/p&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;

&lt;li id=&#34;fn:4&#34;&gt;&lt;p&gt;In general, the maximum possible centrality is $(k-1)/k$ in a
graph with $k$ non-$\top$ nodes.&lt;/p&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;

&lt;li id=&#34;fn:5&#34;&gt;&lt;p&gt;Hat tip to Erik J. Olsson&amp;rsquo;s &lt;a href=&#34;https://plato.stanford.edu/entries/justep-coherence/&#34; target=&#34;_blank&#34;&gt;entry on
coherentism&lt;/a&gt;
in the SEP, which uses this example in place of Klein &amp;amp; Warfield&amp;rsquo;s
slightly more involved one.&lt;/p&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:5&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;

&lt;li id=&#34;fn:6&#34;&gt;&lt;p&gt;Google&amp;rsquo;s founders used a variant of eigenvector centrality called
&amp;ldquo;PageRank&amp;rdquo; in their original search engine.&lt;/p&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:6&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Open Handbook of Formal Epistemology</title>
      <link>http://jonathanweisberg.org/post/open-handbook/</link>
      <pubDate>Wed, 26 Jun 2019 00:00:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/open-handbook/</guid>
      <description>&lt;p&gt;Today &lt;em&gt;The Open Handbook of Formal Epistemology&lt;/em&gt; is &lt;a href=&#34;http://jonathanweisberg.org/pdf/open-handbook-of-formal-epistemology.pdf&#34;&gt;available for download&lt;/a&gt;. It&amp;rsquo;s an open access book, the first published by PhilPapers itself. (The editors are &lt;a href=&#34;https://richardpettigrew.com/&#34; target=&#34;_blank&#34;&gt;Richard Pettigrew&lt;/a&gt; and me.)&lt;/p&gt;

&lt;p&gt;The book features 11 outstanding entries by 11 wonderful philosophers.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&amp;ldquo;Precise Credences&amp;rdquo;, by &lt;a href=&#34;https://sites.google.com/site/michaeltitelbaum/&#34; target=&#34;_blank&#34;&gt;Michael G. Titelbaum&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Decision Theory&amp;rdquo;, by &lt;a href=&#34;https://johannathoma.com/&#34; target=&#34;_blank&#34;&gt;Johanna Thoma&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Imprecise Probabilities&amp;rdquo;, by &lt;a href=&#34;http://www.lse.ac.uk/cpnss/people/anna-mahtani?from_serp=1&#34; target=&#34;_blank&#34;&gt;Anna Mahtani&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Primitive Conditional Probabilities&amp;rdquo;, by &lt;a href=&#34;http://www.kennyeaswaran.org/&#34; target=&#34;_blank&#34;&gt;Kenny Easwaran&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Infinitesimal Probabilities&amp;rdquo;, by &lt;a href=&#34;http://www.sylviawenmackers.be/&#34; target=&#34;_blank&#34;&gt;Sylvia Wenmackers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Comparative Probabilities&amp;rdquo;, by &lt;a href=&#34;https://jason-konek.squarespace.com/&#34; target=&#34;_blank&#34;&gt;Jason Konek&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Belief Revision Theory&amp;rdquo;, by &lt;a href=&#34;https://sites.google.com/site/hantilinphil/&#34; target=&#34;_blank&#34;&gt;Hanti Lin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Ranking Theory&amp;rdquo;, by &lt;a href=&#34;https://huber.blogs.chass.utoronto.ca/&#34; target=&#34;_blank&#34;&gt;Franz Huber&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Full &amp;amp; Partial Belief&amp;rdquo;, by &lt;a href=&#34;https://kgenin.github.io/&#34; target=&#34;_blank&#34;&gt;Konstantin Genin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Doxastic Logic&amp;rdquo;, by &lt;a href=&#34;https://sites.google.com/site/caiemike/&#34; target=&#34;_blank&#34;&gt;Michael Caie&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Conditionals&amp;rdquo;, by &lt;a href=&#34;https://philosophy.stanford.edu/people/ray-briggs&#34; target=&#34;_blank&#34;&gt;R. A. Briggs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We wanted to include lots more, but didn&amp;rsquo;t want to hold up publication any longer. Hopefully a second edition will cover more.&lt;/p&gt;

&lt;p&gt;For me personally, a central aim of this project was to demonstrate a point about open access publishing and shared standards. The budget for this book was exactly $0.00, and this was only possible because we didn&amp;rsquo;t need a human typesetter.&lt;/p&gt;

&lt;p&gt;Pretty much everyone in formal epistemology uses the same, standardized format to do their writing. And that format plugs in to a high-quality, freely available &lt;a href=&#34;https://en.wikipedia.org/wiki/TeX&#34; target=&#34;_blank&#34;&gt;typesetting program&lt;/a&gt;. So all you have to do to turn a dozen contributions from different authors into a unified book is paste them into a template and click &amp;ldquo;typeset&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Ok, it did actually take some noodling to iron out the kinks. But mainly just because of my poor planning. Having done it once now and learned the gotchas, a second go would come pretty close to the copy→paste→typeset dream.&lt;/p&gt;

&lt;p&gt;So for me, the moral is that philosophers in general should settle on a similar standard  (all academics, really). If we did, we&amp;rsquo;d have a lot more freedom from commercial publishers. We could publish open access books like this on the regular. The books would be freely and easily available to all, and authors would retain copyright.&lt;/p&gt;

&lt;p&gt;Collective action problems plague academia, and philosophical publishing in particular. But this one&amp;rsquo;s about as close to an opportunity for a major Pareto improvement as we&amp;rsquo;re likely to get.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Prestige and Placement in North American Philosophy</title>
      <link>http://jonathanweisberg.org/post/prestige-placement/</link>
      <pubDate>Tue, 04 Jun 2019 00:00:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/prestige-placement/</guid>
      <description>

&lt;p&gt;How does prestige correlate with placement in academic philosophy?
There&amp;rsquo;s good stuff on this already, like &lt;a href=&#34;http://placementdata.com:8182/the-philosophical-gourmet-report-and-placement/&#34; target=&#34;_blank&#34;&gt;this
post&lt;/a&gt;
by Carolyn Dicey Jennings, Pablo Contreras Kallens, and Justin
Vlasits.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; This post uses the same data sources, but emphasizes
different things (visualization, North American PhDs, and primarily
tenure-track jobs).&lt;/p&gt;

&lt;h1 id=&#34;tt-placement-in-north-america&#34;&gt;TT Placement in North America&lt;/h1&gt;

&lt;p&gt;Let&amp;rsquo;s start with a simple question of broad interest. In North America,
how well does the &lt;a href=&#34;https://www.philosophicalgourmet.com/&#34; target=&#34;_blank&#34;&gt;PGR&lt;/a&gt; rating of
one&amp;rsquo;s PhD-granting program predict one&amp;rsquo;s chances of landing a
tenure-track (TT) job?&lt;/p&gt;

&lt;p&gt;Consider all the people who got a PhD from a North American philosophy
program in the years 2012&amp;ndash;14.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; Focus for now on those from PhD
programs ranked by the 2006 edition of the PGR.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; Now group them
according to those PGR ratings, rounded to the nearest 0.5.&lt;/p&gt;

&lt;p&gt;This gives us 7 groups of PhDs (rankings range from 2.0 to 5.0).
According to &lt;a href=&#34;http://placementdata.com&#34; target=&#34;_blank&#34;&gt;the APDA&amp;rsquo;s data&lt;/a&gt;, the portion
from each group who ended up in TT jobs are as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/prestige-placement/unnamed-chunk-1-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s clearly a positive connection; almost perfectly linear in fact.
And the gist&amp;mdash;&lt;em&gt;very crudely speaking&lt;/em&gt;&amp;mdash;is that a high prestige PhD
about doubles your chances of landing a TT job over a low-prestige PhD:
from ~30% to ~60%.&lt;/p&gt;

&lt;p&gt;Note that the data are sparse at the extremes though. Consider this raw
look, where each point is a PhD graduate.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/prestige-placement/unnamed-chunk-2-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;A &amp;ldquo;violin plot&amp;rdquo; shows the same thing but easier to read: the thickness
of the violins indicates the density of points at each &lt;em&gt;x&lt;/em&gt;-position.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/prestige-placement/unnamed-chunk-3-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;With so few points at the ends, we shouldn&amp;rsquo;t read too much into the
exact placement rates there.&lt;/p&gt;

&lt;h1 id=&#34;other-placement-types&#34;&gt;Other Placement Types&lt;/h1&gt;

&lt;p&gt;What about other kinds of jobs? Let&amp;rsquo;s consider five categories, defined
as follows.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Postdoc: &amp;ldquo;Fellowship/Postdoc&amp;rdquo; in the APDA database.&lt;/li&gt;
&lt;li&gt;Permanent: any of the following in the APDA database.

&lt;ul&gt;
&lt;li&gt;&amp;ldquo;Tenure-Track&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Lecturer (Permanent)&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Instructor (Permanent)&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Adjunct (Permanent)&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Other (Permanent)&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Tenure-Track: &amp;ldquo;Tenure-Track&amp;rdquo; in the APDA database.&lt;/li&gt;
&lt;li&gt;PhD Program: Tenure-Track at a PhD-granting program.&lt;/li&gt;
&lt;li&gt;PGR Ranked: Tenure-Track at a 2006 PGR-ranked program.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These are hardly perfect definitions, but they&amp;rsquo;re manageable with this
data while still being pretty informative.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/prestige-placement/unnamed-chunk-4-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Note that a graduate can appear in multiple categories (Tenure-Track is
a subset of Permanent, after all).&lt;/p&gt;

&lt;h1 id=&#34;unranked-programs&#34;&gt;Unranked Programs&lt;/h1&gt;

&lt;p&gt;What about PhD programs not ranked in the 2006 PGR?&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; The numbers may
be iffier here. Some programs have only one graduate listed for example,
a graduate who got a TT job. But there are only a few such programs, and
more than 600 graduates otherwise. So the numbers may still be good
approximations.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Postdoc&lt;/th&gt;
&lt;th&gt;Permanent&lt;/th&gt;
&lt;th&gt;TT&lt;/th&gt;
&lt;th&gt;PhD&lt;/th&gt;
&lt;th&gt;PGR&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0.13&lt;/td&gt;
&lt;td&gt;0.46&lt;/td&gt;
&lt;td&gt;0.39&lt;/td&gt;
&lt;td&gt;0.05&lt;/td&gt;
&lt;td&gt;0.01&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;If you&amp;rsquo;re curious which programs stand out among the unranked, here are
the top 10 by TT placement (excluding those with 5 or fewer graduates).&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Program&lt;/th&gt;
&lt;th&gt;N&lt;/th&gt;
&lt;th&gt;Postdoc&lt;/th&gt;
&lt;th&gt;Permanent&lt;/th&gt;
&lt;th&gt;TT&lt;/th&gt;
&lt;th&gt;PhD&lt;/th&gt;
&lt;th&gt;PGR&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;The Catholic University of America&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;td&gt;0.91&lt;/td&gt;
&lt;td&gt;0.91&lt;/td&gt;
&lt;td&gt;0.18&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Baylor University&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;0.15&lt;/td&gt;
&lt;td&gt;0.92&lt;/td&gt;
&lt;td&gt;0.85&lt;/td&gt;
&lt;td&gt;0.08&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;DePaul University&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;td&gt;0.82&lt;/td&gt;
&lt;td&gt;0.82&lt;/td&gt;
&lt;td&gt;0.09&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;University of Tennessee&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;td&gt;0.77&lt;/td&gt;
&lt;td&gt;0.77&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;University of New Mexico&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;td&gt;0.71&lt;/td&gt;
&lt;td&gt;0.71&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Vanderbilt University&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;0.11&lt;/td&gt;
&lt;td&gt;0.67&lt;/td&gt;
&lt;td&gt;0.67&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;University of South Florida&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;0.13&lt;/td&gt;
&lt;td&gt;0.60&lt;/td&gt;
&lt;td&gt;0.60&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Florida State University&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;0.17&lt;/td&gt;
&lt;td&gt;0.58&lt;/td&gt;
&lt;td&gt;0.58&lt;/td&gt;
&lt;td&gt;0.08&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;University of Oregon&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;0.17&lt;/td&gt;
&lt;td&gt;0.67&lt;/td&gt;
&lt;td&gt;0.58&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;University of Kansas&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;td&gt;0.67&lt;/td&gt;
&lt;td&gt;0.56&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Note that the top 3 are at Christian universities, and as you might
expect, a lot of their placement is driven by hires at Christian
schools.&lt;/p&gt;

&lt;p&gt;Here are the 10 &amp;ldquo;largest&amp;rdquo; programs, i.e. those with the most graduates
listed in the APDA database.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Program&lt;/th&gt;
&lt;th&gt;N&lt;/th&gt;
&lt;th&gt;Postdoc&lt;/th&gt;
&lt;th&gt;Permanent&lt;/th&gt;
&lt;th&gt;TT&lt;/th&gt;
&lt;th&gt;PhD&lt;/th&gt;
&lt;th&gt;PGR&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Boston College&lt;/td&gt;
&lt;td&gt;28&lt;/td&gt;
&lt;td&gt;0.21&lt;/td&gt;
&lt;td&gt;0.68&lt;/td&gt;
&lt;td&gt;0.50&lt;/td&gt;
&lt;td&gt;0.04&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;The New School&lt;/td&gt;
&lt;td&gt;26&lt;/td&gt;
&lt;td&gt;0.23&lt;/td&gt;
&lt;td&gt;0.31&lt;/td&gt;
&lt;td&gt;0.27&lt;/td&gt;
&lt;td&gt;0.08&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Purdue University&lt;/td&gt;
&lt;td&gt;22&lt;/td&gt;
&lt;td&gt;0.14&lt;/td&gt;
&lt;td&gt;0.32&lt;/td&gt;
&lt;td&gt;0.27&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Stony Brook University&lt;/td&gt;
&lt;td&gt;22&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;td&gt;0.55&lt;/td&gt;
&lt;td&gt;0.41&lt;/td&gt;
&lt;td&gt;0.05&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Emory University&lt;/td&gt;
&lt;td&gt;21&lt;/td&gt;
&lt;td&gt;0.52&lt;/td&gt;
&lt;td&gt;0.62&lt;/td&gt;
&lt;td&gt;0.52&lt;/td&gt;
&lt;td&gt;0.10&lt;/td&gt;
&lt;td&gt;0.05&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Southern Illinois University&lt;/td&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;0.05&lt;/td&gt;
&lt;td&gt;0.30&lt;/td&gt;
&lt;td&gt;0.30&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Duquesne University&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;td&gt;0.22&lt;/td&gt;
&lt;td&gt;0.50&lt;/td&gt;
&lt;td&gt;0.44&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Fordham University&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;td&gt;0.22&lt;/td&gt;
&lt;td&gt;0.44&lt;/td&gt;
&lt;td&gt;0.39&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Villanova University&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;td&gt;0.78&lt;/td&gt;
&lt;td&gt;0.39&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;University of Guelph&lt;/td&gt;
&lt;td&gt;17&lt;/td&gt;
&lt;td&gt;0.12&lt;/td&gt;
&lt;td&gt;0.06&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&#34;departmental-tt-placement&#34;&gt;Departmental TT Placement&lt;/h1&gt;

&lt;p&gt;Looking at placement rates by department raises the question: how well
does a department&amp;rsquo;s PGR rating predict its TT placement rate?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/prestige-placement/unnamed-chunk-8-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s a clear connection, but also a lot of variation. Which are the
programs that especially stand out from the trend? Suppressing the
sizing for visibility, we can label those programs above/below the
trendline by at least 0.2.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/prestige-placement/unnamed-chunk-9-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For complete listings of departmental placement rates, check out the
APDA&amp;rsquo;s infograms &lt;a href=&#34;http://www.placementdata.com/data/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The code for this analysis is available &lt;a href=&#34;https://github.com/jweisber/prestige-placement&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;&lt;p&gt;Also check out Figure 1 in &lt;a href=&#34;https://quod.lib.umich.edu/e/ergo/12405314.0005.010?view=text;rgn=main&#34; target=&#34;_blank&#34;&gt;this
paper&lt;/a&gt;
by Helen De Cruz.&lt;/p&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;

&lt;li id=&#34;fn:2&#34;&gt;&lt;p&gt;Why these years? Because that&amp;rsquo;s where the data is best. The APDA
has focused its collection efforts so far on graduates from the
years 2012&amp;ndash;16, so that&amp;rsquo;s where the data is the most plentiful. But
the data for 2015 and 2016 graduates probably aren&amp;rsquo;t &amp;ldquo;ripe&amp;rdquo; enough
yet for our purposes; many graduates who will ultimately find TT
jobs are probably still in postdocs and other temporary gigs. Thanks
to Brian Weatherson for &lt;a href=&#34;https://twitter.com/bweatherson/status/1134570972315508736&#34; target=&#34;_blank&#34;&gt;pushing me to take this into
account&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Of course, the 2012&amp;ndash;2014 data aren&amp;rsquo;t fully ripe either. But
&lt;a href=&#34;https://jonathanweisberg.org/post/prestige-and-time-to-tt/&#34; target=&#34;_blank&#34;&gt;previous
noodling&lt;/a&gt;
suggests they&amp;rsquo;re probably pretty close.&lt;/p&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;

&lt;li id=&#34;fn:3&#34;&gt;&lt;p&gt;Why the 2006 edition? Partly for continuity with &lt;a href=&#34;http://placementdata.com:8182/the-philosophical-gourmet-report-and-placement/&#34; target=&#34;_blank&#34;&gt;the APDA&amp;rsquo;s own
analysis&lt;/a&gt;.
But also because students often use PGR rankings to choose PhD
programs, and the rankings available to them typically predate the
year of their PhD by 6 or 7 years.&lt;/p&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;

&lt;li id=&#34;fn:4&#34;&gt;&lt;p&gt;Thanks to &lt;a href=&#34;https://philosopherscocoon.typepad.com/blog/2019/05/phd-program-prestige-and-tt-placement.html?cid=6a014e89cbe0fd970d0240a48b2f6b200d#comment-6a014e89cbe0fd970d0240a48b2f6b200d&#34; target=&#34;_blank&#34;&gt;Amanda at the Philosophers&amp;rsquo;
Cocoon&lt;/a&gt;
for prompting me to look at this.&lt;/p&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Nobody Expects the Chance Function!</title>
      <link>http://jonathanweisberg.org/post/samet-theorem/</link>
      <pubDate>Fri, 15 Feb 2019 11:52:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/samet-theorem/</guid>
      <description>

&lt;p&gt;Here&amp;rsquo;s a striking result that caught me off guard the other day. It came up in a facebook thread, and judging by the discussion there it caught a few other people in this neighbourhood off guard too.&lt;/p&gt;

&lt;p&gt;The short version: chances are &amp;ldquo;self-expecting&amp;rdquo; pretty much if and only if they&amp;rsquo;re &amp;ldquo;self-certain&amp;rdquo;. Less cryptically: the chance of a proposition equals its expected chance just in case the chance function assigns probability 1 to itself being the true chance function, modulo an exception to be discussed below.&lt;/p&gt;

&lt;p&gt;The same result applies to any probabilities of course, whether they represent physical chances or evidential probabilities or whatever. In fact, thanks to friends on facebook, I learned that it drives &lt;a href=&#34;https://philpapers.org/archive/DOREAG.pdf&#34; target=&#34;_blank&#34;&gt;this lovely paper by Kevin Dorst&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I just happened to stumble across it while thinking about chances, because Richard Pettigrew uses the assumption that chances are self-expecting in &lt;a href=&#34;http://fitelson.org/coherence/pettigrew_chance.pdf&#34; target=&#34;_blank&#34;&gt;his &lt;em&gt;Phil Review&lt;/em&gt; paper on accuracy and the Principal Principle&lt;/a&gt;. But later, in &lt;a href=&#34;https://philpapers.org/rec/PETAAT-7&#34; target=&#34;_blank&#34;&gt;his landmark book on accuracy&lt;/a&gt;, he switches to the requirement that they be self-certain. It turns out this isn&amp;rsquo;t a coincidence. The result we&amp;rsquo;re about to look at illuminates this shift.&lt;/p&gt;

&lt;p&gt;The result goes back to 1997 at least, in &lt;a href=&#34;https://homepages.cwi.nl/~jve/books/oldgass/high-order-beliefs.pdf&#34; target=&#34;_blank&#34;&gt;a paper by Dov Samet&lt;/a&gt;. Proving the full result is a bit more involved than what I&amp;rsquo;ll present here. For simplicity, I&amp;rsquo;ll only prove a special case at the end. But along the way we&amp;rsquo;ll look at some suggestive examples that illustrate the full version.&lt;/p&gt;

&lt;h1 id=&#34;the-chance-matrix&#34;&gt;The Chance Matrix&lt;/h1&gt;

&lt;p&gt;Imagine we have just four possible worlds, resulting from two tosses of a coin. What are the physical chances at each of the four possible worlds $HH$, $HT$, $TH$, and $TT$? $\newcommand{\mstar}{\mathfrak{m}^*} \newcommand{\C}{\mathbf{C}}$&lt;/p&gt;

&lt;p&gt;One natural thought is to apply Laplace&amp;rsquo;s classic rule of succession: given $s$ heads out of $n$ tosses, conclude that the probability of heads on each toss is $(s+1)/(n+2)$. So at $HH$-world for example, the chance of heads was $3/ 4$ on each toss.&lt;/p&gt;

&lt;p&gt;If we assume the tosses are independent, then $HH$-world had chance $(3/ 4)(3/ 4) = 9/16$ of being actual, according to the chance function at $HH$-world. Whereas $HT$-world had chance $(3/ 4)(1/ 4) = 3/16$ of being actual at $HH$-world. The full chance function at $HH$-world can be displayed as a column vector:
$$
\left(
    \begin{matrix}
        9/16\\&lt;br /&gt;
        3/16\\&lt;br /&gt;
        3/16\\&lt;br /&gt;
        1/16
    \end{matrix}
\right).
$$
Applying the same recipe at $HT$-world would give us a different column vector. And sticking the columns for all four worlds together, we get a $4 \times 4$ &lt;em&gt;chance matrix&lt;/em&gt; for our space of possible worlds:
$$
\mathbf{C} =
    \left(
        \begin{matrix}
            9/16 &amp;amp; 1/ 4 &amp;amp; 1/ 4 &amp;amp; 1/16\\&lt;br /&gt;
            3/16 &amp;amp; 1/ 4 &amp;amp; 1/ 4 &amp;amp; 3/16\\&lt;br /&gt;
            3/16 &amp;amp; 1/ 4 &amp;amp; 1/ 4 &amp;amp; 3/16\\&lt;br /&gt;
            1/16 &amp;amp; 1/ 4 &amp;amp; 1/ 4 &amp;amp; 9/16
        \end{matrix}
    \right).
$$
Each column gives the chances &lt;em&gt;at&lt;/em&gt; a world, while a row gives the chances &lt;em&gt;of&lt;/em&gt; a world. For example, entry $c_{14}$ gives the chance &lt;em&gt;at&lt;/em&gt; world $4$ &lt;em&gt;of&lt;/em&gt; world $1$ being actual. It says how likely the sequence $HH$ was if the actual unfolding of events is instead $TT$, namely $1/16$.&lt;/p&gt;

&lt;p&gt;A different thought would be to appeal to Carnap&amp;rsquo;s notorious &amp;ldquo;logical&amp;rdquo; prior $\mstar$:
$$
\mstar =
    \left(
        \begin{matrix}
            1/3\\&lt;br /&gt;
            1/6\\&lt;br /&gt;
            1/6\\&lt;br /&gt;
            1/3
        \end{matrix}
    \right).
$$
This assignment of probabilities ignores the actual unfolding of events in each world. It falls out of a bit of a priori reasoning instead. There are three possible outcomes: $2$ heads, $1$ head, or $0$ heads. Each is equally likely, $1/ 3$. But there are two ways to get $1$ head, so the $1/ 3$ there gets subdivided equally between $HT$ and $TH$, leaving $1/6$ for each.&lt;/p&gt;

&lt;p&gt;Since these chances ignore the actual unfolding of events in each world, the chance matrix we get here is extremely anti-Humean. It&amp;rsquo;s just four repetitions of $\mstar$:
$$
\mathbf{C} =
    \left(
        \begin{matrix}
            1/3 &amp;amp; 1/3 &amp;amp; 1/3 &amp;amp; 1/3\\&lt;br /&gt;
            1/6 &amp;amp; 1/6 &amp;amp; 1/6 &amp;amp; 1/6\\&lt;br /&gt;
            1/6 &amp;amp; 1/6 &amp;amp; 1/6 &amp;amp; 1/6\\&lt;br /&gt;
            1/3 &amp;amp; 1/3 &amp;amp; 1/3 &amp;amp; 1/3
        \end{matrix}
    \right).
$$
You might think that&amp;rsquo;s a pretty terrible theory of chance, and I sympathize. But what we&amp;rsquo;re about to see is that, of our two chance matrices, only the second is &amp;ldquo;self-expecting&amp;rdquo;. And its terribleness is part of the reason why.&lt;/p&gt;

&lt;h1 id=&#34;self-expectation&#34;&gt;Self Expectation&lt;/h1&gt;

&lt;p&gt;Pettigrew&amp;rsquo;s &lt;em&gt;Phil Review&lt;/em&gt; paper assumes that chance functions are &amp;ldquo;self-expecting&amp;rdquo;. The chance of a proposition at a given world must equal its expected value, where the expectation is taken according to the chances at that world.&lt;/p&gt;

&lt;p&gt;In terms of a chance matrix $\C$, this amounts to the requirement that $\C \C = \C$. When we multiply $\C$ by $\C$, we take dot-products of rows and columns. For example, if we were doing the calculation by hand, we&amp;rsquo;d start by multiplying the first row of $\C$ by the first column of $\C$. And this is just the weighted average of the various possible chances &lt;em&gt;of&lt;/em&gt; the first world, where the weights are the chances &lt;em&gt;at&lt;/em&gt; that world. In other words, it&amp;rsquo;s the expected chance &lt;em&gt;of&lt;/em&gt; $HH$-world &lt;em&gt;at&lt;/em&gt; $HH$-world.&lt;/p&gt;

&lt;p&gt;In general, the dot product of row $i$ with column $j$ is the expected chance &lt;em&gt;of&lt;/em&gt; world $i$ &lt;em&gt;at&lt;/em&gt; world $j$. For this expected chance to equal the chance of world $i$ at world $j$, it must be that $\C \C = \C$. More succinctly, $\C^2 = \C$.&lt;/p&gt;

&lt;p&gt;Matrices that have this property&amp;mdash;squaring them leaves them unchanged&amp;mdash;are called &lt;em&gt;idempotent&lt;/em&gt;. And when our matrices are column stochastic (all values are nonnegative and each column sums to $1$), idempotence is a very&amp;hellip; well, potent requirement.&lt;/p&gt;

&lt;p&gt;For example, our first chance matrix based on Laplace&amp;rsquo;s rule of succession is not idempotent. Its square is not itself, but something quite different. Our second, Carnapian matrix is idempotent though. Its square is just itself. And that&amp;rsquo;s not a coincidence.&lt;/p&gt;

&lt;h1 id=&#34;self-certainty&#34;&gt;Self Certainty&lt;/h1&gt;

&lt;p&gt;Any chance matrix whose columns are redundant will be idempotent. After all, if the chances are the same &lt;em&gt;at&lt;/em&gt; every world, the expected value &lt;em&gt;of&lt;/em&gt; any world is always the same. So its expected value just is the value it has at every world.&lt;/p&gt;

&lt;p&gt;But redundant columns also mean that the chances are &lt;em&gt;self certain&lt;/em&gt;. Each world&amp;rsquo;s chance assignment gives zero probability to the chances being anything other than what they are at that world. Because there &lt;em&gt;are&lt;/em&gt; no worlds where the chances are different.&lt;/p&gt;

&lt;p&gt;The chances can vary from world to world and still be self-expecting though. There are idempotent chance matrices where the columns are not simply redundant. For example, here&amp;rsquo;s another idempotent chance matrix:
$$
\left(
    \begin{matrix}
        1/3 &amp;amp; 1/3 &amp;amp; 0      &amp;amp;    0\\&lt;br /&gt;
        2/3 &amp;amp; 2/3 &amp;amp; 0      &amp;amp;    0\\&lt;br /&gt;
            0 &amp;amp;   0 &amp;amp; 1/ 4 &amp;amp; 1/ 4\\&lt;br /&gt;
            0 &amp;amp;   0 &amp;amp; 3/ 4 &amp;amp; 3/ 4
    \end{matrix}
\right).
$$
But notice how it&amp;rsquo;s still kind of a degenerate case. There are two, disjoint regions of modal space here that regard one another as zero-chance. And within each region the chances are the same at each world. Worlds $1$ and $2$ have the same chances, and they give zero chance to worlds $3$ and $4$. And vice versa from the point of view of worlds $3$ and $4$.&lt;/p&gt;

&lt;p&gt;In other words, self-expectation and self-certainty go hand in hand here once again.&lt;/p&gt;

&lt;h1 id=&#34;a-sliver-of-daylight&#34;&gt;A Sliver of Daylight&lt;/h1&gt;

&lt;p&gt;Is there any daylight at all then between self-expectation and self-certainty?&lt;/p&gt;

&lt;p&gt;Self-certainty entails self-expectation, and the argument is pretty short. If the only worlds with positive chance according to world $j$ assign the same chances as world $j$ does, then any average of those chances will just be those same chances.&lt;/p&gt;

&lt;p&gt;But self-expectation doesn&amp;rsquo;t &lt;em&gt;quite&lt;/em&gt; entail self-certainty. For example, here&amp;rsquo;s an idempotent chance matrix that&amp;rsquo;s not self-certain:
$$
\left(
    \begin{matrix}
        1/3 &amp;amp; 1/3 &amp;amp; 0      &amp;amp;    0 &amp;amp; 25/94\\&lt;br /&gt;
        2/3 &amp;amp; 2/3 &amp;amp; 0      &amp;amp;    0 &amp;amp; 25/47\\&lt;br /&gt;
            0 &amp;amp;   0 &amp;amp; 1/ 4 &amp;amp; 1/ 4 &amp;amp; 19/376\\&lt;br /&gt;
            0 &amp;amp;   0 &amp;amp; 3/ 4 &amp;amp; 3/ 4 &amp;amp; 57/376\\&lt;br /&gt;
            0 &amp;amp;   0 &amp;amp;    0 &amp;amp;    0 &amp;amp; 0
    \end{matrix}
\right).
$$
It&amp;rsquo;s kind of a lame counterexample though, because the new, fifth world we&amp;rsquo;ve introduced (the coin explodes or something idk) has zero chance at every world, even itself.&lt;/p&gt;

&lt;p&gt;In fact this is what Samet proves: this is the only kind of counterexample possible! If probabilities are self-expecting, then they must be either self-certain or self-effacing. They must assign zero chance to the chances being otherwise, or they must assign chance one to them being otherwise.&lt;/p&gt;

&lt;p&gt;In terms of matrices, there are only three kinds of idempotent chance matrix:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;All columns are identical.&lt;/li&gt;
&lt;li&gt;The matrix is &lt;a href=&#34;https://en.wikipedia.org/wiki/Block_matrix#Block_diagonal_matrices&#34; target=&#34;_blank&#34;&gt;block diagonal&lt;/a&gt;, with identical columns inside each block.&lt;/li&gt;
&lt;li&gt;The matrix is as in (2), except for some columns $j_1, \ldots, j_n$. But the corresponding rows $j_1, \ldots, j_n$ contain only zeros.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Strictly speaking (1) is actually a special case of (2). But (1) deserves direct attention because it arises in a way that&amp;rsquo;s interesting both philosophically and mathematically.&lt;/p&gt;

&lt;h1 id=&#34;the-connected-case&#34;&gt;The Connected Case&lt;/h1&gt;

&lt;p&gt;Here&amp;rsquo;s a natural thought, one that&amp;rsquo;s driven a lot of the literature on chance and Lewis&amp;rsquo; Principal Principle. The thought: however events unfold at one world, there&amp;rsquo;s a chance they could have evolved differently. There&amp;rsquo;s even some small, non-zero chance they could have evolved quite differently.&lt;/p&gt;

&lt;p&gt;Taking this thought a bit further, you might think there&amp;rsquo;s a region of modal space where, even though worlds $w_1$ and $w_n$ have different chances, $w_n$ is always reachable from world $w_1$. More exactly, there&amp;rsquo;s always a connecting sequence of worlds $w_1, w_2, \ldots, w_n$ where $w_i$ gives non-zero chance to $w_{i+1}$.&lt;/p&gt;

&lt;p&gt;In terms of coin tosses, maybe it&amp;rsquo;s a law of nature that all coins land heads when all hundred out of one hundred flips land heads. But when there&amp;rsquo;s a mix of heads and tails, there&amp;rsquo;s at least some chance the mix could have had a few more heads, or a few more tails. So every world where the sequence isn&amp;rsquo;t perfectly uniform can be reached from every other. If not in a single, positive-chance hop, then at least by a series of hops, perhaps by switching the outcomes of the flips one at a time for example.&lt;/p&gt;

&lt;p&gt;In terms of graphs, such a region of modal space is said to be &lt;em&gt;connected&lt;/em&gt;. In terms of matrices, it amounts to the chance matrix for this region being &lt;em&gt;regular&lt;/em&gt;: there must be some power $n$ such that $\C^n$ contains all positive entries.&lt;/p&gt;

&lt;p&gt;Now, regular matrices have the remarkable property that, as we multiply them against themselves more and more times, the result converges to a matrix $\mathbf{P}$ whose columns are all identical:
$$ \lim_{n \rightarrow \infty} \C^n = \mathbf{P} =
\left(
    \begin{matrix}
        p_1    &amp;amp; \ldots &amp;amp; p_1    \\&lt;br /&gt;
        \vdots &amp;amp; \ldots &amp;amp; \vdots \\&lt;br /&gt;
        p_k    &amp;amp; \ldots &amp;amp; p_k    \\&lt;br /&gt;
    \end{matrix}
\right).
$$
Now recall that for $\C$ to be self-expecting, it must be idempotent, meaning $\C^2 = \C$. But that means $\C^n = \C$ for any power $n$. But then $\C = \mathbf{P}$, so $\C$ must already have redundant columns.&lt;/p&gt;

&lt;p&gt;What does this mean for us? One way to think about it: there isn&amp;rsquo;t as much room for chances to vary from world to world as one might have thought. If the chances are going to be self-expecting, they must be the same at every world across a whole region of modal space despite the facts turning out quite differently at various worlds across that region.&lt;/p&gt;

&lt;p&gt;This point is strongly reminiscent of David Lewis&amp;rsquo; famous &amp;ldquo;Big Bad Bug&amp;rdquo; of course. And there&amp;rsquo;s tons of relevant literature, most of which I confess I never really absorbed. So I&amp;rsquo;ll close by linking to just one paper I&amp;rsquo;m finding especially helpful on this right now, Richard Pettigrew&amp;rsquo;s &lt;a href=&#34;https://drive.google.com/file/d/0B-Gzj6gcSXKrcW5MeUhIN2Jsd1k/view&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;What Chance-Credence Norms Should Not Be&amp;rdquo;&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Prestige and Time to the Tenure-Track</title>
      <link>http://jonathanweisberg.org/post/prestige-and-time-to-tt/</link>
      <pubDate>Thu, 03 Jan 2019 00:00:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/prestige-and-time-to-tt/</guid>
      <description>

&lt;p&gt;How much does a PhD from a prestigious program help you on the job
market in academic philosophy?&lt;/p&gt;

&lt;p&gt;It makes a big difference to &lt;em&gt;where&lt;/em&gt; you get a tenure-track job, if you
do get one (see
&lt;a href=&#34;https://quod.lib.umich.edu/e/ergo/12405314.0005.010/--prestige-bias-an-obstacle-to-a-just-academic-philosophy?rgn=main;view=fulltext#4&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;).
It also seems to make some difference to &lt;em&gt;whether&lt;/em&gt; you get a
tenure-track job (though maybe not as much as one might have thought:
see
&lt;a href=&#34;http://placementdata.com:8182/the-philosophical-gourmet-report-and-placement/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;But here I want to consider whether it makes a difference to &lt;em&gt;how long&lt;/em&gt;
it takes to get a tenure-track job, if you do get one.&lt;/p&gt;

&lt;p&gt;A priori I would have guessed a prestigious PhD shortens one&amp;rsquo;s expected
time on the job market. But in some correspondence about my
&lt;a href=&#34;https://jonathanweisberg.org/post/page-rank-1/&#34; target=&#34;_blank&#34;&gt;last&lt;/a&gt;
&lt;a href=&#34;https://jonathanweisberg.org/post/page-rank-2/&#34; target=&#34;_blank&#34;&gt;two&lt;/a&gt; posts, the
oppposite hypothesis came up. The thought is that prestigious PhDs are
more likely to make stopovers in postdocs and fellowships before
arriving at the tenure track. But then, on the third hand, maybe less
prestigious PhDs are actually more likely to make stopovers, just in
less cushy temporary gigs.&lt;/p&gt;

&lt;p&gt;So which is it: does PhD prestige shorten expected time to the tenure
track, or lengthen it?&lt;/p&gt;

&lt;p&gt;Curiously, the answer so far seems to be: neither. If we use PGR ratings
to measure prestige, and data from the APDA homepage to measure
time-to-TT, there seems to be almost no connection. (Again, that&amp;rsquo;s
assuming you &lt;em&gt;do&lt;/em&gt; end up in a tenure-track job, which a prestigious PhD
does seem to help with at least a bit.)&lt;/p&gt;

&lt;p&gt;Because this conclusion surprised me so much (and because it&amp;rsquo;s likely to
be controversial, even weaponized in the PGR wars), I&amp;rsquo;m going to make
this post way more pedantic than usual. I&amp;rsquo;m going to walk through the
analysis one step at a time, and even show all the code.&lt;/p&gt;

&lt;p&gt;Hopefully, this way, careful readers will catch and correct any errors.
And non-careful readers will be so turned off they&amp;rsquo;ll shut up and go
away (instead of @-ing me with their volcanic takes on twitter or
stirring up shit elsewhere in the blogosphere).&lt;/p&gt;

&lt;p&gt;Ok let&amp;rsquo;s get started.&lt;/p&gt;

&lt;h1 id=&#34;setting-up&#34;&gt;Setting Up&lt;/h1&gt;

&lt;p&gt;As usual I&amp;rsquo;m working in R, and since I&amp;rsquo;m a &lt;a href=&#34;http://hadley.nz/&#34; target=&#34;_blank&#34;&gt;Hadley&lt;/a&gt;
stan we start by importing the &lt;code&gt;tidyverse&lt;/code&gt; package. But we&amp;rsquo;ll also
import the &lt;code&gt;tools&lt;/code&gt; package for its &lt;code&gt;toTitleCase&lt;/code&gt; function, so that we
can enforce consistent capitalization. We&amp;rsquo;ll be combining data from two
different sources, the PGR and the APDA, so we&amp;rsquo;ll need the names of PhD
programs capitalized the same way in order to &lt;code&gt;join&lt;/code&gt; the two datasets
properly.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)
library(tools)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next we import the APDA data. This is the same data scraped from the
APDA homepage in my &lt;a href=&#34;https://jonathanweisberg.org/post/page-rank-2/&#34; target=&#34;_blank&#34;&gt;last
post&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_apda &amp;lt;- read_csv(&amp;quot;data/apda-2018-11-9.csv&amp;quot;,
                    col_types = cols(
                      grad_program = col_character(),
                      id = col_character(),
                      year_graduated = col_character(),
                      aos = col_character(),
                      year = col_integer(),
                      placement_program = col_character(),
                      type = col_character()
                    ))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It needs a bit of cleaning: we standardize the capitalization, fix a few
typos&amp;hellip; but we also filter out duplicate entries, entries where the
year of the placement is missing or unknown, and entries where the type
(TT/postdoc/etc.) is missing:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_apda$grad_program &amp;lt;- toTitleCase(df_apda$grad_program)

df_apda$year[df_apda$year == 19182] &amp;lt;- 1982
df_apda$year[df_apda$year == 2104] &amp;lt;- 2014

df_apda &amp;lt;- df_apda %&amp;gt;%
  distinct() %&amp;gt;%
  filter(!is.na(year)) %&amp;gt;%
  filter(year_graduated != &amp;quot;Current Student or Graduation Year Unknown&amp;quot;) %&amp;gt;%
  filter(!is.na(type))

df_apda$year_graduated &amp;lt;- as.integer(df_apda$year_graduated)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next we calculate &lt;code&gt;time_to_tt&lt;/code&gt; as follows: (i) filter out all but TT
placements, (ii) select the first placement (by year) for each graduate,
and (iii) subtract the year they graduated from that:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_apda &amp;lt;- df_apda %&amp;gt;%
  filter(type == &amp;quot;Tenure-Track&amp;quot;) %&amp;gt;%
  group_by(grad_program, id) %&amp;gt;%
  arrange(year) %&amp;gt;%
  slice(1) %&amp;gt;%
  mutate(time_to_tt = year - year_graduated) %&amp;gt;%
  ungroup()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For a glimpse of the results, here are ten randomly chosen entries:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(42)
sample_n(df_apda, 10) %&amp;gt;% 
  select(grad_program, year_graduated, year, time_to_tt)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 4
##    grad_program                        year_graduated  year time_to_tt
##    &amp;lt;chr&amp;gt;                                        &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;
##  1 University of Virginia                        2011  2014          3
##  2 University of Wisconsin-Madison               2013  2016          3
##  3 New York University                           2016  2015         -1
##  4 University of Pittsburgh                      2014  2014          0
##  5 University of Edinburgh                       2015  2017          2
##  6 University of California, Davis               2003  2009          6
##  7 University of Minnesota Twin Cities           2011  2015          4
##  8 Emory University                              2007  2007          0
##  9 University of Illinois at Chicago             1996  2005          9
## 10 University of Memphis                         2012  2014          2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we import the PGR data:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_pgr &amp;lt;- read_csv(&amp;quot;data/pgr/pgr.csv&amp;quot;, 
                   col_types = cols(program = col_character(),
                                    mean = col_double(), 
                                    locale = col_character(), 
                                    year = col_integer()))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A random glimpse again:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sample_n(df_pgr, 10)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 4
##    program                             mean locale       year
##    &amp;lt;chr&amp;gt;                              &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;       &amp;lt;int&amp;gt;
##  1 University of British Columbia       3   Canada       2017
##  2 University of Missouri               2.2 US           2008
##  3 University of Wisconsin-Madison      3.2 US           2008
##  4 Ohio State University                3.1 US           2008
##  5 University of British Columbia       2.6 Canada       2011
##  6 York University                      1.8 Canada       2008
##  7 Washington University in St. Louis   3.3 US           2017
##  8 Florida State University             2.3 US           2014
##  9 University of Calgary                2.3 Canada       2017
## 10 University of Canterbury             1.5 Australasia  2008
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that we have ratings from the last six iterations of the PGR here:
2006, 2008, 2009, 2011, 2014, 2017. So we can pair graduates with PGR
ratings from around the time they went on the market.&lt;/p&gt;

&lt;p&gt;More precisely, we&amp;rsquo;ll assign each graduate a &lt;code&gt;pgr_year&lt;/code&gt; corresponding to
the latest report available the year they graduated. Unless they
graduated prior to 2006 in which case we just use the 2006 ratings:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pgr_years &amp;lt;- unique(df_pgr$year)
df_apda &amp;lt;- df_apda %&amp;gt;%
  group_by(grad_program, id) %&amp;gt;%
  mutate(pgr_year = ifelse(year_graduated &amp;lt;= 2006,
                           2006,
                           max(pgr_years[pgr_years &amp;lt;= year_graduated]))) %&amp;gt;%
  ungroup()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(Yes yes we could get earlier PGR data but frankly I&amp;rsquo;m too lazy to go
through all that again. Most of our graduates are from 2006 or later
anyway, virtually all are from 2000 or later, and PGR scores don&amp;rsquo;t
change too much from iteration to iteration.)&lt;/p&gt;

&lt;p&gt;Finally, we join our two tables together, matching by program name and
year:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df &amp;lt;- df_apda %&amp;gt;%
  inner_join(df_pgr, 
             by = c(&amp;quot;grad_program&amp;quot; = &amp;quot;program&amp;quot;, &amp;quot;pgr_year&amp;quot; = &amp;quot;year&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Phew, ok! We finally have a table of graduates from PGR-ranked programs
who got TT jobs, along with the PGR-rating of their program around that
time, and the time it took them to land their first TT job:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sample_n(df, 10) %&amp;gt;% select(grad_program, mean, time_to_tt)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 3
##    grad_program                                        mean time_to_tt
##    &amp;lt;chr&amp;gt;                                              &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;
##  1 University of Virginia                               2.7          1
##  2 Graduate Center of the City University of New York   3.6          3
##  3 Yale University                                      4            0
##  4 University of York                                   2.3          2
##  5 Cornell University                                   3.7          0
##  6 University of California, Los Angeles                4            0
##  7 Stanford University                                  3.9          1
##  8 University of Utah                                   2.2          3
##  9 University of Arizona                                3.7          1
## 10 University of Pittsburgh                             4            0
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;results&#34;&gt;Results&lt;/h1&gt;

&lt;p&gt;So, is there any correlation between PGR score and time to TT? Let&amp;rsquo;s
plot our data, with some random jitter added for visibility, and a LOESS
curve to capture trends:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(df, aes(mean, time_to_tt)) +
 geom_jitter() +
 xlab(&amp;quot;PGR Rating&amp;quot;) + ylab(&amp;quot;Years to First TT Job&amp;quot;) +
 geom_smooth(method = &amp;quot;loess&amp;quot;) +
 theme_minimal()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/prestige-and-time-to-tt/unnamed-chunk-7-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Wow, just&amp;hellip; flat. Pretty much all the way across. And calculating the
correlation, we get basically zero:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cor(df$mean, df$time_to_tt)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] -0.03016805
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are some pretty weird anomalies though, like that outlier way out
at the top right. Who took over 50 years to get their first TT job?!?
Turns out that&amp;rsquo;s David C. Makinson, author of the classic 1965 paper,
&lt;a href=&#34;https://doi.org/10.1093/analys/25.6.205&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;The Paradox of the Preface&amp;rdquo;&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;What on earth happened here? Makinson graduated from Oxford in 1965, and
recently took up a visiting professorship at LSE in 2017, a difference
of 52 years. That wasn&amp;rsquo;t his first tenured gig of course. But it&amp;rsquo;s the
only one listed in the APDA database.&lt;/p&gt;

&lt;p&gt;I guess any analysis of this size is bound to include at least a few
such errors. (Shut up, I&amp;rsquo;m a dad.) But let&amp;rsquo;s try to minimize them.&lt;/p&gt;

&lt;p&gt;As a first stab, we might just cut out any extreme outliers, say all
points where &lt;code&gt;time_to_tt&lt;/code&gt; came out less than -2 or greater than 8.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df2 &amp;lt;- df %&amp;gt;% filter(-2 &amp;lt;= time_to_tt &amp;amp; time_to_tt &amp;lt;= 8)

ggplot(df2, aes(mean, time_to_tt)) +
 geom_jitter() +
 xlab(&amp;quot;PGR Rating&amp;quot;) + ylab(&amp;quot;Years to First TT Job&amp;quot;) +
 geom_smooth(method = &amp;quot;loess&amp;quot;) +
 theme_minimal()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/prestige-and-time-to-tt/unnamed-chunk-9-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Things look a bit less flat now. Maybe there&amp;rsquo;s a slight dip in
time-to-TT once PGR rating gets above 3 or so. But if so, it looks very
small. And the overall correlation is again pretty much zero:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cor(df2$mean, df2$time_to_tt)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] -0.06784644
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ok, one more try. Things work differently in different parts of the
world. PhD programs and job markets differ in their practices between
the US and the UK, between the UK and the rest of Europe, etc. So let&amp;rsquo;s
try looking at just the US (the largest locale we have data for and the
one I understand best):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df3 &amp;lt;- df2 %&amp;gt;% filter(locale == &amp;quot;US&amp;quot;)

ggplot(df3, aes(mean, time_to_tt)) +
 geom_jitter() +
 xlab(&amp;quot;PGR Rating&amp;quot;) + ylab(&amp;quot;Years to First TT Job&amp;quot;) +
 geom_smooth(method = &amp;quot;loess&amp;quot;) +
 theme_minimal()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/prestige-and-time-to-tt/unnamed-chunk-11-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Again, it looks like there&amp;rsquo;s only a very slight dip as PGR rating
exceeds 3, with a correlation very close to zero:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cor(df3$mean, df3$time_to_tt)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] -0.06922355
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;We could go on slicing and dicing, and I don&amp;rsquo;t by any means see this as
the last word. But my expectations have certainly been shaken up here.
If there is a connection between prestige and time-to-TT, the available
evidence suggests it&amp;rsquo;s much weaker than I would have guessed, and
perhaps depends a lot on other factors.&lt;/p&gt;

&lt;p&gt;Hopefully we&amp;rsquo;ll get a clearer picture of all this as the APDA database
grows. Their focus so far &lt;a href=&#34;http://placementdata.com:8182/the-philosophical-gourmet-report-and-placement/&#34; target=&#34;_blank&#34;&gt;has been on graduates from
2012&amp;ndash;2016&lt;/a&gt;.
So the earlier data is probably much less complete (see: Makinson). And
not enough time has passed yet for the 2012&amp;ndash;2016 data to be as
informative about time-to-TT.&lt;/p&gt;

&lt;p&gt;So even though this conclusion is striking, I still view it as
tentative. #JustOneStudy&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Placement, PageRank, and the PGR: Part 2</title>
      <link>http://jonathanweisberg.org/post/page-rank-2/</link>
      <pubDate>Thu, 06 Dec 2018 00:00:01 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/page-rank-2/</guid>
      <description>

&lt;p&gt;This post is the second of two devoted to &lt;a href=&#34;https://bit.ly/2z12SoB&#34; target=&#34;_blank&#34;&gt;an idea of David
Wallace’s&lt;/a&gt;: applying Google’s PageRank algorithm
to the &lt;a href=&#34;http://www.placementdata.com/&#34; target=&#34;_blank&#34;&gt;APDA&lt;/a&gt; placement data.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://jonathanweisberg.org/post/page-rank-1&#34;&gt;Part 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Part 2&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/jweisber/page-rank&#34; target=&#34;_blank&#34;&gt;Source on GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;http://jonathanweisberg.org/post/page-rank-1/&#34;&gt;Last time&lt;/a&gt; we looked at the motivation and
theory behind the idea. Now we’ll try predicting PageRanks. Can students
who care about PageRank use the latest
&lt;a href=&#34;https://www.philosophicalgourmet.com/&#34; target=&#34;_blank&#34;&gt;PGR&lt;/a&gt; to guesstimate a program’s
PageRank 5 or 10 years in the future? Can they use the latest placement
data?&lt;/p&gt;

&lt;h1 id=&#34;the-data&#34;&gt;The Data&lt;/h1&gt;

&lt;p&gt;We’ll use a somewhat different data set from last time, since we need
things broken down by year. Our data comes now from &lt;a href=&#34;http://placementdata.com&#34; target=&#34;_blank&#34;&gt;the APDA
homepage&lt;/a&gt;, where you can search by PhD program
and get a list of jobs where its graduates have landed.&lt;/p&gt;

&lt;p&gt;Unfortunately the search results are in a pretty unfriendly format. This
means doing some nasty
&lt;a href=&#34;https://en.wikipedia.org/wiki/Web_scraping&#34; target=&#34;_blank&#34;&gt;scraping&lt;/a&gt;, a notoriously
error-prone process. And even before scraping, the data seems to have
some errors and quirks (duplicate entries, inconsistent capitalization,
missing values, etc.). I’ve patched what I can, but we should keep in
mind that we’re already working with a pretty noisy signal even before
we get to any analysis.&lt;/p&gt;

&lt;p&gt;A quick poke around before we get to the main event: how much data do we
have for each year? The years since 2010 really dominate this data set.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/page-rank-2/unnamed-chunk-2-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;That’s going to be a problem, when we try predicting future PageRank
based on past PageRank. Ideally we’d like to have two data-rich periods
separated by a 5-year span. Then we can see how well a hypothetical
prospective student would have done at predicting the PageRanks they’d
face on the job market, using the PageRanks available when they were
choosing a program. But we don’t have that. So we’ll have to live with
some additional noise when we get to this below.&lt;/p&gt;

&lt;p&gt;For now let’s turn to placement type. The APDA tracks whether a job is
TT, a postdoc, temporary letureship, etc. TT and postdoc placements
dominate the landscape, followed by temporary and non-academic
positions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/page-rank-2/unnamed-chunk-3-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Placement types change dramatically over the years though. From the
early to late naughties, TT changes from being the dominant type to a
minority.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/page-rank-2/unnamed-chunk-4-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Is this the story of the Great Recession? That’s likely a big factor.
But our data gets spottier as we go back through the naughties. So a
selection effect is another likely culprit.&lt;/p&gt;

&lt;p&gt;Now let’s turn to predicting PageRanks.&lt;/p&gt;

&lt;h1 id=&#34;predicting-pagerank-from-pgr&#34;&gt;Predicting PageRank from PGR&lt;/h1&gt;

&lt;p&gt;We’ll calculate all PageRanks on a five-year window (a somewhat
arbitrary choice). So the “future” PageRanks—the numbers we’ll be trying
to predict—are calculated from the placement data in the years
2014–2018.&lt;/p&gt;

&lt;p&gt;For now we’ll count postdocs as well as TT placements, for continuity
with the last post and with Wallace’s original analysis. We’ll rerun the
analysis using only TT jobs at the end.&lt;/p&gt;

&lt;p&gt;Given these choices, here are the top 10 programs by PageRank for
2014–18.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th style=&#34;text-align: left;&#34;&gt;Program&lt;/th&gt;
&lt;th style=&#34;text-align: right;&#34;&gt;PageRank 2014-18&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Princeton University&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0.0437282&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Stanford University&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0.0434163&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;New York University&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0.0408766&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;University of Oxford&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0.0374294&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;University of Arizona&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0.0367260&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Harvard University&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0.0323110&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;University of Pittsburgh (HPS)&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0.0296616&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Massachusetts Institute of Technology&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0.0278274&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;University of Chicago&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0.0277885&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;University of Toronto&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0.0260106&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;How well do PGR scores predict these rankings? Following Wallace we’ll
start with the 2006 PGR. But we’ll also consider the five iterations
since. I’ve put the PageRanks (the &lt;em&gt;y&lt;/em&gt;-axis) on a log scale for
visibility.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/page-rank-2/unnamed-chunk-6-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Overall the correlation looks pretty strong, ranging from 0.66 in 2006
to 0.73 in 2014. And that seems consistent with Wallace’s original
finding, a correlation of 0.75 between the 2006 PGR and all placement
data up through 2016.&lt;/p&gt;

&lt;p&gt;Notice though, the connection gets much weaker when PGR ratings are
lower. Below a PGR rating of 3 or so, PageRank doesn’t seem to increase
much with PGR rating: the average correlation is only 0.25.&lt;/p&gt;

&lt;p&gt;We have two conclusions so far then. First, as Wallace found, PGR rating
seems to predict PageRank pretty well, even when the ratings are
collected almost a decade in advance. But second, this effect is much
stronger for programs with high PGR ratings.&lt;/p&gt;

&lt;h1 id=&#34;predicting-pagerank-from-pagerank&#34;&gt;Predicting PageRank from PageRank&lt;/h1&gt;

&lt;p&gt;What if our hypothetical grad student had relied on the available
placement data instead of PGR scores in deciding where to go?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/page-rank-2/unnamed-chunk-8-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Well they’d probably have been better off using the PGR in 2006. But
PageRank gets pretty competitive with PGR starting around 2010, when the
placement data gets richer.&lt;/p&gt;

&lt;p&gt;It might be that PageRank has the potential to be a better
predictor—even long range—provided we have enough placement data. Or it
might be that PageRank is only better at close range. We can’t be sure,
but hopefully we’ll find out in a few years as the APDA database grows.&lt;/p&gt;

&lt;h1 id=&#34;tenure-track-only&#34;&gt;Tenure-track Only&lt;/h1&gt;

&lt;p&gt;Finally, let’s do the same analysis but counting only tenure-track
placements toward a department’s PageRank. Here are our top 10 programs
then (a more similar group to what we saw last time, note).&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th style=&#34;text-align: left;&#34;&gt;Program&lt;/th&gt;
&lt;th style=&#34;text-align: right;&#34;&gt;PageRank 2014-18&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Princeton University&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0.0837437&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;University of Pittsburgh (HPS)&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0.0640854&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;University of Oxford&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0.0573890&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;University of Chicago&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0.0519388&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Yale University&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0.0452552&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Harvard University&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0.0420271&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Massachusetts Institute of Technology&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0.0374587&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;Rutgers University&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0.0360075&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;New York University&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0.0335406&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;University of California, Berkeley&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0.0290834&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Here are the comparisons with past PGR scores.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/page-rank-2/unnamed-chunk-11-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;And here are past PageRanks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/page-rank-2/unnamed-chunk-12-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Unsurprisingly, restricting ourselves to TT placements makes things
noisier across the board. PGR-based predictions show more resilience
here, possibly because they’re only affected by the added noise at one
end.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Placement, PageRank, and the PGR: Part 1</title>
      <link>http://jonathanweisberg.org/post/page-rank-1/</link>
      <pubDate>Thu, 06 Dec 2018 00:00:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/page-rank-1/</guid>
      <description>

&lt;p&gt;This is the first of two posts devoted to &lt;a href=&#34;https://bit.ly/2z12SoB&#34; target=&#34;_blank&#34;&gt;an idea of David
Wallace&amp;rsquo;s&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Part 1&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://jonathanweisberg.org/post/page-rank-2&#34;&gt;Part 2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/jweisber/page-rank&#34; target=&#34;_blank&#34;&gt;Source on GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Suppose you pick a philosophy PhD program at random and you go visit
their website. There you pick a random person from the faculty list and
see where they got their PhD. Then you go to that program&amp;rsquo;s website and
repeat the exercise: pick a random faculty member, see where they did
their PhD, and go to that program&amp;rsquo;s website. And again, and again.&lt;/p&gt;

&lt;p&gt;You&amp;rsquo;d come back to some programs more often than others in the long run.
Which ones? Those whose graduates are in demand at other programs whose
graduates are in demand at still other programs etc. In other words, the
programs you keep coming back to are central in the hiring network of
philosophy PhD programs.&lt;/p&gt;

&lt;p&gt;This is the idea behind Google&amp;rsquo;s famous &amp;ldquo;PageRank&amp;rdquo; algorithm. In &lt;a href=&#34;https://ac.els-cdn.com/S1389128612003611/1-s2.0-S1389128612003611-main.pdf?_tid=a2d2e085-ca3d-4933-b0f3-fa0f383599df&amp;amp;acdnat=1541034827_134e15cd9b3a557c55ba481746397b93&#34; target=&#34;_blank&#34;&gt;a
classic
paper&lt;/a&gt;,
Google&amp;rsquo;s founders imagined a web surfer starting at a random page,
following a random link from there, then following a random link found
on the new page, and so on. Pages where the surfer winds up more often
are more likely to be of interest to the users of a search engine.&lt;/p&gt;

&lt;p&gt;Wallace&amp;rsquo;s idea was to apply the same algorithm to the
&lt;a href=&#34;http://www.placementdata.com/&#34; target=&#34;_blank&#34;&gt;APDA&lt;/a&gt;&amp;rsquo;s placement data. And he found
that the resultant rankings correlated closely with the &lt;a href=&#34;https://www.philosophicalgourmet.com/&#34; target=&#34;_blank&#34;&gt;Philosophical
Gourmet Report&lt;/a&gt;&amp;rsquo;s ratings from
2006.&lt;/p&gt;

&lt;p&gt;These posts expand on the idea in two parts. This post explains the
theory behind the PageRank algorithm, and reproduces Wallace&amp;rsquo;s rankings.
&lt;a href=&#34;http://jonathanweisberg.org/post/page-rank-2.html&#34;&gt;The next post&lt;/a&gt; considers the possibility of
predicting a department&amp;rsquo;s PageRank, using either past PGR scores, or
past placement data fed into the PageRank algorithm itself.&lt;/p&gt;

&lt;h1 id=&#34;motivation&#34;&gt;Motivation&lt;/h1&gt;

&lt;p&gt;Why care about a department&amp;rsquo;s PageRank? You might think it&amp;rsquo;s an
indicator of a program&amp;rsquo;s &amp;ldquo;quality&amp;rdquo;, but I&amp;rsquo;m more interested in its
potential use to students. Some want to become professors at programs
where they will train PhDs who go on to do the same.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m also just intrigued by the PageRank algorithm itself. It&amp;rsquo;s
mathematically very nifty, and it has a delightfully philosophical
flavour too. It takes a problem that looks like a vicious regress and
shows how to give it a virtuous grounding.&lt;/p&gt;

&lt;p&gt;So these posts are partly an exercise for me, to learn about the
algorithm and the math behind it. Plus I just like playing with data.&lt;/p&gt;

&lt;h1 id=&#34;theory&#34;&gt;Theory&lt;/h1&gt;

&lt;p&gt;There are two ways to understand the PageRank algorithm. First is the
random surfer idea already described. The second we might call the &amp;ldquo;vote
of confidence&amp;rdquo; model.&lt;/p&gt;

&lt;p&gt;When program X hires a graduate of program Y, that&amp;rsquo;s a vote of
confidence for program Y. But this vote carries more weight if program X
itself is well regarded. So we have to see where program X&amp;rsquo;s own
graduates have been placed, and determine how much confidence people
have in those programs. And so on.&lt;/p&gt;

&lt;p&gt;The threat of regress looms. How do we break out? With a dash of high
school algebra.&lt;/p&gt;

&lt;p&gt;Imagine we have just three programs, A, B, and C. Program A got 80% of
its faculty from B, and 10% each from C and from A itself. B is similar,
except&amp;hellip; well, here&amp;rsquo;s the whole story:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;A&amp;rsquo;s Faculty&lt;/th&gt;
&lt;th&gt;B&amp;rsquo;s Faculty&lt;/th&gt;
&lt;th&gt;C&amp;rsquo;s Faculty&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;PhD from A&lt;/td&gt;
&lt;td&gt;10%&lt;/td&gt;
&lt;td&gt;20%&lt;/td&gt;
&lt;td&gt;30%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;PhD from B&lt;/td&gt;
&lt;td&gt;80%&lt;/td&gt;
&lt;td&gt;70%&lt;/td&gt;
&lt;td&gt;40%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;PhD from C&lt;/td&gt;
&lt;td&gt;10%&lt;/td&gt;
&lt;td&gt;10%&lt;/td&gt;
&lt;td&gt;30%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Thinking row-wise, A cast 10% of its votes for A so to speak; B cast 20%
of its votes for A; and C cast 30% of its votes for A. But how much
weight does a vote from program A carry, compared to say a vote from B?
Labeling these unknown weights $w_A$, $w_B$, and $w_C$, we get a linear
equation: $$ w_A = .1 w_A + .2 w_B + .3 w_C. $$ Applying the same
formula to B and C, and we get a system of three linear equations, in
three unknowns: $$
  \begin{aligned}
    w_A &amp;amp;= .1 w_A + .2 w_B + .3 w_C.\\&lt;br /&gt;
    w_B &amp;amp;= .8 w_A + .7 w_B + .4 w_C.\\&lt;br /&gt;
    w_C &amp;amp;= .1 w_A + .1 w_B + .3 w_C.
  \end{aligned}
$$&lt;/p&gt;

&lt;p&gt;Solving this system we find that $(w_A, w_B, w_C) = (0.19, 0.68, 0.12)$
is the only solution with positive weights. So we&amp;rsquo;ve determined how much
weight a vote from each program carries. And unsurprisingly, votes from
program B carry by far the most weight.&lt;/p&gt;

&lt;p&gt;Will the same reasoning work no matter how many programs we have, and no
matter what their hiring patterns are like? More or less yes. We have to
upgrade the math a bit and add a small tweak. But the heart of the idea
is pretty much the same.&lt;/p&gt;

&lt;h2 id=&#34;general-solution&#34;&gt;General Solution&lt;/h2&gt;

&lt;p&gt;In the general case, we have $n$ programs and we want to give each
program $i$ a weight $w_i$ that reflects the votes of confidence it&amp;rsquo;s
received. Placement data tells us for any two programs $i$ and $j$ the
portion of faculty at program $j$ hired from program $i$, call it
$m_{ij}$. Since the weight of program $i$ is the weighted sum of these
numbers, we have a set of $n$ linear equations with $n$ unknowns:
$$
\begin{aligned}
m_{11} w_1 + m_{12} w_2 + \ldots + m_{1n} w_n &amp;amp;= w_1,\\&lt;br /&gt;
m_{21} w_1 + m_{22} w_2 + \ldots + m_{2n} w_n &amp;amp;= w_2,\\&lt;br /&gt;
  \vdots &amp;amp;  \\&lt;br /&gt;
m_{n1} w_1 + m_{n2} w_2 + \ldots + m_{nn} w_n &amp;amp;= w_n.
\end{aligned}
$$
In matrix terms, we&amp;rsquo;re looking to solve the equation
$\mathbf{M} \mathbf{w} = \mathbf{w}.$ This is an
&lt;a href=&#34;https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors&#34; target=&#34;_blank&#34;&gt;eigenvector&lt;/a&gt;
equation $\mathbf{M} \mathbf{w} = \lambda \mathbf{w}$, where the
eigenvalue $\lambda = 1$. And a &lt;a href=&#34;https://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem&#34; target=&#34;_blank&#34;&gt;famous
theorem&lt;/a&gt;
guarantees that a unique solution exists, given assumptions to be
discussed momentarily.&lt;/p&gt;

&lt;p&gt;The same theorem also guarantees that this solution is the result of the
random-surfing exercise we opened with. This is why we can understand
PageRank using either of our two interpretations. The weight $w_B$ will
also be the frequency with which our random surfer comes back to program
B&amp;rsquo;s faculty page.&lt;/p&gt;

&lt;p&gt;Why? Well suppose we start on some random program&amp;rsquo;s page, say program A.
We represent this with the vector $\mathbf{p} = (1, 0, 0)$, because
right now we&amp;rsquo;re on program A&amp;rsquo;s page with probability 1. Then we start
our random walk. What is the probability we&amp;rsquo;ll arrive at A, or B, or C
next? To find out we multiply $\mathbf{M}$ against $\mathbf{p}$: $$
  \begin{aligned}
    \mathbf{M} \mathbf{p}
      &amp;amp;=
        \left(
          \begin{matrix}
            0.1 &amp;amp; 0.2 &amp;amp; 0.3\\&lt;br /&gt;
            0.8 &amp;amp; 0.7 &amp;amp; 0.4\\&lt;br /&gt;
            0.1 &amp;amp; 0.1 &amp;amp; 0.3
          \end{matrix}
        \right)
        \left(
          \begin{matrix}
            1\\&lt;br /&gt;
            0\\&lt;br /&gt;
            0
          \end{matrix}
        \right)\\&lt;br /&gt;
      &amp;amp;=
        \left(
          \begin{matrix}
            0.1\\&lt;br /&gt;
            0.8\\&lt;br /&gt;
            0.1
          \end{matrix}
        \right).
  \end{aligned}
$$ To find out where we&amp;rsquo;ll probably be at the next step, we multiply
this result by $\mathbf{M}$ again, i.e. we compute
$\mathbf{M}(\mathbf{M}\mathbf{p})$. Since matrix multiplication is
associative, this is the same as $\mathbf{M}^2 \mathbf{p}$. And in
general the probabilities of the $k$-th step are given by
$\mathbf{M}^k \mathbf{p}$.&lt;/p&gt;

&lt;p&gt;Our theorem guarantees now (given conditions still to be specified):
$$\lim_{k \rightarrow \infty} \mathbf{M}^k \mathbf{p} = \mathbf{w},$$ where
$\mathbf{w}$ is the solution to $\mathbf{M}\mathbf{w} = \mathbf{w}$ we
found earlier. In other words, as the random walk progresses, the
portion of time spent at a program&amp;rsquo;s page converges to the weight that
program&amp;rsquo;s votes carry.&lt;/p&gt;

&lt;p&gt;Importantly, this convergence happens no matter where the random walk
starts. In fact we get the same convergence for &lt;em&gt;any&lt;/em&gt; probability vector
$\mathbf{p}$ we might start with. (A probability vector is a list of
non-negative numbers that sum to one.)&lt;/p&gt;

&lt;p&gt;So what conditions guarantee this happy convergence? Our matrix
$\mathbf{M}$ is
&lt;a href=&#34;https://en.wikipedia.org/wiki/Stochastic_matrix&#34; target=&#34;_blank&#34;&gt;stochastic&lt;/a&gt;, meaning
its columns are probability vectors: they contain nonnegative numbers
that sum to one. Given that, it suffices for $\mathbf{M}$ to be
&lt;a href=&#34;https://en.wikipedia.org/wiki/Regular_matrix&#34; target=&#34;_blank&#34;&gt;regular&lt;/a&gt;, meaning
$\mathbf{M}^k$ has all positive entries for some positive integer $k$.
This is equivalent to it being possible to get from the page of any one
program to any other, with positive probability.&lt;/p&gt;

&lt;p&gt;In reality this condition may well fail, but there&amp;rsquo;s any easy fix. We
just have our random surfer occasionally start over, picking a new
program at random to start their surfing from. Then there&amp;rsquo;s always a
positive chance of ending up anywhere, however briefly.&lt;/p&gt;

&lt;h1 id=&#34;application&#34;&gt;Application&lt;/h1&gt;

&lt;p&gt;Let&amp;rsquo;s put this theory into practice. We need a list of PhD programs, and
the number of hires from each of them, by each of them. Happily the APDA
provides &lt;a href=&#34;https://bit.ly/2kZ2ulS&#34; target=&#34;_blank&#34;&gt;a table&lt;/a&gt; of just such data up through
2016.&lt;/p&gt;

&lt;p&gt;This table counts hires broadly, notice. It includes postdoc positions,
and any &amp;ldquo;permanent academic&amp;rdquo; post apparently (see page 65
&lt;a href=&#34;https://bit.ly/2kZ2ulS&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;It also includes &amp;ldquo;selfies&amp;rdquo;: when a program hires one of its own PhDs.
Some selfies are certainly legitimate for our purposes, but others
probably aren&amp;rsquo;t the sort of thing we&amp;rsquo;re after here. For example, KU
Leuven stands out as having far more selfies than any other program save
Oxford. Judging from Leuven&amp;rsquo;s website, most of these posts are not
permanent positions, nor the sort of highly desirable fellowships Oxford
often hires its own graduates into.&lt;/p&gt;

&lt;p&gt;In this analys I&amp;rsquo;m going to exclude selfies. This is a bit arbitrary,
but not entirely. It has a big, negative impact on KU Leuven, not nearly
so much Oxford. Disclosure: it also slightly favours the University of
Toronto, where I work.&lt;/p&gt;

&lt;p&gt;That in mind let&amp;rsquo;s see our top 10 PageRank programs, alongside Wallace&amp;rsquo;s
results for comparison:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Wallace&lt;/th&gt;
&lt;th&gt;Weisberg&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;New York University&lt;/td&gt;
&lt;td&gt;New York University&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Columbia University&lt;/td&gt;
&lt;td&gt;Columbia University&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Princeton University&lt;/td&gt;
&lt;td&gt;Princeton University&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Yale University&lt;/td&gt;
&lt;td&gt;Yale University&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Katholieke Universiteit Leuven&lt;/td&gt;
&lt;td&gt;University of California, Berkeley&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;University of California, Berkeley&lt;/td&gt;
&lt;td&gt;Rutgers University&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;University of Oxford&lt;/td&gt;
&lt;td&gt;University of Pittsburgh (HPS)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Rutgers University&lt;/td&gt;
&lt;td&gt;University of Toronto&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;University of Pittsburgh (HPS)&lt;/td&gt;
&lt;td&gt;University of Oxford&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;University of Toronto&lt;/td&gt;
&lt;td&gt;Harvard University&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The match is quite close. And the two big differences (Oxford and
Leuven) are explained by the exclusion of selfies. So we seem to have
implemented the algorithm correctly.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://jonathanweisberg.org/post/page-rank-2/&#34;&gt;Next time&lt;/a&gt; we&amp;rsquo;ll look at predicting PageRanks, based on past PGR ratings,
and past placement data. Meanwhile, here&amp;rsquo;s the full listing:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Ordinal&lt;/th&gt;
&lt;th&gt;Program&lt;/th&gt;
&lt;th&gt;PageRank&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;New York University&lt;/td&gt;
&lt;td&gt;0.0556761&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;Columbia University&lt;/td&gt;
&lt;td&gt;0.0513814&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;Princeton University&lt;/td&gt;
&lt;td&gt;0.0469022&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;Yale University&lt;/td&gt;
&lt;td&gt;0.0364091&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;University of California, Berkeley&lt;/td&gt;
&lt;td&gt;0.0347961&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;Rutgers University&lt;/td&gt;
&lt;td&gt;0.0320305&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;University of Pittsburgh (HPS)&lt;/td&gt;
&lt;td&gt;0.0306536&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;University of Toronto&lt;/td&gt;
&lt;td&gt;0.0255954&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;University of Oxford&lt;/td&gt;
&lt;td&gt;0.0237988&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;Harvard University&lt;/td&gt;
&lt;td&gt;0.0234968&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;University of North Carolina at Chapel Hill&lt;/td&gt;
&lt;td&gt;0.0226243&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;University of Pittsburgh&lt;/td&gt;
&lt;td&gt;0.0222314&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;Massachusetts Institute of Technology&lt;/td&gt;
&lt;td&gt;0.0189393&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;td&gt;University of California, Los Angeles&lt;/td&gt;
&lt;td&gt;0.0185778&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;University of Chicago&lt;/td&gt;
&lt;td&gt;0.0174908&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;Stanford University&lt;/td&gt;
&lt;td&gt;0.0167941&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;17&lt;/td&gt;
&lt;td&gt;University of Cambridge (HPS)&lt;/td&gt;
&lt;td&gt;0.0132451&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;td&gt;University of California, San Diego&lt;/td&gt;
&lt;td&gt;0.0130881&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;19&lt;/td&gt;
&lt;td&gt;University of Arizona&lt;/td&gt;
&lt;td&gt;0.0125996&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;Brown University&lt;/td&gt;
&lt;td&gt;0.0119340&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;21&lt;/td&gt;
&lt;td&gt;Northwestern University&lt;/td&gt;
&lt;td&gt;0.0115745&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;22&lt;/td&gt;
&lt;td&gt;University of Michigan&lt;/td&gt;
&lt;td&gt;0.0111104&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;23&lt;/td&gt;
&lt;td&gt;London School of Economics and Political Science&lt;/td&gt;
&lt;td&gt;0.0107691&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;24&lt;/td&gt;
&lt;td&gt;Graduate Center of the City University of New York&lt;/td&gt;
&lt;td&gt;0.0100186&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;25&lt;/td&gt;
&lt;td&gt;University of Cambridge&lt;/td&gt;
&lt;td&gt;0.0100180&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;26&lt;/td&gt;
&lt;td&gt;Indiana University Bloomington&lt;/td&gt;
&lt;td&gt;0.0097714&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;27&lt;/td&gt;
&lt;td&gt;University of Notre Dame&lt;/td&gt;
&lt;td&gt;0.0094539&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;28&lt;/td&gt;
&lt;td&gt;King&amp;rsquo;s College London&lt;/td&gt;
&lt;td&gt;0.0091462&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;29&lt;/td&gt;
&lt;td&gt;University of Pennsylvania&lt;/td&gt;
&lt;td&gt;0.0091080&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;td&gt;Ohio State University&lt;/td&gt;
&lt;td&gt;0.0090754&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;31&lt;/td&gt;
&lt;td&gt;University of Maryland, College Park&lt;/td&gt;
&lt;td&gt;0.0089689&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;td&gt;Katholieke Universiteit Leuven&lt;/td&gt;
&lt;td&gt;0.0089271&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;33&lt;/td&gt;
&lt;td&gt;University of Chicago (CHSS)&lt;/td&gt;
&lt;td&gt;0.0086507&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;34&lt;/td&gt;
&lt;td&gt;University of Southern California&lt;/td&gt;
&lt;td&gt;0.0083793&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;35&lt;/td&gt;
&lt;td&gt;Cornell University&lt;/td&gt;
&lt;td&gt;0.0082857&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;36&lt;/td&gt;
&lt;td&gt;Institut Jean Nicod&lt;/td&gt;
&lt;td&gt;0.0082398&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;37&lt;/td&gt;
&lt;td&gt;Western University&lt;/td&gt;
&lt;td&gt;0.0080649&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;38&lt;/td&gt;
&lt;td&gt;Indiana University Bloomington (HPS)&lt;/td&gt;
&lt;td&gt;0.0079434&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;39&lt;/td&gt;
&lt;td&gt;Australian National University&lt;/td&gt;
&lt;td&gt;0.0079356&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;40&lt;/td&gt;
&lt;td&gt;Duke University&lt;/td&gt;
&lt;td&gt;0.0077089&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;41&lt;/td&gt;
&lt;td&gt;The University of Melbourne&lt;/td&gt;
&lt;td&gt;0.0077023&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;42&lt;/td&gt;
&lt;td&gt;University College London&lt;/td&gt;
&lt;td&gt;0.0068843&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;43&lt;/td&gt;
&lt;td&gt;St Andrews and Stirling Graduate Programme in Philosophy&lt;/td&gt;
&lt;td&gt;0.0067107&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;44&lt;/td&gt;
&lt;td&gt;University of California, Irvine (LPS)&lt;/td&gt;
&lt;td&gt;0.0062144&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;45&lt;/td&gt;
&lt;td&gt;University of Alberta&lt;/td&gt;
&lt;td&gt;0.0062053&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;46&lt;/td&gt;
&lt;td&gt;University of Texas at Austin&lt;/td&gt;
&lt;td&gt;0.0061678&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;47&lt;/td&gt;
&lt;td&gt;University of Sheffield&lt;/td&gt;
&lt;td&gt;0.0061209&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;48&lt;/td&gt;
&lt;td&gt;Georgetown University&lt;/td&gt;
&lt;td&gt;0.0057809&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;49&lt;/td&gt;
&lt;td&gt;Carnegie Mellon University&lt;/td&gt;
&lt;td&gt;0.0056723&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;50&lt;/td&gt;
&lt;td&gt;Pennsylvania State University&lt;/td&gt;
&lt;td&gt;0.0056654&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;51&lt;/td&gt;
&lt;td&gt;Arizona State University&lt;/td&gt;
&lt;td&gt;0.0056481&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;52&lt;/td&gt;
&lt;td&gt;University of British Columbia&lt;/td&gt;
&lt;td&gt;0.0055508&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;53&lt;/td&gt;
&lt;td&gt;Boston College&lt;/td&gt;
&lt;td&gt;0.0054974&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;54&lt;/td&gt;
&lt;td&gt;University of Edinburgh&lt;/td&gt;
&lt;td&gt;0.0052731&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;55&lt;/td&gt;
&lt;td&gt;University of Wisconsin-Madison&lt;/td&gt;
&lt;td&gt;0.0050191&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;56&lt;/td&gt;
&lt;td&gt;McGill University&lt;/td&gt;
&lt;td&gt;0.0049022&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;57&lt;/td&gt;
&lt;td&gt;Tulane University&lt;/td&gt;
&lt;td&gt;0.0048671&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;58&lt;/td&gt;
&lt;td&gt;University of Memphis&lt;/td&gt;
&lt;td&gt;0.0047615&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;59&lt;/td&gt;
&lt;td&gt;McMaster University&lt;/td&gt;
&lt;td&gt;0.0045715&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;60&lt;/td&gt;
&lt;td&gt;University of Minnesota Twin Cities&lt;/td&gt;
&lt;td&gt;0.0045700&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;61&lt;/td&gt;
&lt;td&gt;University of Oregon&lt;/td&gt;
&lt;td&gt;0.0045430&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;62&lt;/td&gt;
&lt;td&gt;Boston University&lt;/td&gt;
&lt;td&gt;0.0041329&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;63&lt;/td&gt;
&lt;td&gt;University of Connecticut&lt;/td&gt;
&lt;td&gt;0.0039652&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;University of California, Riverside&lt;/td&gt;
&lt;td&gt;0.0039192&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;65&lt;/td&gt;
&lt;td&gt;The University of Sydney&lt;/td&gt;
&lt;td&gt;0.0038372&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;66&lt;/td&gt;
&lt;td&gt;Johns Hopkins University&lt;/td&gt;
&lt;td&gt;0.0037778&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;67&lt;/td&gt;
&lt;td&gt;University of Colorado Boulder&lt;/td&gt;
&lt;td&gt;0.0036436&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;68&lt;/td&gt;
&lt;td&gt;The New School&lt;/td&gt;
&lt;td&gt;0.0035828&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;69&lt;/td&gt;
&lt;td&gt;University of California, Santa Barbara&lt;/td&gt;
&lt;td&gt;0.0035150&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;70&lt;/td&gt;
&lt;td&gt;University of Virginia&lt;/td&gt;
&lt;td&gt;0.0034553&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;71&lt;/td&gt;
&lt;td&gt;University of South Florida&lt;/td&gt;
&lt;td&gt;0.0034273&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;72&lt;/td&gt;
&lt;td&gt;State University of New York at Buffalo&lt;/td&gt;
&lt;td&gt;0.0033737&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;73&lt;/td&gt;
&lt;td&gt;York University&lt;/td&gt;
&lt;td&gt;0.0030985&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;74&lt;/td&gt;
&lt;td&gt;University of Massachusetts Amherst&lt;/td&gt;
&lt;td&gt;0.0030937&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;75&lt;/td&gt;
&lt;td&gt;University of California, Irvine&lt;/td&gt;
&lt;td&gt;0.0030890&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;76&lt;/td&gt;
&lt;td&gt;Vanderbilt University&lt;/td&gt;
&lt;td&gt;0.0030261&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;77&lt;/td&gt;
&lt;td&gt;Michigan State University&lt;/td&gt;
&lt;td&gt;0.0030149&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;78&lt;/td&gt;
&lt;td&gt;Saint Louis University&lt;/td&gt;
&lt;td&gt;0.0029294&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;79&lt;/td&gt;
&lt;td&gt;Emory University&lt;/td&gt;
&lt;td&gt;0.0028953&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;80&lt;/td&gt;
&lt;td&gt;University of Illinois at Chicago&lt;/td&gt;
&lt;td&gt;0.0028281&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;81&lt;/td&gt;
&lt;td&gt;Purdue University&lt;/td&gt;
&lt;td&gt;0.0027891&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;82&lt;/td&gt;
&lt;td&gt;University of California, Davis&lt;/td&gt;
&lt;td&gt;0.0027106&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;83&lt;/td&gt;
&lt;td&gt;State University of New York at Stony Brook&lt;/td&gt;
&lt;td&gt;0.0025640&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;84&lt;/td&gt;
&lt;td&gt;DePaul University&lt;/td&gt;
&lt;td&gt;0.0025005&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;85&lt;/td&gt;
&lt;td&gt;University of Iowa&lt;/td&gt;
&lt;td&gt;0.0023518&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;86&lt;/td&gt;
&lt;td&gt;Baylor University&lt;/td&gt;
&lt;td&gt;0.0021004&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;87&lt;/td&gt;
&lt;td&gt;Macquarie University&lt;/td&gt;
&lt;td&gt;0.0020773&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;88&lt;/td&gt;
&lt;td&gt;University of Illinois at Urbana-Champaign&lt;/td&gt;
&lt;td&gt;0.0020013&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;89&lt;/td&gt;
&lt;td&gt;The University of Manchester&lt;/td&gt;
&lt;td&gt;0.0019471&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;90&lt;/td&gt;
&lt;td&gt;Syracuse University&lt;/td&gt;
&lt;td&gt;0.0018914&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;91&lt;/td&gt;
&lt;td&gt;William Marsh Rice University&lt;/td&gt;
&lt;td&gt;0.0018034&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;92&lt;/td&gt;
&lt;td&gt;University of Calgary&lt;/td&gt;
&lt;td&gt;0.0017465&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;93&lt;/td&gt;
&lt;td&gt;University of York&lt;/td&gt;
&lt;td&gt;0.0017356&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;94&lt;/td&gt;
&lt;td&gt;University of Hawai&amp;rsquo;i at Manoa&lt;/td&gt;
&lt;td&gt;0.0017302&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;94&lt;/td&gt;
&lt;td&gt;University of Oklahoma&lt;/td&gt;
&lt;td&gt;0.0017302&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;96&lt;/td&gt;
&lt;td&gt;Duquesne University&lt;/td&gt;
&lt;td&gt;0.0016160&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;97&lt;/td&gt;
&lt;td&gt;Kingston University&lt;/td&gt;
&lt;td&gt;0.0015818&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;98&lt;/td&gt;
&lt;td&gt;Tilburg University&lt;/td&gt;
&lt;td&gt;0.0015262&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;99&lt;/td&gt;
&lt;td&gt;The Catholic University of America&lt;/td&gt;
&lt;td&gt;0.0015213&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;Fordham University&lt;/td&gt;
&lt;td&gt;0.0015210&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;101&lt;/td&gt;
&lt;td&gt;University of Arkansas&lt;/td&gt;
&lt;td&gt;0.0015187&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;102&lt;/td&gt;
&lt;td&gt;University of Rochester&lt;/td&gt;
&lt;td&gt;0.0014903&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;103&lt;/td&gt;
&lt;td&gt;University of Nebraska, Lincoln&lt;/td&gt;
&lt;td&gt;0.0014715&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;104&lt;/td&gt;
&lt;td&gt;University of Washington&lt;/td&gt;
&lt;td&gt;0.0014668&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;105&lt;/td&gt;
&lt;td&gt;Washington University in St. Louis&lt;/td&gt;
&lt;td&gt;0.0013589&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;106&lt;/td&gt;
&lt;td&gt;Villanova University&lt;/td&gt;
&lt;td&gt;0.0012493&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;107&lt;/td&gt;
&lt;td&gt;Loyola University Chicago&lt;/td&gt;
&lt;td&gt;0.0011959&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;107&lt;/td&gt;
&lt;td&gt;University of South Carolina&lt;/td&gt;
&lt;td&gt;0.0011959&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;107&lt;/td&gt;
&lt;td&gt;Victoria University of Wellington&lt;/td&gt;
&lt;td&gt;0.0011959&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;110&lt;/td&gt;
&lt;td&gt;University of Florida&lt;/td&gt;
&lt;td&gt;0.0011906&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;111&lt;/td&gt;
&lt;td&gt;Bowling Green State University&lt;/td&gt;
&lt;td&gt;0.0011731&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;112&lt;/td&gt;
&lt;td&gt;University of Guelph&lt;/td&gt;
&lt;td&gt;0.0011575&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;113&lt;/td&gt;
&lt;td&gt;University of Utah&lt;/td&gt;
&lt;td&gt;0.0011462&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;114&lt;/td&gt;
&lt;td&gt;Binghamton University&lt;/td&gt;
&lt;td&gt;0.0011355&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;115&lt;/td&gt;
&lt;td&gt;University of Tennessee&lt;/td&gt;
&lt;td&gt;0.0011087&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;116&lt;/td&gt;
&lt;td&gt;University of Waterloo&lt;/td&gt;
&lt;td&gt;0.0010800&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;117&lt;/td&gt;
&lt;td&gt;University at Albany&lt;/td&gt;
&lt;td&gt;0.0010486&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;118&lt;/td&gt;
&lt;td&gt;University of New Mexico&lt;/td&gt;
&lt;td&gt;0.0010049&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;119&lt;/td&gt;
&lt;td&gt;Birkbeck, University of London&lt;/td&gt;
&lt;td&gt;0.0009510&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;120&lt;/td&gt;
&lt;td&gt;University of Kentucky&lt;/td&gt;
&lt;td&gt;0.0008533&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;121&lt;/td&gt;
&lt;td&gt;Marquette University&lt;/td&gt;
&lt;td&gt;0.0008356&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;122&lt;/td&gt;
&lt;td&gt;University of Cincinnati&lt;/td&gt;
&lt;td&gt;0.0008316&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;123&lt;/td&gt;
&lt;td&gt;University of Georgia&lt;/td&gt;
&lt;td&gt;0.0007732&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;Brandeis University&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;California Institute of Technology&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;Central European University&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;Claremont Graduate University&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;Dalhousie University&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;Deakin University&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;Durham University&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;Eindhoven University of Technology&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;Florida State University&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;Free University of Berlin&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;Goethe University Frankfurt&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;Humboldt University of Berlin&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;Iowa State University&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;Monash University&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;Montclair State University&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;Pantheon-Sorbonne University&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;Southern Illinois University&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;Stockholm University&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;Temple University&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;Texas State University&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;The University of Adelaide&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;The University of Western Australia&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;Trinity College, Dublin&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;University de Montreal&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;University of Aberdeen&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;University of Amsterdam&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;University of Auckland&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;University of Bristol&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;University of California, Santa Cruz&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;University of Cologne&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;University of Dallas&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;University of East Anglia&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;University of Geneva&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;University of Glasgow&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;University of Groningen&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;University of Helsinki&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;University of Kansas&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;University of Kent&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;University of Leeds&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;University of Miami&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;University of Milan&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;University of Missouri&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;University of New South Wales&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;University of Oslo&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;University of Otago&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;University of Ottawa&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;University of Reading&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;University of Sussex&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;University of Tasmania&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;University of Tubingen&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;University of Vienna&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;University of Waikato&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;University of Warwick&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;Uppsala University&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;Wayne State University&lt;/td&gt;
&lt;td&gt;0.0007232&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Model Referees</title>
      <link>http://jonathanweisberg.org/post/Model%20Referees/</link>
      <pubDate>Mon, 15 Oct 2018 00:00:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/Model%20Referees/</guid>
      <description>&lt;p&gt;In &lt;a href=&#34;http://jonathanweisberg.org/post/How Hard Is It to Find Referees/&#34;&gt;the previous post&lt;/a&gt; we saw there&amp;rsquo;s about a $35$% chance a given referee will agree to review a paper for &lt;em&gt;Ergo&lt;/em&gt;. And on average it takes about $5.8$ tries to find two referees for a submission. The full empirical distribution looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/model_referees_files/unnamed-chunk-2-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;But there&amp;rsquo;s also an a priori way of exploring an editor&amp;rsquo;s predicament here, by using a classic model: the &lt;a href=&#34;https://en.wikipedia.org/wiki/Negative_binomial_distribution&#34; target=&#34;_blank&#34;&gt;negative binomial distribution&lt;/a&gt;. So I thougth I&amp;rsquo;d make a little exercise of seeing how well the model captures the empirical reality here.&lt;/p&gt;

&lt;p&gt;Contacting potential referees is a bit like flipping a loaded coin: you keep flipping until you get two heads, then stop. Our question is how many flips it&amp;rsquo;ll take to get to that point.&lt;/p&gt;

&lt;p&gt;Let $p$ be the probability of heads on each toss, and let $T$ be the number of tails you get before landing the second head. The negative binomial model says the probability of getting $t$ tails, $P(T = t)$, is:
$$ P(T = t) = \binom{t + 1}{t} p^t (1 - p)^2. $$
And the mean of this distribution is $2(1-p)/p$.&lt;/p&gt;

&lt;p&gt;If the coin is fair, $p = .5$, and we should expect to get $T = 2$ tails:
$$ 2(1-p)/p = 2(1-.5)/.5 = 2. $$
An editor&amp;rsquo;s &amp;ldquo;coin&amp;rdquo; is biased against them though, at least at &lt;em&gt;Ergo&lt;/em&gt;: $p = .35$. So we would expect $T = 3.7$ referees on average to decline before we get two takers:
$$ 2(1-p)/p = 2(1-.35)/.35 = 3.7. $$
In other words, we expect it to take on average $5.7$ tries to secure two referees for a submission, which very closely matches the empirical average of $5.8$!&lt;/p&gt;

&lt;p&gt;How about the full distribution, how well does it match the empirical reality?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/model_referees_files/unnamed-chunk-3-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The model peaks a bit early, but otherwise it&amp;rsquo;s pretty accurate.&lt;/p&gt;

&lt;p&gt;Of course, mileage may vary depending on the journal. For example, &lt;em&gt;Ergo&lt;/em&gt; has a pretty high desk-rejection rate&amp;mdash;about $67$%. And referees may be more willing to agree when they know a submission has already passed that hurdle.&lt;/p&gt;

&lt;p&gt;So let&amp;rsquo;s conclude by looking at the model&amp;rsquo;s predictions when referees are more/less likely to agree. Here are the predictions for some plausible values of $p$. The mean $\mu$ is the corresponding number of invites required to secure two reviews, on average.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/model_referees_files/unnamed-chunk-4-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;All these models assume that referees&amp;rsquo; responses are independent, like flips of a coin, which isn&amp;rsquo;t too realistic. But given how close the model is for &lt;em&gt;Ergo&lt;/em&gt;, it might still be good enough for other journals too.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How Hard Is It to Find Referees?</title>
      <link>http://jonathanweisberg.org/post/How%20Hard%20Is%20It%20to%20Find%20Referees/</link>
      <pubDate>Sat, 06 Oct 2018 00:00:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/How%20Hard%20Is%20It%20to%20Find%20Referees/</guid>
      <description>&lt;p&gt;Finding willing referees is one of the more tedious parts of an editor’s
job. And with all the talk about how overloaded the peer-review system
is, it’s worth pausing to examine just how hard it is to find referees.&lt;/p&gt;

&lt;p&gt;Well, at &lt;em&gt;Ergo&lt;/em&gt; it takes on average 5.8 tries before we find two
referees to review a submission. The following plot gives the full
picture.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/finding_referees_files/unnamed-chunk-2-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So most submissions take six or fewer invites, and the overwhelming
majority require fewer than 10. But 10–15 is not unheard of. And on very
rare occasions it’s taken more than 20.&lt;/p&gt;

&lt;p&gt;A different perspective is the time it takes to find two willing
referees. The average time between when the first invite is sent out and
two referees have agreed is 10.8 days. Here’s the histogram:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/finding_referees_files/unnamed-chunk-4-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So it usually takes less than two weeks to find two takers, though on
occasion it can take more than a month. And on very rare occasions it’s
taken more than two months.&lt;/p&gt;

&lt;p&gt;That’s all looking just at submissions that had two referees, mind you.
Most submissions to &lt;em&gt;Ergo&lt;/em&gt; aren’t sent out to external reviewers at all,
being desk-rejected instead. But more directly relevant is that about
20% of externally reviewed submissions are rejected just on one
referee’s recommendation, because the referee submits a decisively
negative report before a second can be commissioned. And on some
occasions an editor will even commission three reports.&lt;/p&gt;

&lt;p&gt;So maybe the best way to look at the whole thing is just to calculate
The Big Number: 35% of all invitations sent to referees end with a
completed report. Which, I gotta say, is actually a lot better than I
would have guessed.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Waiting for the Editor: A New App</title>
      <link>http://jonathanweisberg.org/post/Shiny%20Wait%20Times/</link>
      <pubDate>Wed, 27 Jun 2018 00:00:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/Shiny%20Wait%20Times/</guid>
      <description>&lt;p&gt;If waiting to hear back from journals makes you as twitchy as it makes me, you might appreciate &lt;a href=&#34;https://jweisber.shinyapps.io/WaitingForTheEditor/&#34; target=&#34;_blank&#34;&gt;Waiting for the Editor&lt;/a&gt;. It&amp;rsquo;s a little app that displays wait time forecasts and data from &lt;a href=&#34;https://blog.apaonline.org/2017/04/13/journal-surveys-assessing-the-peer-review-process/&#34; target=&#34;_blank&#34;&gt;the APA Journal Survey&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It has two kinds of display, based on &lt;a href=&#34;http://jonathanweisberg.org/post/Journal%20Surveys&#34;&gt;my earlier post&lt;/a&gt; about the APA survey. You can view scatterplots:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://jweisber.shinyapps.io/WaitingForTheEditor/&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/shiny_wait_times/scatter.png&#34; alt=&#34;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Or ridgeplots:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://jweisber.shinyapps.io/WaitingForTheEditor/&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/shiny_wait_times/ridge.png&#34; alt=&#34;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note that the ridgeplots can be misleading.&lt;/strong&gt; They treat old data and new the same. So if a journal&amp;rsquo;s wait times have improved/worsened since the survey started in 2009, this won&amp;rsquo;t be reflected in the ridgeplot.&lt;/p&gt;

&lt;p&gt;You can customize these displays by&amp;hellip;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;selecting which journals to show.&lt;/li&gt;
&lt;li&gt;including all submissions or just e.g. rejected ones (since &lt;a href=&#34;http://jonathanweisberg.org/post/Journal%20Surveys/#acceptance-rates&#34; target=&#34;_blank&#34;&gt;accepted submissions are overrepresented&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;choosing what dates you want to draw the data from, since some journals have gotten better/worse since 2009.&lt;/li&gt;
&lt;li&gt;choosing your own cap for the maximum review time.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Visualizing the Philosophy Journal Surveys</title>
      <link>http://jonathanweisberg.org/post/Journal%20Surveys/</link>
      <pubDate>Tue, 22 May 2018 00:00:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/Journal%20Surveys/</guid>
      <description>

&lt;p&gt;In 2009 Andrew Cullison set up an &lt;a href=&#34;https://blog.apaonline.org/journal-surveys/&#34; target=&#34;_blank&#34;&gt;ongoing survey&lt;/a&gt; for philosophers to report their experiences submitting papers to various journals. For me, a junior philosopher working toward tenure at the time, it was a great resource. It was the best guide I knew to my chances of getting a paper accepted at &lt;em&gt;Journal X&lt;/em&gt;, or at least getting rejected quickly by &lt;em&gt;Journal Y&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;But I always wondered about self-selection bias. I figured disgruntled authors were more likely to use the survey to vent. So I wondered whether the data overestimated things like wait times and rejection rates.&lt;/p&gt;

&lt;p&gt;This post is an attempt to better understand the survey data, especially through visualization and comparisons with other sources.&lt;/p&gt;

&lt;h1 id=&#34;timeline&#34;&gt;Timeline&lt;/h1&gt;

&lt;p&gt;The survey has accrued 7,425 responses as of this writing. Of these, 720 have no date recorded. Here&amp;rsquo;s the timeline for the rest:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/journal_surveys/unnamed-chunk-1-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;Two things jump out right away: the spike at the beginning and the dead zone near the end. What gives?&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m guessing the spike reflects records imported manually from another source at the survey&amp;rsquo;s inception. Here I&amp;rsquo;ll mostly assume these records are legitimate, and include them in our analyses. But since the dates attached to those responses are certainly wrong, I&amp;rsquo;ll exclude them when we get to temporal questions (toward the end of the post).&lt;/p&gt;

&lt;p&gt;What about the 2016&amp;ndash;17 dead zone? I tried contacting people involved with the surveys, but nobody seemed to really know for sure what happened there. This dead period is right around when the surveys were &lt;a href=&#34;https://blog.apaonline.org/2017/04/13/journal-surveys-assessing-the-peer-review-process/&#34; target=&#34;_blank&#34;&gt;handed over to the APA&lt;/a&gt;. In that process the data were moved to a different hosting service, apparently with some changes to the survey format. So maybe the records for this period were lost in translation.&lt;/p&gt;

&lt;p&gt;In any case, it looks like the norm is for the survey to get around 50 to 100 responses each month.&lt;/p&gt;

&lt;h1 id=&#34;journals&#34;&gt;Journals&lt;/h1&gt;

&lt;p&gt;There are 155 journals covered by the survey, but most have only a handful of responses. Here are the journals with 50 or more:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/journal_surveys/unnamed-chunk-2-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;How do these numbers compare to the ground truth? Do &lt;em&gt;Phil Studies&lt;/em&gt; and &lt;em&gt;Phil Quarterly&lt;/em&gt; really get the most submissions, for example? And do they really get 4&amp;ndash;5 times as many as, say, &lt;em&gt;BJPS&lt;/em&gt;?&lt;/p&gt;

&lt;p&gt;One way to check is to compare these numbers with those reported by the journals themselves to the APA and BPA in &lt;a href=&#34;http://www.apaonline.org/page/journalsurveys&#34; target=&#34;_blank&#34;&gt;this study&lt;/a&gt; from 2011&amp;ndash;13. &lt;em&gt;Phil Studies&lt;/em&gt; isn&amp;rsquo;t included in that report unfortunately, but &lt;em&gt;Phil Quarterly&lt;/em&gt; and &lt;em&gt;BJPS&lt;/em&gt; are. They reported receiving 2,305 and 1,267 submissions, respectively, during 2011&amp;ndash;13. So &lt;em&gt;Phil Quarterly&lt;/em&gt; does seem to get a lot more submissions, though not 4 times as many.&lt;/p&gt;

&lt;p&gt;For a fuller picture let&amp;rsquo;s do the same comparison for all journals that reported their submission totals to the APA/BPA. That gives us a subset of 33 journals. If we look at the number of survey responses for these journals over the years 2011&amp;ndash;2013, we can get a sense of how large each journal looms in the Journal Survey vs. the APA/BPA report:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/journal_surveys/unnamed-chunk-4-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s a pretty a strong correlation evident here. But it&amp;rsquo;s also clear there&amp;rsquo;s some bias in the survey responses. Bias towards what? I&amp;rsquo;m not exactly sure. Roughly the pattern seems to be that the more submissions a journal receives, the more likely it is to be overrepresented in the survey. But it might instead be a bias towards generalist journals, or journals with fast turn around times. This question would need a more careful analysis, I think.&lt;/p&gt;

&lt;h1 id=&#34;acceptance-rates&#34;&gt;Acceptance Rates&lt;/h1&gt;

&lt;p&gt;What about acceptance rates? Here are the acceptance rates for those journals with 30+ responses in the survey:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/journal_surveys/unnamed-chunk-5-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;These numbers look suspiciously high to me. Most philosophy journals I know have an acceptance rate under 10%. So let&amp;rsquo;s compare with an outside source again.&lt;/p&gt;

&lt;p&gt;The most comprehensive list of acceptance rates I know is &lt;a href=&#34;http://certaindoubts.com/philosophy-journal-information-esf-rankings-citation-impact-rejection-rates/&#34; target=&#34;_blank&#34;&gt;this one&lt;/a&gt; based on data from the ESF. It&amp;rsquo;s not as current as I&amp;rsquo;d like (2011), nor as complete (&lt;em&gt;Phil Imprint&lt;/em&gt; isn&amp;rsquo;t included, perhaps too new at the time). It&amp;rsquo;s also not entirely accurate: it reports an acceptance rate of 8% for &lt;em&gt;Phil Quarterly&lt;/em&gt; vs. 3% reported in the APA/BPA study.&lt;/p&gt;

&lt;p&gt;Still, the ESF values do seem to be largely accurate for many prominent journals I&amp;rsquo;ve checked. For example, they&amp;rsquo;re within 1 or 2% of the numbers reported elsewhere by &lt;em&gt;Ethics&lt;/em&gt;, &lt;em&gt;Mind&lt;/em&gt;, &lt;em&gt;Phil Review&lt;/em&gt;, &lt;em&gt;JPhil&lt;/em&gt;, &lt;em&gt;Nous&lt;/em&gt;, and &lt;em&gt;PPR&lt;/em&gt;.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:Sources-the-APA&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:Sources-the-APA&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; So they&amp;rsquo;re useful for at least a rough validation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/journal_surveys/unnamed-chunk-6-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;Apparently the Journal Surveys do overrepresent accepted submissions. Consistently so in fact: with the exception of &lt;em&gt;Phil Review&lt;/em&gt;, &lt;em&gt;Analysis&lt;/em&gt;, &lt;em&gt;Ancient Philosophy&lt;/em&gt;, and &lt;em&gt;Phil Sci&lt;/em&gt;, the surveys overrepresent accepted submissions for every other journal in this comparison. And in many cases accepted submissions are drastically overrepresented.&lt;/p&gt;

&lt;p&gt;This surprised me, since I figured the surveys would serve as an outlet for disgruntled authors. But maybe it&amp;rsquo;s the other way around: people are more likely to use the surveys as a way to share happy news. (Draw your own conclusions about human nature.)&lt;/p&gt;

&lt;h1 id=&#34;seniority&#34;&gt;Seniority&lt;/h1&gt;

&lt;p&gt;So who uses the journal surveys: grad students? Faculty? The survey records five categories: Graduate Student, Non-TT Faculty, TT-but-not-T Faculty, Tenured Faculty, and Other. A few entries have no professional position recorded.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/journal_surveys/unnamed-chunk-7-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;Evidently, participation drops off with seniority. Also interesting if not too terribly surprising is that seniority affects acceptance:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/journal_surveys/unnamed-chunk-8-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;Compared to grad students, tenured faculty were about 10% more likely to report their papers as having been accepted.&lt;/p&gt;

&lt;h1 id=&#34;gender&#34;&gt;Gender&lt;/h1&gt;

&lt;p&gt;About 79% of respondents specified their gender. Of those, 16.4% were women and 83.6% were men. How does this compare to journal-submitting philosophers in general?&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://jonathanweisberg.org/post/Referee%20Gender/&#34; target=&#34;_blank&#34;&gt;Various other sources&lt;/a&gt; put the percentage of women in academic philosophy roughly in the 15&amp;ndash;25% range. But we&amp;rsquo;re looking for something more specific: what portion of journal submissions come from women vs. men?&lt;/p&gt;

&lt;p&gt;The APA/BPA report gives the percentage of submissions from women at 14 journals. And we can use those figures to infer that 17.6% of submissions to these journals were from women, which matches the 16.4% in the Journal Surveys fairly well.&lt;/p&gt;

&lt;p&gt;Looking at individual journals gives a more mixed picture, however:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/journal_surveys/unnamed-chunk-10-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;While the numbers are reasonably close for some of these journals, they&amp;rsquo;re significantly different for many of them. So, using the Journal Surveys to estimate the gender makeup of a journal&amp;rsquo;s submission pool probably isn&amp;rsquo;t a good idea.&lt;/p&gt;

&lt;p&gt;Does gender affect acceptance? Looking at the data from all journals together, it seems not:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/journal_surveys/unnamed-chunk-11-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;In fact it&amp;rsquo;s striking how stark the non-effect is here, given the quirks we&amp;rsquo;ve already noted in this data set.&lt;/p&gt;

&lt;p&gt;We could break things down further, going journal by journal. But then we&amp;rsquo;d face &lt;a href=&#34;https://imgs.xkcd.com/comics/significant.png&#34; target=&#34;_blank&#34;&gt;the problem of multiple comparisons&lt;/a&gt;, and we&amp;rsquo;ve already seen that the journal-by-journal numbers on gender aren&amp;rsquo;t terribly reliable. So I won&amp;rsquo;t dig into that exercise here.&lt;/p&gt;

&lt;h1 id=&#34;wait-times&#34;&gt;Wait Times&lt;/h1&gt;

&lt;p&gt;For me, the surveys were always most interesting as a means to compare wait times across journals. But how reliable are these comparisons?&lt;/p&gt;

&lt;p&gt;The APA/BPA report gives the average wait times at 38 journals. It also reports how many decisions were delivered within 2 months, in 2&amp;ndash;6 months, in 7&amp;ndash;11 months, and after 12+ months.&lt;/p&gt;

&lt;p&gt;Trouble is, a lot of these numbers look dodgy. The average wait times are all whole numbers of months&amp;mdash;except inexplicaby for one journal, &lt;em&gt;Ratio&lt;/em&gt;. I guess someone at the APA/BPA has a sense of humour.&lt;/p&gt;

&lt;p&gt;The other wait time figures are also suspiciously round. For example, &lt;em&gt;APQ&lt;/em&gt; is listed as returning 60% of its decisions within 2 months, 35% after 2&amp;ndash;6 months, and the remaining 5% after 7&amp;ndash;11 months. Round percentages like these are the norm. So, at best, most of these numbers are rounded estimates. At worst, they don&amp;rsquo;t always reflect an actual count, but rather the editor&amp;rsquo;s perception of their own performance.&lt;/p&gt;

&lt;p&gt;On top of all that, there are differences between &lt;a href=&#34;https://apaonline.site-ym.com/resource/resmgr/journal_surveys_2014/apa_bpa_survey_data_2014.xlsx&#34; target=&#34;_blank&#34;&gt;the downloadable Excel spreadsheet&lt;/a&gt; and &lt;a href=&#34;https://apaonline.site-ym.com/general/custom.asp?page=journalsurveys&#34; target=&#34;_blank&#34;&gt;the APA&amp;rsquo;s webpages&lt;/a&gt; reporting (supposedly) the same data. For example, the spreadsheet gives an average wait time of 6 months for &lt;em&gt;Phil Imprint&lt;/em&gt; (certainly wrong), while the webpage says &amp;ldquo;not available&amp;rdquo;. In fact the Excel spreadsheet flatly contradicts itself here: it says &lt;em&gt;Phil Imprint&lt;/em&gt; returns 73% of its decisions within 2 months, the rest in 2&amp;ndash;6 months.&lt;/p&gt;

&lt;p&gt;I don&amp;rsquo;t know any other comprehensive list of wait times, though, so we&amp;rsquo;ll have to make do. Here I&amp;rsquo;ll restrict the comparison to journals with 30+ responses in the 2011&amp;ndash;2013 timeframe, and exclude &lt;em&gt;Phil Imprint&lt;/em&gt; because of the inconsistencies just mentioned.&lt;/p&gt;

&lt;p&gt;That leaves us with 11 journals on which to compare average wait times:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/journal_surveys/unnamed-chunk-15-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;The results are pretty stark. The match is close for most of these journals. In fact, if we&amp;rsquo;re forgiving about the rounding, only three journals have a discrepancy that&amp;rsquo;s clearly more than 1 month: &lt;em&gt;Erkenntnis&lt;/em&gt;,  &lt;em&gt;Mind&lt;/em&gt;, and &lt;em&gt;Synthese&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Notably, these are the three journals with the longest wait times according to survey respondents. I&amp;rsquo;d add that the reported 2 month average for &lt;em&gt;Mind&lt;/em&gt; is wildly implausible by reputation. I can&amp;rsquo;t comment on the discrepancies for &lt;em&gt;Erkenntnis&lt;/em&gt; and &lt;em&gt;Synthese&lt;/em&gt;, though, since I know much less about their reputations for turnaround.&lt;/p&gt;

&lt;p&gt;I do want to flag that &lt;em&gt;Mind&lt;/em&gt; has radically improved its review times recently, as we&amp;rsquo;ll soon see. But for the present purpose&amp;mdash;validating the Journal Survey data&amp;mdash;we&amp;rsquo;re confined to look at 2011&amp;ndash;13. And the survey responses align much better with &lt;em&gt;Mind&lt;/em&gt;&amp;rsquo;s reputation during that time period than the 2 month average listed in the APA/BPA report.&lt;/p&gt;

&lt;p&gt;In any case, since the wait time data looks to be carrying a fair amount of signal, let&amp;rsquo;s conclude our analysis with some visualizations of it.&lt;/p&gt;

&lt;h1 id=&#34;visualizing-wait-times&#34;&gt;Visualizing Wait Times&lt;/h1&gt;

&lt;p&gt;A journal&amp;rsquo;s average wait time doesn&amp;rsquo;t tell the whole story, of course. Two journals might have the same average wait time even though one of them is much more consistent and predictable. Or, a journal with a high desk-rejection rate might have a low average wait time, but still take a long time with its few externally reviewed submissions. So it&amp;rsquo;s helpful to see the whole picture.&lt;/p&gt;

&lt;p&gt;One way to see the whole picture is with a scatterplot. This also let&amp;rsquo;s us see how a journal&amp;rsquo;s wait times have changed. To make this feasible, I&amp;rsquo;ll focus on two groups of journals I expect to be of broad interest.&lt;/p&gt;

&lt;p&gt;The first is a list of 18 &amp;ldquo;general&amp;rdquo; journals that were highly rated in &lt;a href=&#34;http://leiterreports.typepad.com/blog/2015/09/the-top-20-general-philosophy-journals-2015.html&#34; target=&#34;_blank&#34;&gt;a pair of polls&lt;/a&gt; at Leiter Reports.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:The-poll-results&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:The-poll-results&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; For the sake of visibility, I&amp;rsquo;ll cap these scatterplots at 24 months. The handful of entries with longer wait times are squashed down to 24 so they can still inform the plot.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/journal_surveys/unnamed-chunk-16-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;In addition to the improvements at &lt;em&gt;Mind&lt;/em&gt; mentioned earlier, &lt;em&gt;Phil Review&lt;/em&gt;, &lt;em&gt;PPQ&lt;/em&gt;, &lt;em&gt;CJP&lt;/em&gt;, and &lt;em&gt;Erkenntnis&lt;/em&gt; all seem to be shortening their wait times. &lt;em&gt;APQ&lt;/em&gt; and &lt;em&gt;EJP&lt;/em&gt; on the other hand appear to be drifting upward.&lt;/p&gt;

&lt;p&gt;Keeping that in mind, let&amp;rsquo;s visualize expected wait times at these journals with a ridgeplot. The plot shows a smoothed estimate of the probable wait times for each journal. Note that here I&amp;rsquo;ve truncated the timeline at 12 months, squashing all wait times longer than 12 months down to 12.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/journal_surveys/unnamed-chunk-17-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;Remember though, the ridgeplot reflects old data as much as new. Authors submitting to journals like &lt;em&gt;Mind&lt;/em&gt; and &lt;em&gt;CJP&lt;/em&gt;, where wait times have significantly improved recently, should definitely not just set their expectations according to this plot. Consult the scatterplot!&lt;/p&gt;

&lt;p&gt;Our second group consists of 8 &amp;ldquo;specialty&amp;rdquo; journals drawn from &lt;a href=&#34;http://leiterreports.typepad.com/blog/2013/07/top-philosophy-journals-without-regard-to-area.html&#34; target=&#34;_blank&#34;&gt;another poll&lt;/a&gt; at Leiter Reports. Here I&amp;rsquo;ll cap the scale at 15 months for the sake of visibility:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/journal_surveys/unnamed-chunk-18-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;And for the ridgeplot we&amp;rsquo;ll return to a cap of 12 months:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/journal_surveys/unnamed-chunk-19-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;Again, remember that the ridgeplot reflects out-of-date information for some journals. Consult the scatterplot! And please direct others to do the same if you share any of this on social media.&lt;/p&gt;

&lt;h1 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/journal_surveys/inigo.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A journal&amp;rsquo;s prominence in the survey is a decent &lt;em&gt;comparative&lt;/em&gt; guide to the quantity of submissions it receives.&lt;/li&gt;
&lt;li&gt;Accepted submissions are overrepresented in the survey. Acceptance rates estimated from the survey will pretty consistently overestimate the true rate&amp;mdash;in many cases by a lot.&lt;/li&gt;
&lt;li&gt;Grad students and non-tenured faculty use the surveys a lot more than tenured faculty.&lt;/li&gt;
&lt;li&gt;Acceptance rates increase with seniority.&lt;/li&gt;
&lt;li&gt;Men and women seem to be represented about the same as in the population of journal-submitting philosophers more generally.&lt;/li&gt;
&lt;li&gt;Gender doesn&amp;rsquo;t seem to affect acceptance rate.&lt;/li&gt;
&lt;li&gt;The Survey seems to be a reasonably good guide to expected wait times, though there may be some anomalies (e.g. &lt;em&gt;Synthese&lt;/em&gt; and &lt;em&gt;Erkenntnis&lt;/em&gt;).&lt;/li&gt;
&lt;li&gt;Some journals&amp;rsquo; wait times have been improving significantly, such as &lt;em&gt;CJP&lt;/em&gt;, &lt;em&gt;Erkenntnis&lt;/em&gt;, &lt;em&gt;Mind&lt;/em&gt;, &lt;em&gt;PPQ&lt;/em&gt;, and &lt;em&gt;Phil Review&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:Sources-the-APA&#34;&gt;Sources: the APA/BPA study, &lt;a href=&#34;http://dailynous.com/2015/01/20/closer-look-philosophy-journal-practices/&#34; target=&#34;_blank&#34;&gt;Daily Nous&lt;/a&gt;, and the websites for &lt;a href=&#34;https://philreview.gorgesapps.us/statistics&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;Phil Review&lt;/em&gt;&lt;/a&gt; and &lt;a href=&#34;https://www.journals.uchicago.edu/pb-assets/docs/journals/ethics-editorial-final-2018-02-07.pdf&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;Ethics&lt;/em&gt;&lt;/a&gt;. One notable exception is &lt;em&gt;CJP&lt;/em&gt;, which reported 17% to the APA/BPA but 6% on Daily Nous. The ESF gives 10%. &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:Sources-the-APA&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:The-poll-results&#34;&gt;The poll results identified 20 journals ranked &amp;ldquo;best&amp;rdquo; by respondents. So why does our list only have 18? Because 3 of those 20 aren&amp;rsquo;t covered in the survey data, and I&amp;rsquo;ve included the &amp;ldquo;runner up&amp;rdquo; journal ranked 21st. &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:The-poll-results&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Where Are They Now? The Healy 2100</title>
      <link>http://jonathanweisberg.org/post/Where%20Are%20They%20Now%20The%20Healy%202100/</link>
      <pubDate>Mon, 23 Apr 2018 00:00:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/Where%20Are%20They%20Now%20The%20Healy%202100/</guid>
      <description>

&lt;p&gt;A &lt;a href=&#34;https://www.timeshighereducation.com/news/how-much-research-goes-completely-uncited&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;Times Higher Education&lt;/em&gt;
piece&lt;/a&gt;
making the rounds last week found that most published philosophy papers
are never cited. More exactly, of the studied philosophy papers
published in 2012, more than half had no citations indexed in &lt;a href=&#34;https://clarivate.com/products/web-of-science/&#34; target=&#34;_blank&#34;&gt;Web of
Science&lt;/a&gt; five years
later.&lt;/p&gt;

&lt;p&gt;At Daily Nous, the &lt;a href=&#34;http://dailynous.com/2018/04/19/philosophy-high-rate-uncited-publications/&#34; target=&#34;_blank&#34;&gt;discussion of that
finding&lt;/a&gt;
turned up some interesting follow-up questions and findings. In
particular, Brian Weatherson found &lt;a href=&#34;http://dailynous.com/2018/04/19/philosophy-high-rate-uncited-publications/#comment-141535&#34; target=&#34;_blank&#34;&gt;quite different
figures&lt;/a&gt;
for papers published in &lt;em&gt;prestigious&lt;/em&gt; philosophy journals. In the journals
he looked at, 89% of the papers published in 2012 had at least one
citation in Web of Science five years later.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; And more than half had five
or more citations.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s a pretty striking difference: &amp;gt;50% vs. ~11%! Seems like where
you publish your paper makes a &lt;em&gt;big&lt;/em&gt; difference to your chances of going
uncited.&lt;/p&gt;

&lt;p&gt;Shocking, I know.&lt;/p&gt;

&lt;p&gt;But this got me thinking about &lt;a href=&#34;https://kieranhealy.org/blog/archives/2015/02/25/gender-and-citation-in-four-general-interest-philosophy-journals-1993-2013/&#34; target=&#34;_blank&#34;&gt;Kieran Healy&amp;rsquo;s
analysis&lt;/a&gt;
from a few years back. He found an &amp;ldquo;uncitation&amp;rdquo; rate higher than
Weatherson&amp;rsquo;s 11%&amp;mdash;almost 20%&amp;mdash;even though he was looking at just four of
philosophy&amp;rsquo;s most prominent journals: &lt;em&gt;Journal of Philosophy&lt;/em&gt;, &lt;em&gt;Mind&lt;/em&gt;,
&lt;em&gt;Noûs&lt;/em&gt;, and &lt;em&gt;Philosophical Review&lt;/em&gt;. (He found that around half of the
papers in these journals had five citations or fewer.)&lt;/p&gt;

&lt;p&gt;So I wondered: what&amp;rsquo;s with the discrepancy? Do these journals not
necessarily get the most citations? Or is it that Healy was looking at
papers from 1993 to 2013, and things changed somehow over those two
decades, so that papers published in 2012 tend to get discussed more
than papers from 1993. Or is it just a symptom of when Healy collected
his data? Papers published in, say, 2011 wouldn&amp;rsquo;t have had much time to
gather citations by 2013 when Healy (apparently?) gathered his data.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s take a look.&lt;/p&gt;

&lt;h1 id=&#34;the-healy-2100&#34;&gt;The Healy 2100&lt;/h1&gt;

&lt;p&gt;Since I don&amp;rsquo;t have Healy&amp;rsquo;s raw data, I went to Web of Science and
grabbed their data for all papers published over 1993&amp;ndash;2013 in the
&amp;ldquo;Healy 4&amp;rdquo; journals. I ended up with a couple hundred more papers than
Healy looked at&amp;mdash;not sure why. But the list of 2,100 papers he studied
is &lt;a href=&#34;https://github.com/kjhealy/philpub&#34; target=&#34;_blank&#34;&gt;available on GitHub&lt;/a&gt;. So I
focused on just those to get more of an apples-to-apples comparison.&lt;/p&gt;

&lt;p&gt;Then I tried to reproduce his original findings, especially &lt;a href=&#34;https://kieranhealy.org/blog/archives/2015/02/25/gender-and-citation-in-four-general-interest-philosophy-journals-1993-2013/#the-matthew-effect-is-a-harsh-mistress&#34; target=&#34;_blank&#34;&gt;this
histogram&lt;/a&gt;. Here&amp;rsquo;s my version:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/where_are_they_now_the_healy_2100/unnamed-chunk-2-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I got pretty close, but I didn&amp;rsquo;t manage to reproduce his results
exactly. I found about 19.9% of the papers had no citations by the end
of 2013, compared to Healy&amp;rsquo;s ~18.5%. And I found about 58.6% with five
or fewer citations, compared with Healy&amp;rsquo;s &amp;ldquo;just over half&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Still, the match is pretty close, so let&amp;rsquo;s go on to see how these papers
have aged since 2013.&lt;/p&gt;

&lt;h1 id=&#34;where-are-they-now&#34;&gt;Where Are They Now?&lt;/h1&gt;

&lt;p&gt;If we include citations up to the present day, only about 9.7% of these
2,100 papers have no citations, and about 39.1% have five or fewer.
Here&amp;rsquo;s the updated histogram:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/where_are_they_now_the_healy_2100/unnamed-chunk-4-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So it looks like the discrepancy with Weatherson&amp;rsquo;s result is (partly?)
down to the obvious thing. There hadn&amp;rsquo;t been enough time for the later
papers in Healy&amp;rsquo;s data set to accrue citations.&lt;/p&gt;

&lt;h1 id=&#34;cutting-out-supplements&#34;&gt;Cutting Out Supplements&lt;/h1&gt;

&lt;p&gt;A lot of these 2,100 papers are actually from the two supplements to
&lt;em&gt;Noûs&lt;/em&gt;: &lt;em&gt;Philosophical Issues&lt;/em&gt; and &lt;em&gt;Philosophical Perspectives&lt;/em&gt;. And it
turns out they&amp;rsquo;re making a big difference.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s what things look like when we cut supplements out:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/where_are_they_now_the_healy_2100/unnamed-chunk-6-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now only 3.8% of our 1,677 papers have no citations to date, and just
30.9% have five or fewer.&lt;/p&gt;

&lt;h1 id=&#34;sliding-windows&#34;&gt;Sliding Windows&lt;/h1&gt;

&lt;p&gt;We&amp;rsquo;ve been looking at all citations accumulated to date for these
papers, which for older papers means 25 years&amp;rsquo; worth of opportunity for
discussion. For more direct comparison to the &lt;em&gt;THE&lt;/em&gt; analysis mentioned
at the outset, we can look at just the five-year window following each
paper&amp;rsquo;s publication.&lt;/p&gt;

&lt;p&gt;So, how many citations did these papers accrue just within five years of
being published? Looking at only the &amp;ldquo;core&amp;rdquo; papers again (no
supplements), 14.4% had no citations within five years of publication,
and 69.6% had five or fewer.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s still a bit higher than the 11% (respectively 50%) found by
Weatherson. So we have to ask: are things changing? Are recent papers in
these journals accruing citations faster?&lt;/p&gt;

&lt;p&gt;It seems so. Here are the &amp;ldquo;uncitation&amp;rdquo; rates for the years 1993&amp;ndash;2013:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/where_are_they_now_the_healy_2100/unnamed-chunk-8-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;And here are the &amp;ldquo;five or fewer citations&amp;rdquo; rates:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/where_are_they_now_the_healy_2100/unnamed-chunk-9-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: Eric Schliesser pointed out to me what should have been obvious here, namely the internet. When Google took over the search engine industry around the year 2000, there was a citation boom across the board. So you&amp;rsquo;d naturally expect 2012 to be a very different year than 1993 as far as uncitation goes.&lt;/p&gt;

&lt;p&gt;At first I thought the data plainly vindicated the &amp;ldquo;Google effect&amp;rdquo; hypothesis, and I had some plots up here to show as much. But it turned out Web of Science had snuck a bunch of supplemental &lt;em&gt;Phil Issues&lt;/em&gt;/&lt;em&gt;Phil Perspectives&lt;/em&gt; papers past me unmarked!&lt;/p&gt;

&lt;p&gt;With those removed, the Google effect isn&amp;rsquo;t looking so big. Here&amp;rsquo;s the long view, from 1970 to 2013:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/where_are_they_now_the_healy_2100/unnamed-chunk-10-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Apparently papers with zero citations after five years have been declining for a while now. There does still seem to be a significant Google effect in the &amp;ldquo;five or fewer&amp;rdquo; measure:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/where_are_they_now_the_healy_2100/unnamed-chunk-11-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;But it still looks like there were changes before Google. Does this reflect a growing profession? A faster-moving profession? An increasing number of publications? Longer bibliographies? Just idiosyncracies of Web of Science&amp;rsquo;s indexing? Something else? I&amp;rsquo;m not sure.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Weatherson looked at 391 articles published in 2012 in &lt;em&gt;Philosophical Review&lt;/em&gt;, &lt;em&gt;Mind&lt;/em&gt;, &lt;em&gt;Journal of Philosophy&lt;/em&gt;, &lt;em&gt;Nous&lt;/em&gt;, &lt;em&gt;Philosophical Studies&lt;/em&gt;, &lt;em&gt;Ethics&lt;/em&gt;, &lt;em&gt;Philosophical Quarterly&lt;/em&gt;, &lt;em&gt;Philosophy of Science&lt;/em&gt;, and &lt;em&gt;Australasian Journal of Philosophy&lt;/em&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Call for Papers: Formal Epistemology Workshop (FEW) 2018</title>
      <link>http://jonathanweisberg.org/post/CFP%20FEW%202018/</link>
      <pubDate>Thu, 21 Dec 2017 10:36:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/CFP%20FEW%202018/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Location:&lt;/strong&gt; University of Toronto&lt;br /&gt;
&lt;strong&gt;Dates:&lt;/strong&gt; June 12–14, 2018&lt;br /&gt;
&lt;strong&gt;Keynote Speakers:&lt;/strong&gt; &lt;a href=&#34;http://www.larabuchak.net/&#34; target=&#34;_blank&#34;&gt;Lara Buchak&lt;/a&gt; and &lt;a href=&#34;https://sites.google.com/site/michaeltitelbaum/&#34; target=&#34;_blank&#34;&gt;Mike Titelbaum&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;Submission Deadline:&lt;/strong&gt; February 12, 2018&lt;br /&gt;
&lt;strong&gt;Authors Notified:&lt;/strong&gt; March 31, 2018&lt;/p&gt;

&lt;p&gt;We are pleased to invite papers in formal epistemology, broadly construed to include related areas of philosophy as well as cognate disciplines like statistics, psychology, economics, computer science, and mathematics.&lt;/p&gt;

&lt;p&gt;Submissions should be:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;prepared for anonymous review,&lt;/li&gt;
&lt;li&gt;no more than 6,000 words,&lt;/li&gt;
&lt;li&gt;accompanied by an abstract of up to 300 words, and&lt;/li&gt;
&lt;li&gt;in PDF format.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Submission is via the &lt;a href=&#34;https://easychair.org/conferences/?conf=few2018&#34; target=&#34;_blank&#34;&gt;EasyChair website&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The final selection of the program will be made with an eye to diversity. We especially encourage submissions from PhD candidates, early career researchers, and members of groups underrepresented in academic philosophy.&lt;/p&gt;

&lt;p&gt;Some funds are available to reimburse speakers&amp;rsquo; travel expenses. The available amounts are still being determined, but we hope to cover most/all expenses for student and early career speakers. Childcare can also be arranged.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;http://jonathanweisberg.org/few2018&#34; target=&#34;_blank&#34;&gt;conference website is here&lt;/a&gt;. The contact address is &lt;a href=&#34;mailto:few2018toronto@gmail.com&#34; target=&#34;_blank&#34;&gt;few2018toronto@gmail.com&lt;/a&gt;. The local organizers are &lt;a href=&#34;http://www.davidjamesbar.net/&#34; target=&#34;_blank&#34;&gt;David James Barnett&lt;/a&gt;, &lt;a href=&#34;http://individual.utoronto.ca/jnagel/Home_Page.html&#34; target=&#34;_blank&#34;&gt;Jennifer Nagel&lt;/a&gt;, and &lt;a href=&#34;http://jonathanweisberg.org/&#34; target=&#34;_blank&#34;&gt;Jonathan Weisberg&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>