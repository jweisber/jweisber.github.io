<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Epistemology on Jonathan Weisberg</title>
    <link>http://jonathanweisberg.org/tags/epistemology/index.xml</link>
    <description>Recent content in Epistemology on Jonathan Weisberg</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="http://jonathanweisberg.org/tags/epistemology/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Coherentism Without Coherence</title>
      <link>http://jonathanweisberg.org/post/coherentism-1/</link>
      <pubDate>Wed, 14 Aug 2019 00:00:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/coherentism-1/</guid>
      <description>

&lt;p&gt;If you look at the little network diagram below, you&amp;rsquo;ll probably
agree that $P$ is the most &amp;ldquo;central&amp;rdquo; node in some intuitive sense.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/coherentism-1/fig3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This post is about using a belief&amp;rsquo;s centrality in the web of belief to
give a coherentist account of its justification. The more central a
belief is, the more justified it is.&lt;/p&gt;

&lt;p&gt;But how do we quantify &amp;ldquo;centrality&amp;rdquo;? The rough idea: the more ways there
are to arrive at a proposition by following inferential pathways in the
web of belief, the more central it is.&lt;/p&gt;

&lt;p&gt;Since we&amp;rsquo;re coherentists today (for the next 10 minutes, anyway), cyclic
pathways are allowed here. If we travel
$P \rightarrow Q \rightarrow R \rightarrow P$, that counts as an
inferential path leading to $P$. And if we go around that cycle twice,
that counts as another such pathway.&lt;/p&gt;

&lt;p&gt;You might think this just wrecks the whole idea. Every node has
infinitely many such pathways leading to it, after all. By cycling
around and around we can come up with literally any number of pathways
ending at a given node.&lt;/p&gt;

&lt;p&gt;But, by examining how these pathways differ in the limit, we can
differentiate between more and less central nodes/beliefs. We can thus
clarify a sense in which $P$ is most central, and quantify that
centrality. We can even use that quantity to answer a classic objection
to coherentism leveled by &lt;a href=&#34;https://philpapers.org/rec/KLEWPC&#34; target=&#34;_blank&#34;&gt;Klein &amp;amp; Warfield
(1994)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As a bonus, we can do all this without ever giving an account of what
makes a corpus of beliefs &amp;ldquo;coherent.&amp;rdquo; This flips the script on a lot of
contemporary formal work on coherentism.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; Because coherentism is
holistic, you might think it has to evaluate the coherence of a whole
corpus first, before it can assess the individual members.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; But we&amp;rsquo;ll
see this isn&amp;rsquo;t so.
$$
\newcommand\T{\intercal}
\newcommand{\A}{\mathbf{A}}
\renewcommand{\v}{\mathbf{v}}
$$&lt;/p&gt;

&lt;h1 id=&#34;counting-pathways&#34;&gt;Counting Pathways&lt;/h1&gt;

&lt;p&gt;Our idea is to count how many paths there are leading to $P$ vs.Â other
nodes. We start with paths of length $1$, then count paths of length
$2$, then length $3$, and so on. As we count longer and longer paths,
each node&amp;rsquo;s count approaches infinity.&lt;/p&gt;

&lt;p&gt;But not their relative ratios! If, at each step, we divide the number of
paths ending at $P$ by the number of all paths, this ratio converges.&lt;/p&gt;

&lt;p&gt;To find its limit, we represent our graph numerically. A graph can be
represented in a table, where each node corresponds to a row and column.
The columns represent &amp;ldquo;sources&amp;rdquo; and the rows represent &amp;ldquo;targets.&amp;rdquo; We put
a $1$ where the column node points to the row node, otherwise we put a
$0$.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;$P$&lt;/th&gt;
&lt;th&gt;$Q$&lt;/th&gt;
&lt;th&gt;$R$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;$P$&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;$Q$&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;$R$&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Hiding the row and column names gives us a matrix we&amp;rsquo;ll call $\A$: $$
\A =
\left[
  \begin{matrix}
    0 &amp;amp; 1 &amp;amp; 1 \\&lt;br /&gt;
    1 &amp;amp; 0 &amp;amp; 0 \\&lt;br /&gt;
    0 &amp;amp; 1 &amp;amp; 0
  \end{matrix}
\right].
$$ Notice how each row records the length-$1$ paths leading to the
corresponding node. There are two such paths to $P$, and one each to $Q$
and $R$.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;The key to counting longer paths is to take powers of $\A$. If we
multiply $\A$ by itself to get $\A^2$, we get a record of the length-$2$
paths: $$
\A^2 = \A \times \A = \left[
  \begin{matrix}
    0 &amp;amp; 1 &amp;amp; 1 \\&lt;br /&gt;
    1 &amp;amp; 0 &amp;amp; 0 \\&lt;br /&gt;
    0 &amp;amp; 1 &amp;amp; 0
  \end{matrix}
\right] \left[
  \begin{matrix}
    0 &amp;amp; 1 &amp;amp; 1 \\&lt;br /&gt;
    1 &amp;amp; 0 &amp;amp; 0 \\&lt;br /&gt;
    0 &amp;amp; 1 &amp;amp; 0
  \end{matrix}
\right] =
\left[
  \begin{matrix}
    1 &amp;amp; 1 &amp;amp; 0 \\&lt;br /&gt;
    0 &amp;amp; 1 &amp;amp; 1 \\&lt;br /&gt;
    1 &amp;amp; 0 &amp;amp; 0
  \end{matrix}
\right].
$$ There are two such paths to $P$: $$
\begin{aligned}
  Q \rightarrow R \rightarrow P,\\&lt;br /&gt;
  P \rightarrow Q \rightarrow P.
\end{aligned}
$$ Similarly for $Q$: $$
\begin{aligned}
  Q \rightarrow P \rightarrow Q,\\&lt;br /&gt;
  R \rightarrow P \rightarrow Q.
\end{aligned}
$$ While $R$ has just one length-$2$ path:
$$ P \rightarrow Q \rightarrow R. $$ If we go on to examine $\A^3$, its
rows will tally the length-$3$ paths; in general, $\A^n$ tallies the
paths of length-$n$.&lt;/p&gt;

&lt;p&gt;But we want relative ratios, not raw counts. The trick to getting these
is to divide $\A$ at each step by a special number $\lambda$, known as
the &amp;ldquo;leading eigenvalue&amp;rdquo; of $\A$ (details &lt;a href=&#34;#tech&#34;&gt;below&lt;/a&gt;). If we take
the limit
$$ \lim_{n \rightarrow \infty} \left(\frac{\A}{\lambda}\right)^n $$ we
get a matrix whose columns all have a special property: $$
\left[
  \begin{matrix}
    0.41 &amp;amp; 0.55 &amp;amp; 0.31 \\&lt;br /&gt;
    0.31 &amp;amp; 0.41 &amp;amp; 0.23 \\&lt;br /&gt;
    0.23 &amp;amp; 0.31 &amp;amp; 0.18
  \end{matrix}
\right].
$$ They all have the same relative proportions. They&amp;rsquo;re multiples of the
same &amp;ldquo;frequency vector,&amp;rdquo; a vector of positive values that sum to $1$: $$
\left[
  \begin{matrix}
    0.43 \\&lt;br /&gt;
    0.32 \\&lt;br /&gt;
    0.25 \\&lt;br /&gt;
  \end{matrix}
\right].
$$ So as we tally longer and longer paths, we find that $43\%$ of those
paths lead to $P$, compared with $32\%$ for $Q$ and $25\%$ for $R$. Thus
$P$ is about $1.3$ times as justified as $Q$ ($.43/.32$), and about
$1.7$ times as justified as $R$ ($.43/.25$).&lt;/p&gt;

&lt;p&gt;We want absolute degrees of justification though, not just comparative
ones. So we borrow a trick from probability theory and use a tautology
for scale.&lt;/p&gt;

&lt;p&gt;We add a special node $\top$ to our graph, which every other node points
to, though $\top$ doesn&amp;rsquo;t point back.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/coherentism-1/fig4.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Updating our matrix $\A$ accordingly, we insert $\top$ in the first
row/column: $$
\A =
\left[
  \begin{matrix}
    0 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1 \\&lt;br /&gt;
    0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 \\&lt;br /&gt;
    0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \\&lt;br /&gt;
    0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0
  \end{matrix}
\right].
$$ Redoing our limit anlaysis gives us the vector
$(1.00, 0.57, 0.43, 0.33)$. But this isn&amp;rsquo;t our final answer, because
it&amp;rsquo;s actually not possible for the non-$\top$ nodes to get a value
higher than $2/3$ in a graph with just $3$ non-$\top$ nodes.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; So we
divide elementwise by $(1, 2/3, 2/3, 2/3)$ to scale things, giving us
our final result: $$
\left[
  \begin{matrix}
    1.00 \\&lt;br /&gt;
    0.85 \\&lt;br /&gt;
    0.65 \\&lt;br /&gt;
    0.49
  \end{matrix}
\right].
$$ The relative justifications are the same as before, e.g. $P$ is still
$1.3$ times as justified as $Q$. But now we can make absolute
assessments too. $R$ comes out looking pretty bad ($0.49$), as seems
right, while $Q$ looks a bit better ($0.65$). Of course $P$ looks best
($0.85$), though maybe not quite good enough to be justified &lt;em&gt;tout
court&lt;/em&gt;.&lt;/p&gt;

&lt;h1 id=&#34;the-klein-warfield-problem&#34;&gt;The Klein&amp;ndash;Warfield Problem&lt;/h1&gt;

&lt;p&gt;Ok that&amp;rsquo;s theoretically nifty and all, but does it work on actual cases?
Let&amp;rsquo;s try it out by looking at a notorious objection to coherentism.
&lt;a href=&#34;https://philpapers.org/rec/KLEWPC&#34; target=&#34;_blank&#34;&gt;Klein &amp;amp; Warfield (1994)&lt;/a&gt; argue that
coherentism flouts the laws of probability. How so?&lt;/p&gt;

&lt;p&gt;Making sense of things often means believing more: taking on new beliefs
to resolve the tensions in our existing ones. For example, if we think
Tweety is a bird who can&amp;rsquo;t fly, the tension is resolved if we also
believe they&amp;rsquo;re a penguin.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:5&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:5&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;But believing more means believing less probably. Increases in logical
strength bring decreases in probability (unless the stronger content was
already guaranteed with probability $1$). So increasing the coherence in
one&amp;rsquo;s web of belief will generally mean decreasing its probability. How
could increasing coherence increase justification, then?&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://philpapers.org/rec/MEROBO&#34; target=&#34;_blank&#34;&gt;Merricks (1995)&lt;/a&gt; points out that,
even though the probability of the whole corpus goes down, the
probabilities of individual beliefs go up in a way. After all, it&amp;rsquo;s more
likely Tweety can&amp;rsquo;t fly if they&amp;rsquo;re a penguin, than if they&amp;rsquo;re just a
bird of some unknown species.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s only the beginning of a satisfactory answer though. After all, we
might not be justified in believing Tweety&amp;rsquo;s a penguin in the first
place! Adding a new belief to support an existing belief doesn&amp;rsquo;t help if
the new belief has no support itself. We need a more global assessment,
which is where the present account shines.&lt;/p&gt;

&lt;p&gt;Suppose we add $P$ = &lt;em&gt;Tweety is a penguin&lt;/em&gt; to the network containing $B$
= &lt;em&gt;Tweety is a bird&lt;/em&gt; and $\neg F$ = &lt;em&gt;Tweety can&amp;rsquo;t fly&lt;/em&gt;. Will this
increase the centrality/justification of $B$ and of $\neg F$? Yes, but
we need to sort out the support relations to verify this.&lt;/p&gt;

&lt;p&gt;Presumably $P$ supports $B$, and $\neg F$ too. But what about the other
way around? If Tweety is a flightless bird, there&amp;rsquo;s a decent chance
they&amp;rsquo;re a penguin. But it&amp;rsquo;s hardly certain; they might be an emu or kiwi
instead. Come to think of it, isn&amp;rsquo;t support a matter of degree, so don&amp;rsquo;t
we need finer tools than just on/off arrows?&lt;/p&gt;

&lt;p&gt;Yes, and the refinement is easy. We accommodate degrees of support by
attaching weights to our arrows. Instead of just placing a $1$ in our
matrix $\A$ wherever the column-node points to the row-node, we put a
number from the $[0,1]$ interval that reflects the strength of support.
The same limit analysis as before still works, as it turns out.
We just think of our inferential links as &amp;ldquo;leaky pipes&amp;rdquo; now, where
weaker links make for leakier pipelines.&lt;/p&gt;

&lt;p&gt;We still need concrete numbers to analyze the Tweety example. But it&amp;rsquo;s a
toy example, so let&amp;rsquo;s just make up some plausible-ish numbers to get us
going. Let&amp;rsquo;s suppose $1\%$ of birds are flightless, and birds are an
even smaller percentage of the flightless things, say $0.1\%$. Let&amp;rsquo;s
also pretend that $20\%$ of flightless birds are penguins.&lt;/p&gt;

&lt;p&gt;Before believing Tweety is a penguin then, our web of belief looks like
this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/coherentism-1/fig5a.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Calculating the degrees of justification for $B$ and $\neg F$, both come
out very close to $0$ as you&amp;rsquo;d expect (with $B$ closer to $0$ than
$\neg F$). Now we add $P$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/coherentism-1/fig5b.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Recalculating degrees of justification, we find that they increase
drastically. $B$ and $F$ are now justified to degree $0.85$, while $P$
is justified to degree $0.26$. (All numbers approximate.)&lt;/p&gt;

&lt;p&gt;So our account vindicates Merricks. Not only does adding $P$ to the
corpus add &amp;ldquo;local&amp;rdquo; justification for $B$ and for $\neg F$. It also
improves their standing on a more global assessment.&lt;/p&gt;

&lt;p&gt;You might be worried though: did $P$ come out too weakly justified, at
just $0.26$? No: that&amp;rsquo;s either an artifact of oversimplification, or
else it&amp;rsquo;s actually the appropriate outcome. Notice that $B$ and $\neg F$
don&amp;rsquo;t really support Tweety being a penguin. They&amp;rsquo;re a flightless bird,
sure, but maybe they&amp;rsquo;re an emu, kiwi, or moa. We chose to believe
penguin, and maybe we have our reasons. If we do, then the graph is
missing background beliefs which would improve $P$&amp;rsquo;s standing once
added. But otherwise, we just fell prey to stereotyping or
comes-to-mind-bias, in which case it&amp;rsquo;s right that $P$ stand poorly.&lt;/p&gt;

&lt;h1 id=&#34;tech&#34;&gt;Technical Background&lt;/h1&gt;

&lt;p&gt;The notion of centrality used here is a common tool in network analysis,
where it&amp;rsquo;s known as &lt;a href=&#34;https://en.wikipedia.org/wiki/Eigenvector_centrality&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;eigenvector
centrality.&amp;rdquo;&lt;/a&gt;
Because the frequency vector we arrive at in the limit is an
&lt;a href=&#34;https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors&#34; target=&#34;_blank&#34;&gt;eigenvector&lt;/a&gt;
of the matrix $\A$. In fact it&amp;rsquo;s a special eigenvector, the only one
with all-positive values.&lt;/p&gt;

&lt;p&gt;Since we&amp;rsquo;re measuring justification on a $0$-to-$1$ scale, our account
depends on there always being such an eigenvector for $\A$. In fact we
need it to be unique, up to scaling (i.e.Â up to multiplication by a
constant).&lt;/p&gt;

&lt;p&gt;The theorem that guarantees this is actually quite old, going back to
work by Oskar Perron and Georg Frobenius published around 1910. Here&amp;rsquo;s one version of it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Perron&amp;ndash;Frobenius Theorem.&lt;/strong&gt; Let $\A$ be a square matrix whose entries
are all positive. Then all of the following hold.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;$\A$ has an eigenvalue $\lambda$ that is larger (in absolute value)
than $\A$&amp;rsquo;s other eigenvalues. We call $\lambda$ the &lt;em&gt;leading
eigenvalue&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;$\A$&amp;rsquo;s leading eigenvalue has an eigenvector $\v$ whose entries are
all positive. We call $\v$ the &lt;em&gt;leading eigenvector&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;$\A$ has no other positive eigenvectors, save multiples of $\v$.&lt;/li&gt;
&lt;li&gt;The powers $(\A/\lambda)^n$ as $n \rightarrow \infty$ approach a
matrix whose columns are all multiples of $\v$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now, our matrices had some zeros, so they weren&amp;rsquo;t positive in all their
entries. But it doesn&amp;rsquo;t really matter, as it turns out.&lt;/p&gt;

&lt;p&gt;Frobenius&amp;rsquo; contribution was to generalize this result to many cases that
feature zeros. But even in cases where Frobenius&amp;rsquo; weaker conditions
aren&amp;rsquo;t satisfied, we can just borrow a trick from Google.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:6&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:6&#34;&gt;6&lt;/a&gt;&lt;/sup&gt; Instead of
using a $0$-to-$1$ scale, we use $\epsilon$-to-$1$ for some very small
positive number $\epsilon$. Then all entries in $\A$ are guaranteed to
be positive, and we just rescale our results accordingly. (Choose
$\epsilon$ small enough and the difference is negligible in practice.)&lt;/p&gt;

&lt;h1 id=&#34;acknowledgments&#34;&gt;Acknowledgments&lt;/h1&gt;

&lt;p&gt;This post owes a lot to prior work by Elena Derksen and Selim Berker.
I&amp;rsquo;d never really thought much about how coherence and justification
relate prior to reading &lt;a href=&#34;http://www.philpeople.com/elenarabinoffderksen/research.html&#34; target=&#34;_blank&#34;&gt;Derksen&amp;rsquo;s
work&lt;/a&gt;. And
&lt;a href=&#34;https://philpapers.org/rec/BERCVG&#34; target=&#34;_blank&#34;&gt;Berker&amp;rsquo;s&lt;/a&gt; prompted me to take graphs
more seriously as a way of formalizing coherentism. I&amp;rsquo;m also grateful to
David Wallace for &lt;a href=&#34;http://jonathanweisberg.org/post/page-rank-1/&#34;&gt;introducing me to the Perron&amp;ndash;Frobenius theorem&amp;rsquo;s use
as a tool in network
analysis&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;&lt;p&gt;See &lt;a href=&#34;https://philpapers.org/rec/SHOICT-2&#34; target=&#34;_blank&#34;&gt;Shogenji (1999)&lt;/a&gt; and
&lt;a href=&#34;https://philpapers.org/rec/FITAPT&#34; target=&#34;_blank&#34;&gt;Fitelson (2003)&lt;/a&gt; for some early
accounts. See Section 6 of &lt;a href=&#34;https://plato.stanford.edu/entries/justep-coherence/#ProMeaCoh&#34; target=&#34;_blank&#34;&gt;Olsson&amp;rsquo;s SEP
entry&lt;/a&gt;
for a survey and more recent references.&lt;/p&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;

&lt;li id=&#34;fn:2&#34;&gt;&lt;p&gt;In his seminal book on coherentism, &lt;a href=&#34;https://philpapers.org/rec/BONTSO-4&#34; target=&#34;_blank&#34;&gt;Bonjour
(1985)&lt;/a&gt; writes: &amp;ldquo;the
justification of a particular empirical belief finally depends, not
on other particular beliefs as the linear conception of
justification would have it, but instead on the overall system and
its coherence.&amp;rdquo; This doesn&amp;rsquo;t commit us to
assessing overall coherence before individual
justification. But that&amp;rsquo;s a natural conclusion you might come away with.&lt;/p&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;

&lt;li id=&#34;fn:3&#34;&gt;&lt;p&gt;We could count every proposition as pointing to itself. This would
mean putting $1$&amp;rsquo;s down the diagonal, i.e.Â adding the identity
matrix $\mathbf{I}$ to $\A$. This can be useful as a way to ensure
the limits we&amp;rsquo;ll require exist. But we&amp;rsquo;ll solve that problem
differently in the &amp;ldquo;Technical Background&amp;rdquo; section. And otherwise it
doesn&amp;rsquo;t really affect our results. It increases the leading
eigenvalue by $1$, but doesn&amp;rsquo;t affect the leading eigenvector.&lt;/p&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;

&lt;li id=&#34;fn:4&#34;&gt;&lt;p&gt;In general, the maximum possible centrality is $(k-1)/k$ in a
graph with $k$ non-$\top$ nodes.&lt;/p&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;

&lt;li id=&#34;fn:5&#34;&gt;&lt;p&gt;Hat tip to Erik J. Olsson&amp;rsquo;s &lt;a href=&#34;https://plato.stanford.edu/entries/justep-coherence/&#34; target=&#34;_blank&#34;&gt;entry on
coherentism&lt;/a&gt;
in the SEP, which uses this example in place of Klein &amp;amp; Warfield&amp;rsquo;s
slightly more involved one.&lt;/p&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:5&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;

&lt;li id=&#34;fn:6&#34;&gt;&lt;p&gt;Google&amp;rsquo;s founders used a variant of eigenvector centrality called
&amp;ldquo;PageRank&amp;rdquo; in their original search engine.&lt;/p&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:6&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Open Handbook of Formal Epistemology</title>
      <link>http://jonathanweisberg.org/post/open-handbook/</link>
      <pubDate>Wed, 26 Jun 2019 00:00:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/open-handbook/</guid>
      <description>&lt;p&gt;Today &lt;em&gt;The Open Handbook of Formal Epistemology&lt;/em&gt; is &lt;a href=&#34;http://jonathanweisberg.org/pdf/open-handbook-of-formal-epistemology.pdf&#34;&gt;available for download&lt;/a&gt;. It&amp;rsquo;s an open access book, the first published by PhilPapers itself. (The editors are &lt;a href=&#34;https://richardpettigrew.com/&#34; target=&#34;_blank&#34;&gt;Richard Pettigrew&lt;/a&gt; and me.)&lt;/p&gt;

&lt;p&gt;The book features 11 outstanding entries by 11 wonderful philosophers.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&amp;ldquo;Precise Credences&amp;rdquo;, by &lt;a href=&#34;https://sites.google.com/site/michaeltitelbaum/&#34; target=&#34;_blank&#34;&gt;Michael G. Titelbaum&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Decision Theory&amp;rdquo;, by &lt;a href=&#34;https://johannathoma.com/&#34; target=&#34;_blank&#34;&gt;Johanna Thoma&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Imprecise Probabilities&amp;rdquo;, by &lt;a href=&#34;http://www.lse.ac.uk/cpnss/people/anna-mahtani?from_serp=1&#34; target=&#34;_blank&#34;&gt;Anna Mahtani&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Primitive Conditional Probabilities&amp;rdquo;, by &lt;a href=&#34;http://www.kennyeaswaran.org/&#34; target=&#34;_blank&#34;&gt;Kenny Easwaran&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Infinitesimal Probabilities&amp;rdquo;, by &lt;a href=&#34;http://www.sylviawenmackers.be/&#34; target=&#34;_blank&#34;&gt;Sylvia Wenmackers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Comparative Probabilities&amp;rdquo;, by &lt;a href=&#34;https://jason-konek.squarespace.com/&#34; target=&#34;_blank&#34;&gt;Jason Konek&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Belief Revision Theory&amp;rdquo;, by &lt;a href=&#34;https://sites.google.com/site/hantilinphil/&#34; target=&#34;_blank&#34;&gt;Hanti Lin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Ranking Theory&amp;rdquo;, by &lt;a href=&#34;https://huber.blogs.chass.utoronto.ca/&#34; target=&#34;_blank&#34;&gt;Franz Huber&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Full &amp;amp; Partial Belief&amp;rdquo;, by &lt;a href=&#34;https://kgenin.github.io/&#34; target=&#34;_blank&#34;&gt;Konstantin Genin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Doxastic Logic&amp;rdquo;, by &lt;a href=&#34;https://sites.google.com/site/caiemike/&#34; target=&#34;_blank&#34;&gt;Michael Caie&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Conditionals&amp;rdquo;, by &lt;a href=&#34;https://philosophy.stanford.edu/people/ray-briggs&#34; target=&#34;_blank&#34;&gt;R. A. Briggs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We wanted to include lots more, but didn&amp;rsquo;t want to hold up publication any longer. Hopefully a second edition will cover more.&lt;/p&gt;

&lt;p&gt;For me personally, a central aim of this project was to demonstrate a point about open access publishing and shared standards. The budget for this book was exactly $0.00, and this was only possible because we didn&amp;rsquo;t need a human typesetter.&lt;/p&gt;

&lt;p&gt;Pretty much everyone in formal epistemology uses the same, standardized format to do their writing. And that format plugs in to a high-quality, freely available &lt;a href=&#34;https://en.wikipedia.org/wiki/TeX&#34; target=&#34;_blank&#34;&gt;typesetting program&lt;/a&gt;. So all you have to do to turn a dozen contributions from different authors into a unified book is paste them into a template and click &amp;ldquo;typeset&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Ok, it did actually take some noodling to iron out the kinks. But mainly just because of my poor planning. Having done it once now and learned the gotchas, a second go would come pretty close to the copyâpasteâtypeset dream.&lt;/p&gt;

&lt;p&gt;So for me, the moral is that philosophers in general should settle on a similar standard  (all academics, really). If we did, we&amp;rsquo;d have a lot more freedom from commercial publishers. We could publish open access books like this on the regular. The books would be freely and easily available to all, and authors would retain copyright.&lt;/p&gt;

&lt;p&gt;Collective action problems plague academia, and philosophical publishing in particular. But this one&amp;rsquo;s about as close to an opportunity for a major Pareto improvement as we&amp;rsquo;re likely to get.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Nobody Expects the Chance Function!</title>
      <link>http://jonathanweisberg.org/post/samet-theorem/</link>
      <pubDate>Fri, 15 Feb 2019 11:52:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/samet-theorem/</guid>
      <description>

&lt;p&gt;Here&amp;rsquo;s a striking result that caught me off guard the other day. It came up in a facebook thread, and judging by the discussion there it caught a few other people in this neighbourhood off guard too.&lt;/p&gt;

&lt;p&gt;The short version: chances are &amp;ldquo;self-expecting&amp;rdquo; pretty much if and only if they&amp;rsquo;re &amp;ldquo;self-certain&amp;rdquo;. Less cryptically: the chance of a proposition equals its expected chance just in case the chance function assigns probability 1 to itself being theÂ true chance function, modulo an exception to be discussed below.&lt;/p&gt;

&lt;p&gt;The same result applies to any probabilities of course, whether they represent physical chances or evidential probabilities or whatever. In fact, thanks to friends on facebook, I learned that it drives &lt;a href=&#34;https://philpapers.org/archive/DOREAG.pdf&#34; target=&#34;_blank&#34;&gt;this lovely paper by Kevin Dorst&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I just happened to stumble across it while thinking about chances, because Richard Pettigrew uses the assumption that chances are self-expecting in &lt;a href=&#34;http://fitelson.org/coherence/pettigrew_chance.pdf&#34; target=&#34;_blank&#34;&gt;his &lt;em&gt;Phil Review&lt;/em&gt; paper on accuracy and the Principal Principle&lt;/a&gt;. But later, in &lt;a href=&#34;https://philpapers.org/rec/PETAAT-7&#34; target=&#34;_blank&#34;&gt;his landmark book on accuracy&lt;/a&gt;, he switches to the requirement that they be self-certain. It turns out this isn&amp;rsquo;t a coincidence. The result we&amp;rsquo;re about to look at illuminates this shift.&lt;/p&gt;

&lt;p&gt;The result goes back to 1997 at least, in &lt;a href=&#34;https://homepages.cwi.nl/~jve/books/oldgass/high-order-beliefs.pdf&#34; target=&#34;_blank&#34;&gt;a paper by Dov Samet&lt;/a&gt;. Proving the full result is a bit more involved than what I&amp;rsquo;ll present here. For simplicity, I&amp;rsquo;ll only prove a special case at the end. But along the way we&amp;rsquo;ll look at some suggestive examples that illustrate the full version.&lt;/p&gt;

&lt;h1 id=&#34;the-chance-matrix&#34;&gt;The Chance Matrix&lt;/h1&gt;

&lt;p&gt;Imagine we have just four possible worlds, resulting from two tosses of a coin. What are the physical chances at each of the four possible worlds $HH$, $HT$, $TH$, and $TT$? $\newcommand{\mstar}{\mathfrak{m}^*} \newcommand{\C}{\mathbf{C}}$&lt;/p&gt;

&lt;p&gt;One natural thought is to apply Laplace&amp;rsquo;s classic rule of succession: given $s$ heads out of $n$ tosses, conclude that the probability of heads on each toss is $(s+1)/(n+2)$. So at $HH$-world for example, the chance of heads was $3/ 4$ on each toss.&lt;/p&gt;

&lt;p&gt;If we assume the tosses are independent, then $HH$-world had chance $(3/ 4)(3/ 4) = 9/16$ of being actual, according to the chance function at $HH$-world. Whereas $HT$-world had chance $(3/ 4)(1/ 4) = 3/16$ of being actual at $HH$-world. The full chance function at $HH$-world can be displayed as a column vector:
$$
\left(
    \begin{matrix}
        9/16\\&lt;br /&gt;
        3/16\\&lt;br /&gt;
        3/16\\&lt;br /&gt;
        1/16
    \end{matrix}
\right).
$$
Applying the same recipe at $HT$-world would give us a different column vector. And sticking the columns for all four worlds together, we get a $4 \times 4$ &lt;em&gt;chance matrix&lt;/em&gt; for our space of possible worlds:
$$
\mathbf{C} =
    \left(
        \begin{matrix}
            9/16 &amp;amp; 1/ 4 &amp;amp; 1/ 4 &amp;amp; 1/16\\&lt;br /&gt;
            3/16 &amp;amp; 1/ 4 &amp;amp; 1/ 4 &amp;amp; 3/16\\&lt;br /&gt;
            3/16 &amp;amp; 1/ 4 &amp;amp; 1/ 4 &amp;amp; 3/16\\&lt;br /&gt;
            1/16 &amp;amp; 1/ 4 &amp;amp; 1/ 4 &amp;amp; 9/16
        \end{matrix}
    \right).
$$
Each column gives the chances &lt;em&gt;at&lt;/em&gt; a world, while a row gives the chances &lt;em&gt;of&lt;/em&gt; a world. For example, entry $c_{14}$ gives the chance &lt;em&gt;at&lt;/em&gt; world $4$ &lt;em&gt;of&lt;/em&gt; world $1$ being actual. It says how likely the sequence $HH$ was if the actual unfolding of events is instead $TT$, namely $1/16$.&lt;/p&gt;

&lt;p&gt;A different thought would be to appeal to Carnap&amp;rsquo;s notorious &amp;ldquo;logical&amp;rdquo; prior $\mstar$:
$$
\mstar =
    \left(
        \begin{matrix}
            1/3\\&lt;br /&gt;
            1/6\\&lt;br /&gt;
            1/6\\&lt;br /&gt;
            1/3
        \end{matrix}
    \right).
$$
This assignment of probabilities ignores the actual unfolding of events in each world. It falls out of a bit of a priori reasoning instead. There are three possible outcomes: $2$ heads, $1$ head, or $0$ heads. Each is equally likely, $1/ 3$. But there are two ways to get $1$ head, so the $1/ 3$ there gets subdivided equally between $HT$ and $TH$, leaving $1/6$ for each.&lt;/p&gt;

&lt;p&gt;Since these chances ignore the actual unfolding of events in each world, the chance matrix we get here is extremely anti-Humean. It&amp;rsquo;s just four repetitions of $\mstar$:
$$
\mathbf{C} =
    \left(
        \begin{matrix}
            1/3 &amp;amp; 1/3 &amp;amp; 1/3 &amp;amp; 1/3\\&lt;br /&gt;
            1/6 &amp;amp; 1/6 &amp;amp; 1/6 &amp;amp; 1/6\\&lt;br /&gt;
            1/6 &amp;amp; 1/6 &amp;amp; 1/6 &amp;amp; 1/6\\&lt;br /&gt;
            1/3 &amp;amp; 1/3 &amp;amp; 1/3 &amp;amp; 1/3
        \end{matrix}
    \right).
$$
You might think that&amp;rsquo;s a pretty terrible theory of chance, and I sympathize. But what we&amp;rsquo;re about to see is that, of our two chance matrices, only the second is &amp;ldquo;self-expecting&amp;rdquo;. And its terribleness is part of the reason why.&lt;/p&gt;

&lt;h1 id=&#34;self-expectation&#34;&gt;Self Expectation&lt;/h1&gt;

&lt;p&gt;Pettigrew&amp;rsquo;s &lt;em&gt;Phil Review&lt;/em&gt; paper assumes that chance functions are &amp;ldquo;self-expecting&amp;rdquo;. The chance of a proposition at a given world must equal its expected value, where the expectation is taken according to the chances at that world.&lt;/p&gt;

&lt;p&gt;In terms of a chance matrix $\C$, this amounts to the requirement that $\C \C = \C$. When we multiply $\C$ by $\C$, we take dot-products of rows and columns. For example, if we were doing the calculation by hand, we&amp;rsquo;d start by multiplying the first row of $\C$ by the first column of $\C$. And this is just the weighted average of the various possible chances &lt;em&gt;of&lt;/em&gt; the first world, where the weights are the chances &lt;em&gt;at&lt;/em&gt; that world. In other words, it&amp;rsquo;s the expected chance &lt;em&gt;of&lt;/em&gt; $HH$-world &lt;em&gt;at&lt;/em&gt; $HH$-world.&lt;/p&gt;

&lt;p&gt;In general, the dot product of row $i$ with column $j$ is the expected chance &lt;em&gt;of&lt;/em&gt; world $i$ &lt;em&gt;at&lt;/em&gt; world $j$. For this expected chance to equal the chance of world $i$ at world $j$, it must be that $\C \C = \C$. More succinctly, $\C^2 = \C$.&lt;/p&gt;

&lt;p&gt;Matrices that have this property&amp;mdash;squaring them leaves them unchanged&amp;mdash;are called &lt;em&gt;idempotent&lt;/em&gt;. And when our matrices are column stochastic (all values are nonnegative and each column sums to $1$), idempotence is a very&amp;hellip; well, potent requirement.&lt;/p&gt;

&lt;p&gt;For example, our first chance matrix based on Laplace&amp;rsquo;s rule of succession is not idempotent. Its square is not itself, but something quite different. Our second, Carnapian matrix is idempotent though. Its square is just itself. And that&amp;rsquo;s not a coincidence.&lt;/p&gt;

&lt;h1 id=&#34;self-certainty&#34;&gt;Self Certainty&lt;/h1&gt;

&lt;p&gt;Any chance matrix whose columns are redundant will be idempotent. After all, if the chances are the same &lt;em&gt;at&lt;/em&gt; every world, the expected value &lt;em&gt;of&lt;/em&gt; any world is always the same. So its expected value just is the value it has at every world.&lt;/p&gt;

&lt;p&gt;But redundant columns also mean that the chances are &lt;em&gt;self certain&lt;/em&gt;. Each world&amp;rsquo;s chance assignment gives zero probability to the chances being anything other than what they are at that world. Because there &lt;em&gt;are&lt;/em&gt; no worlds where the chances are different.&lt;/p&gt;

&lt;p&gt;The chances can vary from world to world and still be self-expecting though. There are idempotent chance matrices where the columns are not simply redundant. For example, here&amp;rsquo;s another idempotent chance matrix:
$$
\left(
    \begin{matrix}
        1/3 &amp;amp; 1/3 &amp;amp; 0      &amp;amp;    0\\&lt;br /&gt;
        2/3 &amp;amp; 2/3 &amp;amp; 0      &amp;amp;    0\\&lt;br /&gt;
            0 &amp;amp;   0 &amp;amp; 1/ 4 &amp;amp; 1/ 4\\&lt;br /&gt;
            0 &amp;amp;   0 &amp;amp; 3/ 4 &amp;amp; 3/ 4
    \end{matrix}
\right).
$$
But notice how it&amp;rsquo;s still kind of a degenerate case. There are two, disjoint regions of modal space here that regard one another as zero-chance. And within each region the chances are the same at each world. Worlds $1$ and $2$ have the same chances, and they give zero chance to worlds $3$ and $4$. And vice versa from the point of view of worlds $3$ and $4$.&lt;/p&gt;

&lt;p&gt;In other words, self-expectation and self-certainty go hand in hand here once again.&lt;/p&gt;

&lt;h1 id=&#34;a-sliver-of-daylight&#34;&gt;A Sliver of Daylight&lt;/h1&gt;

&lt;p&gt;Is there any daylight at all then between self-expectation and self-certainty?&lt;/p&gt;

&lt;p&gt;Self-certainty entails self-expectation, and the argument is pretty short. If the only worlds with positive chance according to world $j$ assign the same chances as world $j$ does, then any average of those chances will just be those same chances.&lt;/p&gt;

&lt;p&gt;But self-expectation doesn&amp;rsquo;t &lt;em&gt;quite&lt;/em&gt; entail self-certainty. For example, here&amp;rsquo;s an idempotent chance matrix that&amp;rsquo;s not self-certain:
$$
\left(
    \begin{matrix}
        1/3 &amp;amp; 1/3 &amp;amp; 0      &amp;amp;    0 &amp;amp; 25/94\\&lt;br /&gt;
        2/3 &amp;amp; 2/3 &amp;amp; 0      &amp;amp;    0 &amp;amp; 25/47\\&lt;br /&gt;
            0 &amp;amp;   0 &amp;amp; 1/ 4 &amp;amp; 1/ 4 &amp;amp; 19/376\\&lt;br /&gt;
            0 &amp;amp;   0 &amp;amp; 3/ 4 &amp;amp; 3/ 4 &amp;amp; 57/376\\&lt;br /&gt;
            0 &amp;amp;   0 &amp;amp;    0 &amp;amp;    0 &amp;amp; 0
    \end{matrix}
\right).
$$
It&amp;rsquo;s kind of a lame counterexample though, because the new, fifth world we&amp;rsquo;ve introduced (the coin explodes or something idk) has zero chance at every world, even itself.&lt;/p&gt;

&lt;p&gt;In fact this is what Samet proves: this is the only kind of counterexample possible! If probabilities are self-expecting, then they must be either self-certain or self-effacing. They must assign zero chance to the chances being otherwise, or they must assign chance one to them being otherwise.&lt;/p&gt;

&lt;p&gt;In terms of matrices, there are only three kinds of idempotent chance matrix:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;All columns are identical.&lt;/li&gt;
&lt;li&gt;The matrix is &lt;a href=&#34;https://en.wikipedia.org/wiki/Block_matrix#Block_diagonal_matrices&#34; target=&#34;_blank&#34;&gt;block diagonal&lt;/a&gt;, with identical columns inside each block.&lt;/li&gt;
&lt;li&gt;The matrix is as in (2), except for some columns $j_1, \ldots, j_n$. But the corresponding rows $j_1, \ldots, j_n$ contain only zeros.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Strictly speaking (1) is actually a special case of (2). But (1) deserves direct attention because it arises in a way that&amp;rsquo;s interesting both philosophically and mathematically.&lt;/p&gt;

&lt;h1 id=&#34;the-connected-case&#34;&gt;The Connected Case&lt;/h1&gt;

&lt;p&gt;Here&amp;rsquo;s a natural thought, one that&amp;rsquo;s driven a lot of the literature on chance and Lewis&amp;rsquo; Principal Principle. The thought: however events unfold at one world, there&amp;rsquo;s a chance they could have evolved differently. There&amp;rsquo;s even some small, non-zero chance they could have evolved quite differently.&lt;/p&gt;

&lt;p&gt;Taking this thought a bit further, you might think there&amp;rsquo;s a region of modal space where, even though worlds $w_1$ and $w_n$ have different chances, $w_n$ is always reachable from world $w_1$. More exactly, there&amp;rsquo;s always a connecting sequence of worlds $w_1, w_2, \ldots, w_n$ where $w_i$ gives non-zero chance to $w_{i+1}$.&lt;/p&gt;

&lt;p&gt;In terms of coin tosses, maybe it&amp;rsquo;s a law of nature that all coins land heads when all hundred out of one hundred flips land heads. But when there&amp;rsquo;s a mix of heads and tails, there&amp;rsquo;s at least some chance the mix could have had a few more heads, or a few more tails. So every world where the sequence isn&amp;rsquo;t perfectly uniform can be reached from every other. If not in a single, positive-chance hop, then at least by a series of hops, perhaps by switching the outcomes of the flips one at a time for example.&lt;/p&gt;

&lt;p&gt;In terms of graphs, such a region of modal space is said to be &lt;em&gt;connected&lt;/em&gt;. In terms of matrices, it amounts to the chance matrix for this region being &lt;em&gt;regular&lt;/em&gt;: there must be some power $n$ such that $\C^n$ contains all positive entries.&lt;/p&gt;

&lt;p&gt;Now, regular matrices have the remarkable property that, as we multiply them against themselves more and more times, the result converges to a matrix $\mathbf{P}$ whose columns are all identical:
$$ \lim_{n \rightarrow \infty} \C^n = \mathbf{P} =
\left(
    \begin{matrix}
        p_1    &amp;amp; \ldots &amp;amp; p_1    \\&lt;br /&gt;
        \vdots &amp;amp; \ldots &amp;amp; \vdots \\&lt;br /&gt;
        p_k    &amp;amp; \ldots &amp;amp; p_k    \\&lt;br /&gt;
    \end{matrix}
\right).
$$
Now recall that for $\C$ to be self-expecting, it must be idempotent, meaning $\C^2 = \C$. But that means $\C^n = \C$ for any power $n$. But then $\C = \mathbf{P}$, so $\C$ must already have redundant columns.&lt;/p&gt;

&lt;p&gt;What does this mean for us? One way to think about it: there isn&amp;rsquo;t as much room for chances to vary from world to world as one might have thought. If the chances are going to be self-expecting, they must be the same at every world across a whole region of modal space despite the facts turning out quite differently at various worlds across that region.&lt;/p&gt;

&lt;p&gt;This point is strongly reminiscent of David Lewis&amp;rsquo; famous &amp;ldquo;Big Bad Bug&amp;rdquo; of course. And there&amp;rsquo;s tons of relevant literature, most of which I confess I never really absorbed. So I&amp;rsquo;ll close by linking to just one paper I&amp;rsquo;m finding especially helpful on this right now, Richard Pettigrew&amp;rsquo;s &lt;a href=&#34;https://drive.google.com/file/d/0B-Gzj6gcSXKrcW5MeUhIN2Jsd1k/view&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;What Chance-Credence Norms Should Not Be&amp;rdquo;&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>