<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Neural Networks on Jonathan Weisberg</title>
    <link>http://jonathanweisberg.org/tags/neural-networks/index.xml</link>
    <description>Recent content in Neural Networks on Jonathan Weisberg</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="http://jonathanweisberg.org/tags/neural-networks/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Building a Neural Network from Scratch: Part 2</title>
      <link>http://jonathanweisberg.org/post/A%20Neural%20Network%20from%20Scratch%20-%20Part%202/</link>
      <pubDate>Wed, 07 Mar 2018 00:00:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/A%20Neural%20Network%20from%20Scratch%20-%20Part%202/</guid>
      <description>&lt;p&gt;In this post we&amp;rsquo;ll improve our training algorithm from the &lt;a href=&#34;http://jonathanweisberg.org/post/A%20Neural%20Network%20from%20Scratch%20-%20Part%201/&#34;&gt;previous post&lt;/a&gt;. When we&amp;rsquo;re done we&amp;rsquo;ll be able to achieve 98% precision on the MNIST data set, after just 9 epochs of training&amp;mdash;which only takes about 30 seconds to run on my laptop.&lt;/p&gt;

&lt;p&gt;For comparison, last time we only achieved 92% precision after 2,000 epochs of training, which took over an hour!&lt;/p&gt;

&lt;p&gt;The main driver in this improvement is just switching from batch gradient descent to &lt;em&gt;mini&lt;/em&gt;-batch gradient descent. But we&amp;rsquo;ll also make two other, smaller improvements: we&amp;rsquo;ll add momentum to our descent algorithm, and we&amp;rsquo;ll smarten up the initialization of our network&amp;rsquo;s weights.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll also reorganize our code a bit while we&amp;rsquo;re at it, making things more modular.&lt;/p&gt;

&lt;p&gt;But first we need to import and massage our data. These steps are the same as in the previous post:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.datasets import fetch_mldata
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

# import
mnist = fetch_mldata(&#39;MNIST original&#39;)
X, y = mnist[&amp;quot;data&amp;quot;], mnist[&amp;quot;target&amp;quot;]

# scale
X = X / 255

# one-hot encode labels
digits = 10
examples = y.shape[0]
y = y.reshape(1, examples)
Y_new = np.eye(digits)[y.astype(&#39;int32&#39;)]
Y_new = Y_new.T.reshape(digits, examples)

# split, reshape, shuffle
m = 60000
m_test = X.shape[0] - m
X_train, X_test = X[:m].T, X[m:].T
Y_train, Y_test = Y_new[:,:m], Y_new[:,m:]
shuffle_index = np.random.permutation(m)
X_train, Y_train = X_train[:, shuffle_index], Y_train[:, shuffle_index]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we&amp;rsquo;ll define our key functions. Only the last two are new, and they just put the steps of forward and backward propagation into their own functions. This tidies up the training code to follow, so that we can focus on the novel elements, especially mini-batch descent and momentum.&lt;/p&gt;

&lt;p&gt;Notice that in the process we introduce three dictionaries:&lt;code&gt;params&lt;/code&gt;, &lt;code&gt;cache&lt;/code&gt;, and &lt;code&gt;grads&lt;/code&gt;. These are for conveniently passing information back and forth between the forward and backward passes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def sigmoid(z):
    s = 1. / (1. + np.exp(-z))
    return s

def compute_loss(Y, Y_hat):

    L_sum = np.sum(np.multiply(Y, np.log(Y_hat)))
    m = Y.shape[1]
    L = -(1./m) * L_sum

    return L

def feed_forward(X, params):

    cache = {}

    cache[&amp;quot;Z1&amp;quot;] = np.matmul(params[&amp;quot;W1&amp;quot;], X) + params[&amp;quot;b1&amp;quot;]
    cache[&amp;quot;A1&amp;quot;] = sigmoid(cache[&amp;quot;Z1&amp;quot;])
    cache[&amp;quot;Z2&amp;quot;] = np.matmul(params[&amp;quot;W2&amp;quot;], cache[&amp;quot;A1&amp;quot;]) + params[&amp;quot;b2&amp;quot;]
    cache[&amp;quot;A2&amp;quot;] = np.exp(cache[&amp;quot;Z2&amp;quot;]) / np.sum(np.exp(cache[&amp;quot;Z2&amp;quot;]), axis=0)

    return cache

def back_propagate(X, Y, params, cache):

    dZ2 = cache[&amp;quot;A2&amp;quot;] - Y
    dW2 = (1./m_batch) * np.matmul(dZ2, cache[&amp;quot;A1&amp;quot;].T)
    db2 = (1./m_batch) * np.sum(dZ2, axis=1, keepdims=True)

    dA1 = np.matmul(params[&amp;quot;W2&amp;quot;].T, dZ2)
    dZ1 = dA1 * sigmoid(cache[&amp;quot;Z1&amp;quot;]) * (1 - sigmoid(cache[&amp;quot;Z1&amp;quot;]))
    dW1 = (1./m_batch) * np.matmul(dZ1, X.T)
    db1 = (1./m_batch) * np.sum(dZ1, axis=1, keepdims=True)

    grads = {&amp;quot;dW1&amp;quot;: dW1, &amp;quot;db1&amp;quot;: db1, &amp;quot;dW2&amp;quot;: dW2, &amp;quot;db2&amp;quot;: db2}

    return grads
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now for the substantive stuff.&lt;/p&gt;

&lt;p&gt;To switch to mini-batch descent, we add another &lt;code&gt;for&lt;/code&gt; loop inside the pass through each epoch. At each pass we randomly shuffle the training set, then iterate through it in chunks of &lt;code&gt;batch_size&lt;/code&gt;, which we&amp;rsquo;ll arbitrarily set to 128. We&amp;rsquo;ll see the code for all this in a moment.&lt;/p&gt;

&lt;p&gt;Next, to add momentum, we keep a moving average of our gradients. So instead of updating our parameters by doing e.g.:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;params[&amp;quot;W1&amp;quot;] = params[&amp;quot;W1&amp;quot;] - learning_rate * grads[&amp;quot;dW1&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;we do this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;V_dW1 = (beta * V_dW1 + (1. - beta) * grads[&amp;quot;dW1&amp;quot;])
params[&amp;quot;W1&amp;quot;] = params[&amp;quot;W1&amp;quot;] - learning_rate * V_dW1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, to smarten up our initialization, we shrink the variance of the weights in each layer. Following &lt;a href=&#34;https://www.coursera.org/learn/deep-neural-network/lecture/RwqYe/weight-initialization-for-deep-networks&#34; target=&#34;_blank&#34;&gt;this nice video&lt;/a&gt; by Andrew Ng (whose excellent Coursera materials I&amp;rsquo;ve been relying on heavily in these posts), we&amp;rsquo;ll set the variance for each layer to $1/n$, where $n$ is the number of inputs feeding into that layer.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ve been using the &lt;code&gt;np.random.randn()&lt;/code&gt; function to get our initial weights. And this function draws from the standard normal distribution. So to adjust the variance to $1/n$, we just divide by $\sqrt{n}$. In code this means that instead of doing e.g. &lt;code&gt;np.random.randn(n_h, n_x)&lt;/code&gt;, we do &lt;code&gt;np.random.randn(n_h, n_x) * np.sqrt(1. / n_x)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Ok that covers our three improvements. Let&amp;rsquo;s build and train!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(138)

# hyperparameters
n_x = X_train.shape[0]
n_h = 64
learning_rate = 4
beta = .9
batch_size = 128
batches = -(-m // batch_size)

# initialization
params = { &amp;quot;W1&amp;quot;: np.random.randn(n_h, n_x) * np.sqrt(1. / n_x),
           &amp;quot;b1&amp;quot;: np.zeros((n_h, 1)) * np.sqrt(1. / n_x),
           &amp;quot;W2&amp;quot;: np.random.randn(digits, n_h) * np.sqrt(1. / n_h),
           &amp;quot;b2&amp;quot;: np.zeros((digits, 1)) * np.sqrt(1. / n_h) }

V_dW1 = np.zeros(params[&amp;quot;W1&amp;quot;].shape)
V_db1 = np.zeros(params[&amp;quot;b1&amp;quot;].shape)
V_dW2 = np.zeros(params[&amp;quot;W2&amp;quot;].shape)
V_db2 = np.zeros(params[&amp;quot;b2&amp;quot;].shape)

# train
for i in range(9):

    permutation = np.random.permutation(X_train.shape[1])
    X_train_shuffled = X_train[:, permutation]
    Y_train_shuffled = Y_train[:, permutation]

    for j in range(batches):

        begin = j * batch_size
        end = min(begin + batch_size, X_train.shape[1] - 1)
        X = X_train_shuffled[:, begin:end]
        Y = Y_train_shuffled[:, begin:end]
        m_batch = end - begin

        cache = feed_forward(X, params)
        grads = back_propagate(X, Y, params, cache)

        V_dW1 = (beta * V_dW1 + (1. - beta) * grads[&amp;quot;dW1&amp;quot;])
        V_db1 = (beta * V_db1 + (1. - beta) * grads[&amp;quot;db1&amp;quot;])
        V_dW2 = (beta * V_dW2 + (1. - beta) * grads[&amp;quot;dW2&amp;quot;])
        V_db2 = (beta * V_db2 + (1. - beta) * grads[&amp;quot;db2&amp;quot;])

        params[&amp;quot;W1&amp;quot;] = params[&amp;quot;W1&amp;quot;] - learning_rate * V_dW1
        params[&amp;quot;b1&amp;quot;] = params[&amp;quot;b1&amp;quot;] - learning_rate * V_db1
        params[&amp;quot;W2&amp;quot;] = params[&amp;quot;W2&amp;quot;] - learning_rate * V_dW2
        params[&amp;quot;b2&amp;quot;] = params[&amp;quot;b2&amp;quot;] - learning_rate * V_db2

    cache = feed_forward(X_train, params)
    train_cost = compute_loss(Y_train, cache[&amp;quot;A2&amp;quot;])
    cache = feed_forward(X_test, params)
    test_cost = compute_loss(Y_test, cache[&amp;quot;A2&amp;quot;])
    print(&amp;quot;Epoch {}: training cost = {}, test cost = {}&amp;quot;.format(i+1 ,train_cost, test_cost))

print(&amp;quot;Done.&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Epoch 1: training cost = 0.15587093418167058, test cost = 0.16223940981168986
Epoch 2: training cost = 0.09417519634799829, test cost = 0.11032242938356147
Epoch 3: training cost = 0.07205872840102934, test cost = 0.0958078559246339
Epoch 4: training cost = 0.07008115814138867, test cost = 0.1010270024817398
Epoch 5: training cost = 0.05501929068580713, test cost = 0.09527116695490956
Epoch 6: training cost = 0.042663638371140164, test cost = 0.08268937190759178
Epoch 7: training cost = 0.03615501088752129, test cost = 0.08188431384719108
Epoch 8: training cost = 0.03610956910064329, test cost = 0.08675249924246693
Epoch 9: training cost = 0.027582647825206745, test cost = 0.08023754855128316
Done.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;How&amp;rsquo;d we do?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cache = feed_forward(X_test, params)
predictions = np.argmax(cache[&amp;quot;A2&amp;quot;], axis=0)
labels = np.argmax(Y_test, axis=0)

print(classification_report(predictions, labels))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;             precision    recall  f1-score   support

          0       0.99      0.98      0.98       984
          1       0.99      0.99      0.99      1136
          2       0.98      0.98      0.98      1037
          3       0.97      0.98      0.97      1004
          4       0.97      0.98      0.98       970
          5       0.97      0.96      0.97       900
          6       0.97      0.99      0.98       942
          7       0.97      0.97      0.97      1026
          8       0.97      0.96      0.97       982
          9       0.97      0.96      0.97      1019

avg / total       0.98      0.98      0.98     10000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And there it is: 98% precision in just 9 epochs of training.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Building a Neural Network from Scratch: Part 1</title>
      <link>http://jonathanweisberg.org/post/a%20neural%20network%20from%20scratch%20-%20part%201/</link>
      <pubDate>Mon, 05 Mar 2018 00:00:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/a%20neural%20network%20from%20scratch%20-%20part%201/</guid>
      <description>

&lt;p&gt;In this post we&amp;rsquo;re going to build a neural network from scratch. We&amp;rsquo;ll train it to recognize hand-written digits, using the famous MNIST data set.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll use just basic Python with NumPy to build our network (no high-level stuff like Keras or TensorFlow). We will dip into scikit-learn, but only to get the MNIST data and to assess our model once its built.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll start with the simplest possible &amp;ldquo;network&amp;rdquo;: a single node that recognizes just the digit 0. This is actually just an implementation of logistic regression, which may seem kind of silly. But it&amp;rsquo;ll help us get some key components working before things get more complicated.&lt;/p&gt;

&lt;p&gt;Then we&amp;rsquo;ll extend that into a network with one hidden layer, still recognizing just 0. Then we&amp;rsquo;ll add a softmax for recognizing all the digits 0 through 9. That&amp;rsquo;ll give us a 92% accurate digit-recognizer, bringing us up to the cutting edge of 1985 technology.&lt;/p&gt;

&lt;p&gt;In a followup post we&amp;rsquo;ll bring that up into the high nineties by making sundry improvements: better optimization, more hidden layers, and smarter initialization.&lt;/p&gt;

&lt;h1 id=&#34;1-hello-mnist&#34;&gt;1. Hello, MNIST&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/MNIST_database&#34; target=&#34;_blank&#34;&gt;MNIST&lt;/a&gt; contains 70,000 images of hand-written digits, each 28 x 28 pixels, in greyscale with pixel-values from 0 to 255. We could &lt;a href=&#34;http://yann.lecun.com/exdb/mnist/&#34; target=&#34;_blank&#34;&gt;download&lt;/a&gt; and preprocess the data ourselves. But the makers of scikit-learn already did that for us. Since it would be rude to neglect their efforts, we&amp;rsquo;ll just import it:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.datasets import fetch_mldata
mnist = fetch_mldata(&#39;MNIST original&#39;)
X, y = mnist[&amp;quot;data&amp;quot;], mnist[&amp;quot;target&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;rsquo;ll normalize the data to keep our gradients manageable:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X = X / 255
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The default MNIST labels record &lt;code&gt;7&lt;/code&gt; for an image of a seven, &lt;code&gt;4&lt;/code&gt; for an image of a four, etc. But we&amp;rsquo;re just building a zero-classifier for now. So we want our labels to say &lt;code&gt;1&lt;/code&gt; when we have a zero, and &lt;code&gt;0&lt;/code&gt; otherwise (intuitive, I know). So we&amp;rsquo;ll overwrite the labels to make that happen:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np

y_new = np.zeros(y.shape)
y_new[np.where(y == 0.0)[0]] = 1
y = y_new
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can make our train/test split. The MNIST images are pre-arranged so that the first 60,000 can be used for training, and the last 10,000 for testing. We&amp;rsquo;ll also transform the data into the shape we want, with each example in a column (instead of a row):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m = 60000
m_test = X.shape[0] - m

X_train, X_test = X[:m].T, X[m:].T
y_train, y_test = y[:m].reshape(1,m), y[m:].reshape(1,m_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally we&amp;rsquo;ll shuffle the training set for good measure:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(138)
shuffle_index = np.random.permutation(m)
X_train, y_train = X_train[:,shuffle_index], y_train[:,shuffle_index]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s have a look at a random image and label just to make sure we didn&amp;rsquo;t throw anything out of wack:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt

i = 3
plt.imshow(X_train[:,i].reshape(28,28), cmap = matplotlib.cm.binary)
plt.axis(&amp;quot;off&amp;quot;)
plt.show()
print(y_train[:,i])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/nn_from_scratch/output_13_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[1.]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s a zero, so we want the label to be &lt;code&gt;1&lt;/code&gt;, which it is. Looks good, so let&amp;rsquo;s build our first network.&lt;/p&gt;

&lt;h1 id=&#34;2-a-single-neuron-aka-logistic-regression&#34;&gt;2. A Single Neuron (aka Logistic Regression)&lt;/h1&gt;

&lt;p&gt;We want to build a simple, feed-forward network with 784 inputs (=28 x 28), and a single sigmoid unit generating the output.&lt;/p&gt;

&lt;h2 id=&#34;2-1-forward-propogation&#34;&gt;2.1 Forward Propogation&lt;/h2&gt;

&lt;p&gt;The forward pass on a single example $x$ executes the following computation:
$$ \hat{y} = \sigma(w^T x + b). $$
Here $\sigma$ is the sigmoid function:
$$ \sigma(z) = \frac{1}{1 + e^{-z}}. $$
So let&amp;rsquo;s define:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def sigmoid(z):
    s = 1 / (1 + np.exp(-z))
    return s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;rsquo;ll vectorize by stacking examples side-by-side, so that our input matrix $X$ has an example in each column. The vectorized form of the forward pass is then:
$$ \hat{y} = \sigma(w^T X + b). $$
Note that $\hat{y}$ is now a vector, not a scalar as it was in the previous equation.&lt;/p&gt;

&lt;p&gt;In our code we&amp;rsquo;ll compute this in two stages: &lt;code&gt;Z = np.matmul(W.T, X) + b&lt;/code&gt; and then &lt;code&gt;A = sigmoid(Z)&lt;/code&gt;. (&lt;code&gt;A&lt;/code&gt; for Activation.) Breaking things up into stages like this is just for tidiness—it&amp;rsquo;ll make our forward propagation computations mirror the steps in our backward propagation computations.&lt;/p&gt;

&lt;h2 id=&#34;2-2-cost-function&#34;&gt;2.2 Cost Function&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;ll use cross-entropy for our cost function. The formula for a single training example is:
$$ L(y, \hat{y}) = -y \log(\hat{y}) - (1-y) \log(1-\hat{y}). $$
Averaging over a training set of $m$ examples we then have:
$$ L(Y, \hat{Y}) = -\frac{1}{m} \sum_{i=1}^m \left( y^{(i)} \log(\hat{y}^{(i)}) + (1-y^{(i)}) \log(1-\hat{y}^{(i)}) \right). $$
So let&amp;rsquo;s define:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_loss(Y, Y_hat):

    m = Y.shape[1]
    L = -(1./m) * ( np.sum( np.multiply(np.log(Y_hat),Y) ) + np.sum( np.multiply(np.log(1-Y_hat),(1-Y)) ) )

    return L
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;2-3-backward-propagation&#34;&gt;2.3 Backward Propagation&lt;/h2&gt;

&lt;p&gt;For backpropagation, we&amp;rsquo;ll need to know how $L$ changes with respect to each component $w_j$ of $w$. That is, we must compute each $\partial L / \partial w_j$.&lt;/p&gt;

&lt;p&gt;Focusing on a single example will make it easier to derive the formulas we need. Holding all values except $w_j$ fixed, we can think of $L$ as being computed in three steps: $w_j \rightarrow z \rightarrow \hat{y} \rightarrow L$. The formulas for these steps are:
$$
  \begin{align}
  z &amp;amp;= w^T x + b,\newline
  \hat{y} &amp;amp;= \sigma(z),\newline
  L(y, \hat{y}) &amp;amp;= -y \log(\hat{y}) - (1-y) \log(1-\hat{y}).
  \end{align}
$$
And the chain rule tells us:
$$
  \frac{\partial L}{\partial w_j} = \frac{\partial L}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial z} \frac{\partial z}{\partial w_j}.
$$
Looking at $\partial L / \partial \hat{y}$ first:
$$
  \begin{align}
    \frac{\partial L}{\partial \hat{y}} &amp;amp;= \frac{\partial}{\partial \hat{y}} \left( -y \log(\hat{y}) - (1-y) \log(1-\hat{y}) \right)\newline
      &amp;amp;= -y \frac{\partial}{\partial \hat{y}} \log(\hat{y}) - (1-y) \frac{\partial}{\partial \hat{y}} \log(1-\hat{y})\newline
      &amp;amp;= \frac{-y}{\hat{y}} + \frac{(1-y) }{1 - \hat{y}}\newline
      &amp;amp;= \frac{\hat{y} - y}{\hat{y}(1 - \hat{y})}.
  \end{align}
$$
Next we want $\partial \hat{y} / \partial z$:
$$
  \begin{align}
    \frac{\partial}{\partial z} \sigma(z) &amp;amp;= \frac{\partial}{\partial z} \left( \frac{1}{1 + e^{-z}} \right)\newline
      &amp;amp;= - \frac{1}{(1 + e^{-z})^2} \frac{\partial}{\partial z} \left( 1 + e^{-z} \right)\newline
      &amp;amp;= \frac{e^{-z}}{(1 + e^{-z})^2}\newline
      &amp;amp;= \frac{1}{1 + e^{-z}} \frac{e^{-z}}{1 + e^{-z}}\newline
      &amp;amp;= \sigma(z) \frac{e^{-z}}{1 + e^{-z}}\newline
      &amp;amp;= \sigma(z) \left( 1 - \frac{1}{1 + e^{-z}} \right)\newline
      &amp;amp;= \sigma(z) \left( 1 - \sigma(z) \right)\newline
      &amp;amp;= \hat{y} (1-\hat{y}).
  \end{align}
$$
Lastly we tackle $\partial z / \partial w_j$:
$$
  \begin{align}
    \frac{\partial}{\partial w_j} (w^T x + b) &amp;amp;= \frac{\partial}{\partial w_j} (w_0 x_0 + \ldots + w_n x_n + b)\newline
      &amp;amp;= w_j.
  \end{align}
$$
Finally we can substitute into the chain rule to find:
$$
  \begin{align}
    \frac{\partial L}{\partial w_j} &amp;amp;= \frac{\partial L}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial z} \frac{\partial z}{\partial w_j}\newline
    &amp;amp;= \frac{\hat{y} - y}{\hat{y}(1 - \hat{y})} \hat{y} (1-\hat{y}) w_j\newline
    &amp;amp;= (\hat{y} - y) w_j.\newline
  \end{align}
$$
In vectorized form with $m$ training examples this gives us:
$$
  \frac{\partial L}{\partial w} = \frac{1}{m} X (\hat{y} - y)^T.
$$
What about $\partial L / \partial b$? A very similar derivation yields, for a single example:
$$
  \begin{align}
    \frac{\partial L}{\partial b} &amp;amp;= (\hat{y} - y).
  \end{align}
$$
Which in vectorized form amounts to:
$$
  \frac{\partial L}{\partial b} = \frac{1}{m} \sum_{i=1}^m (\hat{y}^{(i)} - y^{(i)}).
$$
In our code we&amp;rsquo;ll label these gradients according to their denominators, as &lt;code&gt;dW&lt;/code&gt; and &lt;code&gt;db&lt;/code&gt;. So for backpropagation we&amp;rsquo;ll compute &lt;code&gt;dW = (1/m) * np.matmul(X, (A-Y).T)&lt;/code&gt; and &lt;code&gt;db = (1/m) * np.sum(A-Y, axis=1, keepdims=True)&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;2-4-build-train&#34;&gt;2.4 Build &amp;amp; Train&lt;/h2&gt;

&lt;p&gt;Ok we&amp;rsquo;re ready to build and train our network!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;learning_rate = 1

X = X_train
Y = y_train

n_x = X.shape[0]
m = X.shape[1]

W = np.random.randn(n_x, 1) * 0.01
b = np.zeros((1, 1))

for i in range(2000):
    Z = np.matmul(W.T, X) + b
    A = sigmoid(Z)

    cost = compute_loss(Y, A)

    dW = (1/m) * np.matmul(X, (A-Y).T)
    db = (1/m) * np.sum(A-Y, axis=1, keepdims=True)

    W = W - learning_rate * dW
    b = b - learning_rate * db

    if (i % 100 == 0):
        print(&amp;quot;Epoch&amp;quot;, i, &amp;quot;cost: &amp;quot;, cost)

print(&amp;quot;Final cost:&amp;quot;, cost)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Epoch 0 cost:  0.6840801595436431
Epoch 100 cost:  0.041305162058342754
... *snip* ...
Final cost: 0.02514156608481825
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We could probably eek out a bit more accuracy with some more training. But the gains have slowed considerably. So let&amp;rsquo;s just see how we did, by looking at the confusion matrix:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.metrics import classification_report, confusion_matrix

Z = np.matmul(W.T, X_test) + b
A = sigmoid(Z)

predictions = (A&amp;gt;.5)[0,:]
labels = (y_test == 1)[0,:]

print(confusion_matrix(predictions, labels))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[[8980   33]
 [  40  947]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hey, that&amp;rsquo;s actually pretty good! We got 947 of the zeros and missed only 33, while getting nearly all the negative cases right. In terms of f1-score that&amp;rsquo;s 0.99:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(classification_report(predictions, labels))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;             precision    recall  f1-score   support

      False       1.00      1.00      1.00      9013
       True       0.97      0.96      0.96       987

avg / total       0.99      0.99      0.99     10000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, now that we&amp;rsquo;ve got a working model and optimization algorithm, let&amp;rsquo;s enrich it.&lt;/p&gt;

&lt;h1 id=&#34;3-one-hidden-layer&#34;&gt;3. One Hidden Layer&lt;/h1&gt;

&lt;p&gt;Let&amp;rsquo;s add a hidden layer now, with 64 units (a mostly arbitrary choice). I won&amp;rsquo;t go through the derivations of all the formulas for the forward and backward passes this time; they&amp;rsquo;re a pretty direct extension of the work we did earlier. Instead let&amp;rsquo;s just dive right in and build the model:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X = X_train
Y = y_train

n_x = X.shape[0]
n_h = 64
learning_rate = 1

W1 = np.random.randn(n_h, n_x)
b1 = np.zeros((n_h, 1))
W2 = np.random.randn(1, n_h)
b2 = np.zeros((1, 1))

for i in range(2000):

    Z1 = np.matmul(W1, X) + b1
    A1 = sigmoid(Z1)
    Z2 = np.matmul(W2, A1) + b2
    A2 = sigmoid(Z2)

    cost = compute_loss(Y, A2)

    dZ2 = A2-Y
    dW2 = (1./m) * np.matmul(dZ2, A1.T)
    db2 = (1./m) * np.sum(dZ2, axis=1, keepdims=True)

    dA1 = np.matmul(W2.T, dZ2)
    dZ1 = dA1 * sigmoid(Z1) * (1 - sigmoid(Z1))
    dW1 = (1./m) * np.matmul(dZ1, X.T)
    db1 = (1./m) * np.sum(dZ1, axis=1, keepdims=True)

    W2 = W2 - learning_rate * dW2
    b2 = b2 - learning_rate * db2
    W1 = W1 - learning_rate * dW1
    b1 = b1 - learning_rate * db1

    if i % 100 == 0:
        print(&amp;quot;Epoch&amp;quot;, i, &amp;quot;cost: &amp;quot;, cost)

print(&amp;quot;Final cost:&amp;quot;, cost)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Epoch 0 cost:  0.9144384083567224
Epoch 100 cost:  0.08856953026938433
... *snip* ...
Final cost: 0.024249298861903648
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;How&amp;rsquo;d we do?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Z1 = np.matmul(W1, X_test) + b1
A1 = sigmoid(Z1)
Z2 = np.matmul(W2, A1) + b2
A2 = sigmoid(Z2)

predictions = (A2&amp;gt;.5)[0,:]
labels = (y_test == 1)[0,:]

print(confusion_matrix(predictions, labels))
print(classification_report(predictions, labels))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[[8984   36]
 [  36  944]]
             precision    recall  f1-score   support

      False       1.00      1.00      1.00      9020
       True       0.96      0.96      0.96       980

avg / total       0.99      0.99      0.99     10000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hmm, not bad, but about the same as our one-neuron model did. We could do more training and add more nodes/layers. But it&amp;rsquo;ll be slow going until we improve our optimization algorithm, which we&amp;rsquo;ll do in a followup post.&lt;/p&gt;

&lt;p&gt;So for now let&amp;rsquo;s turn to recognizing all ten digits.&lt;/p&gt;

&lt;h1 id=&#34;4-upgrading-to-multiclass&#34;&gt;4. Upgrading to Multiclass&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://medias.spotern.com/spots/w1280/3477.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;4-1-labels&#34;&gt;4.1 Labels&lt;/h2&gt;

&lt;p&gt;First we need to redo our labels. We&amp;rsquo;ll re-import everything, so that we don&amp;rsquo;t have to go back and coordinate with our earlier shuffling:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mnist = fetch_mldata(&#39;MNIST original&#39;)
X, y = mnist[&amp;quot;data&amp;quot;], mnist[&amp;quot;target&amp;quot;]

X = X / 255
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we&amp;rsquo;ll one-hot encode MNIST&amp;rsquo;s labels, to get a 10 x 70,000 array.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;digits = 10
examples = y.shape[0]

y = y.reshape(1, examples)

Y_new = np.eye(digits)[y.astype(&#39;int32&#39;)]
Y_new = Y_new.T.reshape(digits, examples)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we re-split, re-shape, and re-shuffle our training set:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m = 60000
m_test = X.shape[0] - m

X_train, X_test = X[:m].T, X[m:].T
Y_train, Y_test = Y_new[:,:m], Y_new[:,m:]

shuffle_index = np.random.permutation(m)
X_train, Y_train = X_train[:, shuffle_index], Y_train[:, shuffle_index]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A quick check that things are as they should be:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;i = 12
plt.imshow(X_train[:,i].reshape(28,28), cmap = matplotlib.cm.binary)
plt.axis(&amp;quot;off&amp;quot;)
plt.show()
Y_train[:,i]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/nn_from_scratch/output_43_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Looks good, so let&amp;rsquo;s consider what changes we need to make to the model itself.&lt;/p&gt;

&lt;h2 id=&#34;4-2-forward-propagation&#34;&gt;4.2 Forward Propagation&lt;/h2&gt;

&lt;p&gt;Only the last layer of our network is changing. To add the softmax, we have to replace our lone, final node with a 10-unit layer. Its final activations are the exponentials of its $z$-values, normalized across all ten such exponentials. So instead of just computing $\sigma(z)$, we compute the activation for each unit $i$:
$$ \frac{e^{z_i}}{\sum_{j=0}^{9} e^{z_j}}.$$
So, in our vectorized code, the last line of forward propagation will be &lt;code&gt;A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0)&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;4-3-cost-function&#34;&gt;4.3 Cost Function&lt;/h2&gt;

&lt;p&gt;Our cost function now has to generalize to more than two classes. The general formula for $n$ classes is:
$$ L(y, \hat{y}) = -\sum_{i = 0}^n y_i \log(\hat{y}_i). $$
Averaging over $m$ training examples this becomes:
$$ L(Y, \hat{Y}) = - \frac{1}{m} \sum_{j = 0}^m \sum_{i = 0}^n y_i^{(j)} \log(\hat{y}_i^{(j)}). $$
So let&amp;rsquo;s define:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_multiclass_loss(Y, Y_hat):

    L_sum = np.sum(np.multiply(Y, np.log(Y_hat)))
    m = Y.shape[1]
    L = -(1/m) * L_sum

    return L
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;4-4-backprop&#34;&gt;4.4 Backprop&lt;/h2&gt;

&lt;p&gt;Luckily it turns out that backprop isn&amp;rsquo;t really affected by the switch to a softmax. A softmax generalizes the sigmoid activiation we&amp;rsquo;ve been using, and in such a way that the code we wrote earlier still works. We could verify this by deriving:
$$\frac{\partial L}{\partial z_i} = \hat{y}_i - y_i.$$
But I won&amp;rsquo;t walk through the steps here. Let&amp;rsquo;s just go ahead and build our final network.&lt;/p&gt;

&lt;h2 id=&#34;4-5-build-train&#34;&gt;4.5 Build &amp;amp; Train&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;n_x = X_train.shape[0]
n_h = 64
learning_rate = 1

W1 = np.random.randn(n_h, n_x)
b1 = np.zeros((n_h, 1))
W2 = np.random.randn(digits, n_h)
b2 = np.zeros((digits, 1))

X = X_train
Y = Y_train

for i in range(2000):

    Z1 = np.matmul(W1,X) + b1
    A1 = sigmoid(Z1)
    Z2 = np.matmul(W2,A1) + b2
    A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0)

    cost = compute_multiclass_loss(Y, A2)

    dZ2 = A2-Y
    dW2 = (1./m) * np.matmul(dZ2, A1.T)
    db2 = (1./m) * np.sum(dZ2, axis=1, keepdims=True)

    dA1 = np.matmul(W2.T, dZ2)
    dZ1 = dA1 * sigmoid(Z1) * (1 - sigmoid(Z1))
    dW1 = (1./m) * np.matmul(dZ1, X.T)
    db1 = (1./m) * np.sum(dZ1, axis=1, keepdims=True)

    W2 = W2 - learning_rate * dW2
    b2 = b2 - learning_rate * db2
    W1 = W1 - learning_rate * dW1
    b1 = b1 - learning_rate * db1

    if (i % 100 == 0):
        print(&amp;quot;Epoch&amp;quot;, i, &amp;quot;cost: &amp;quot;, cost)

print(&amp;quot;Final cost:&amp;quot;, cost)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Epoch 0 cost:  9.243960401572568
... *snip* ...
Epoch 1900 cost:  0.24585173887243117
Final cost: 0.24072776877870128
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s see how we did:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Z1 = np.matmul(W1, X_test) + b1
A1 = sigmoid(Z1)
Z2 = np.matmul(W2, A1) + b2
A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0)

predictions = np.argmax(A2, axis=0)
labels = np.argmax(Y_test, axis=0)

print(confusion_matrix(predictions, labels))
print(classification_report(predictions, labels))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[[ 946    0   14    3    3   10   12    2    9    4]
 [   0 1112    3    2    1    1    2    8    3    4]
 [   3    4  937   24   10    7    8   18    8    3]
 [   4    2   17  924    1   39    4   13   26    9]
 [   0    1   10    0  905    9   11    9   10   40]
 [  12    5    2   26    3  786   15    3   24   14]
 [   8    1   19    2    9   10  902    1    9    1]
 [   2    1   13   14    3    5    1  946    9   25]
 [   5    9   16   11    5   18    3    5  868    9]
 [   0    0    1    4   42    7    0   23    8  900]]
             precision    recall  f1-score   support

          0       0.97      0.94      0.95      1003
          1       0.98      0.98      0.98      1136
          2       0.91      0.92      0.91      1022
          3       0.91      0.89      0.90      1039
          4       0.92      0.91      0.92       995
          5       0.88      0.88      0.88       890
          6       0.94      0.94      0.94       962
          7       0.92      0.93      0.92      1019
          8       0.89      0.91      0.90       949
          9       0.89      0.91      0.90       985

avg / total       0.92      0.92      0.92     10000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;rsquo;re at 92% accuracy across all digits, not bad! And it looks like we could still improve with more training.&lt;/p&gt;

&lt;p&gt;But let&amp;rsquo;s work on speeding up our optimization alogirthm first. We&amp;rsquo;ll pick things up there in the next post.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>