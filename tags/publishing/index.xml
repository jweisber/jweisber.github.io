<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Publishing on Jonathan Weisberg</title>
    <link>http://jonathanweisberg.org/tags/publishing/index.xml</link>
    <description>Recent content in Publishing on Jonathan Weisberg</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="http://jonathanweisberg.org/tags/publishing/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Visualizing the Philosophy Journal Surveys</title>
      <link>http://jonathanweisberg.org/post/Journal%20Surveys/</link>
      <pubDate>Tue, 22 May 2018 00:00:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/Journal%20Surveys/</guid>
      <description>

&lt;p&gt;In 2009 Andrew Cullison set up an &lt;a href=&#34;https://blog.apaonline.org/journal-surveys/&#34; target=&#34;_blank&#34;&gt;ongoing survey&lt;/a&gt; for philosophers to report their experiences submitting papers to various journals. For me, a junior philosopher working toward tenure at the time, it was a great resource. It was the best guide I knew to my chances of getting a paper accepted at &lt;em&gt;Journal X&lt;/em&gt;, or at least getting rejected quickly by &lt;em&gt;Journal Y&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;But I always wondered about self-selection bias. I figured disgruntled authors were more likely to use the survey to vent. So I wondered whether the data overestimated things like wait times and rejection rates.&lt;/p&gt;

&lt;p&gt;This post is an attempt to better understand the survey data, especially through visualization and comparisons with other sources.&lt;/p&gt;

&lt;h1 id=&#34;timeline&#34;&gt;Timeline&lt;/h1&gt;

&lt;p&gt;The survey has accrued 7,425 responses as of this writing. Of these, 720 have no date recorded. Here&amp;rsquo;s the timeline for the rest:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/journal_surveys/unnamed-chunk-1-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;Two things jump out right away: the spike at the beginning and the dead zone near the end. What gives?&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m guessing the spike reflects records imported manually from another source at the survey&amp;rsquo;s inception. Here I&amp;rsquo;ll mostly assume these records are legitimate, and include them in our analyses. But since the dates attached to those responses are certainly wrong, I&amp;rsquo;ll exclude them when we get to temporal questions (toward the end of the post).&lt;/p&gt;

&lt;p&gt;What about the 2016&amp;ndash;17 dead zone? I tried contacting people involved with the surveys, but nobody seemed to really know for sure what happened there. This dead period is right around when the surveys were &lt;a href=&#34;https://blog.apaonline.org/2017/04/13/journal-surveys-assessing-the-peer-review-process/&#34; target=&#34;_blank&#34;&gt;handed over to the APA&lt;/a&gt;. In that process the data were moved to a different hosting service, apparently with some changes to the survey format. So maybe the records for this period were lost in translation.&lt;/p&gt;

&lt;p&gt;In any case, it looks like the norm is for the survey to get around 50 to 100 responses each month.&lt;/p&gt;

&lt;h1 id=&#34;journals&#34;&gt;Journals&lt;/h1&gt;

&lt;p&gt;There are 155 journals covered by the survey, but most have only a handful of responses. Here are the journals with 50 or more:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/journal_surveys/unnamed-chunk-2-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;How do these numbers compare to the ground truth? Do &lt;em&gt;Phil Studies&lt;/em&gt; and &lt;em&gt;Phil Quarterly&lt;/em&gt; really get the most submissions, for example? And do they really get 4&amp;ndash;5 times as many as, say, &lt;em&gt;BJPS&lt;/em&gt;?&lt;/p&gt;

&lt;p&gt;One way to check is to compare these numbers with those reported by the journals themselves to the APA and BPA in &lt;a href=&#34;http://www.apaonline.org/page/journalsurveys&#34; target=&#34;_blank&#34;&gt;this study&lt;/a&gt; from 2011&amp;ndash;13. &lt;em&gt;Phil Studies&lt;/em&gt; isn&amp;rsquo;t included in that report unfortunately, but &lt;em&gt;Phil Quarterly&lt;/em&gt; and &lt;em&gt;BJPS&lt;/em&gt; are. They reported receiving 2,305 and 1,267 submissions, respectively, during 2011&amp;ndash;13. So &lt;em&gt;Phil Quarterly&lt;/em&gt; does seem to get a lot more submissions, though not 4 times as many.&lt;/p&gt;

&lt;p&gt;For a fuller picture let&amp;rsquo;s do the same comparison for all journals that reported their submission totals to the APA/BPA. That gives us a subset of 33 journals. If we look at the number of survey responses for these journals over the years 2011&amp;ndash;2013, we can get a sense of how large each journal looms in the Journal Survey vs. the APA/BPA report:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/journal_surveys/unnamed-chunk-4-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s a pretty a strong correlation evident here. But it&amp;rsquo;s also clear there&amp;rsquo;s some bias in the survey responses. Bias towards what? I&amp;rsquo;m not exactly sure. Roughly the pattern seems to be that the more submissions a journal receives, the more likely it is to be overrepresented in the survey. But it might instead be a bias towards generalist journals, or journals with fast turn around times. This question would need a more careful analysis, I think.&lt;/p&gt;

&lt;h1 id=&#34;acceptance-rates&#34;&gt;Acceptance Rates&lt;/h1&gt;

&lt;p&gt;What about acceptance rates? Here are the acceptance rates for those journals with 30+ responses in the survey:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/journal_surveys/unnamed-chunk-5-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;These numbers look suspiciously high to me. Most philosophy journals I know have an acceptance rate under 10%. So let&amp;rsquo;s compare with an outside source again.&lt;/p&gt;

&lt;p&gt;The most comprehensive list of acceptance rates I know is &lt;a href=&#34;http://certaindoubts.com/philosophy-journal-information-esf-rankings-citation-impact-rejection-rates/&#34; target=&#34;_blank&#34;&gt;this one&lt;/a&gt; based on data from the ESF. It&amp;rsquo;s not as current as I&amp;rsquo;d like (2011), nor as complete (&lt;em&gt;Phil Imprint&lt;/em&gt; isn&amp;rsquo;t included, perhaps too new at the time). It&amp;rsquo;s also not entirely accurate: it reports an acceptance rate of 8% for &lt;em&gt;Phil Quarterly&lt;/em&gt; vs. 3% reported in the APA/BPA study.&lt;/p&gt;

&lt;p&gt;Still, the ESF values do seem to be largely accurate for many prominent journals I&amp;rsquo;ve checked. For example, they&amp;rsquo;re within 1 or 2% of the numbers reported elsewhere by &lt;em&gt;Ethics&lt;/em&gt;, &lt;em&gt;Mind&lt;/em&gt;, &lt;em&gt;Phil Review&lt;/em&gt;, &lt;em&gt;JPhil&lt;/em&gt;, &lt;em&gt;Nous&lt;/em&gt;, and &lt;em&gt;PPR&lt;/em&gt;.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:Sources-the-APA&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:Sources-the-APA&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; So they&amp;rsquo;re useful for at least a rough validation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/journal_surveys/unnamed-chunk-6-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;Apparently the Journal Surveys do overrepresent accepted submissions. Consistently so in fact: with the exception of &lt;em&gt;Phil Review&lt;/em&gt;, &lt;em&gt;Analysis&lt;/em&gt;, &lt;em&gt;Ancient Philosophy&lt;/em&gt;, and &lt;em&gt;Phil Sci&lt;/em&gt;, the surveys overrepresent accepted submissions for every other journal in this comparison. And in many cases accepted submissions are drastically overrepresented.&lt;/p&gt;

&lt;p&gt;This surprised me, since I figured the surveys would serve as an outlet for disgruntled authors. But maybe it&amp;rsquo;s the other way around: people are more likely to use the surveys as a way to share happy news. (Draw your own conclusions about human nature.)&lt;/p&gt;

&lt;h1 id=&#34;seniority&#34;&gt;Seniority&lt;/h1&gt;

&lt;p&gt;So who uses the journal surveys: grad students? Faculty? The survey records five categories: Graduate Student, Non-TT Faculty, TT-but-not-T Faculty, Tenured Faculty, and Other. A few entries have no professional position recorded.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/journal_surveys/unnamed-chunk-7-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;Evidently, participation drops off with seniority. Also interesting if not too terribly surprising is that seniority affects acceptance:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/journal_surveys/unnamed-chunk-8-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;Compared to grad students, tenured faculty were about 10% more likely to report their papers as having been accepted.&lt;/p&gt;

&lt;h1 id=&#34;gender&#34;&gt;Gender&lt;/h1&gt;

&lt;p&gt;About 79% of respondents specified their gender. Of those, 16.4% were women and 83.6% were men. How does this compare to journal-submitting philosophers in general?&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://jonathanweisberg.org/post/Referee%20Gender/&#34; target=&#34;_blank&#34;&gt;Various other sources&lt;/a&gt; put the percentage of women in academic philosophy roughly in the 15&amp;ndash;25% range. But we&amp;rsquo;re looking for something more specific: what portion of journal submissions come from women vs. men?&lt;/p&gt;

&lt;p&gt;The APA/BPA report gives the percentage of submissions from women at 14 journals. And we can use those figures to infer that 17.6% of submissions to these journals were from women, which matches the 16.4% in the Journal Surveys fairly well.&lt;/p&gt;

&lt;p&gt;Looking at individual journals gives a more mixed picture, however:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/journal_surveys/unnamed-chunk-10-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;While the numbers are reasonably close for some of these journals, they&amp;rsquo;re significantly different for many of them. So, using the Journal Surveys to estimate the gender makeup of a journal&amp;rsquo;s submission pool probably isn&amp;rsquo;t a good idea.&lt;/p&gt;

&lt;p&gt;Does gender affect acceptance? Looking at the data from all journals together, it seems not:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/journal_surveys/unnamed-chunk-11-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;In fact it&amp;rsquo;s striking how stark the non-effect is here, given the quirks we&amp;rsquo;ve already noted in this data set.&lt;/p&gt;

&lt;p&gt;We could break things down further, going journal by journal. But then we&amp;rsquo;d face &lt;a href=&#34;https://imgs.xkcd.com/comics/significant.png&#34; target=&#34;_blank&#34;&gt;the problem of multiple comparisons&lt;/a&gt;, and we&amp;rsquo;ve already seen that the journal-by-journal numbers on gender aren&amp;rsquo;t terribly reliable. So I won&amp;rsquo;t dig into that exercise here.&lt;/p&gt;

&lt;h1 id=&#34;wait-times&#34;&gt;Wait Times&lt;/h1&gt;

&lt;p&gt;For me, the surveys were always most interesting as a means to compare wait times across journals. But how reliable are these comparisons?&lt;/p&gt;

&lt;p&gt;The APA/BPA report gives the average wait times at 38 journals. It also reports how many decisions were delivered within 2 months, in 2&amp;ndash;6 months, in 7&amp;ndash;11 months, and after 12+ months.&lt;/p&gt;

&lt;p&gt;Trouble is, a lot of these numbers look dodgy. The average wait times are all whole numbers of months&amp;mdash;except inexplicaby for one journal, &lt;em&gt;Ratio&lt;/em&gt;. I guess someone at the APA/BPA has a sense of humour.&lt;/p&gt;

&lt;p&gt;The other wait time figures are also suspiciously round. For example, &lt;em&gt;APQ&lt;/em&gt; is listed as returning 60% of its decisions within 2 months, 35% after 2&amp;ndash;6 months, and the remaining 5% after 7&amp;ndash;11 months. Round percentages like these are the norm. So, at best, most of these numbers are rounded estimates. At worst, they don&amp;rsquo;t always reflect an actual count, but rather the editor&amp;rsquo;s perception of their own performance.&lt;/p&gt;

&lt;p&gt;On top of all that, there are differences between &lt;a href=&#34;https://apaonline.site-ym.com/resource/resmgr/journal_surveys_2014/apa_bpa_survey_data_2014.xlsx&#34; target=&#34;_blank&#34;&gt;the downloadable Excel spreadsheet&lt;/a&gt; and &lt;a href=&#34;https://apaonline.site-ym.com/general/custom.asp?page=journalsurveys&#34; target=&#34;_blank&#34;&gt;the APA&amp;rsquo;s webpages&lt;/a&gt; reporting (supposedly) the same data. For example, the spreadsheet gives an average wait time of 6 months for &lt;em&gt;Phil Imprint&lt;/em&gt; (certainly wrong), while the webpage says &amp;ldquo;not available&amp;rdquo;. In fact the Excel spreadsheet flatly contradicts itself here: it says &lt;em&gt;Phil Imprint&lt;/em&gt; returns 73% of its decisions within 2 months, the rest in 2&amp;ndash;6 months.&lt;/p&gt;

&lt;p&gt;I don&amp;rsquo;t know any other comprehensive list of wait times, though, so we&amp;rsquo;ll have to make do. Here I&amp;rsquo;ll restrict the comparison to journals with 30+ responses in the 2011&amp;ndash;2013 timeframe, and exclude &lt;em&gt;Phil Imprint&lt;/em&gt; because of the inconsistencies just mentioned.&lt;/p&gt;

&lt;p&gt;That leaves us with 11 journals on which to compare average wait times:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/journal_surveys/unnamed-chunk-15-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;The results are pretty stark. The match is close for most of these journals. In fact, if we&amp;rsquo;re forgiving about the rounding, only three journals have a discrepancy that&amp;rsquo;s clearly more than 1 month: &lt;em&gt;Erkenntnis&lt;/em&gt;,  &lt;em&gt;Mind&lt;/em&gt;, and &lt;em&gt;Synthese&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Notably, these are the three journals with the longest wait times according to survey respondents. I&amp;rsquo;d add that the reported 2 month average for &lt;em&gt;Mind&lt;/em&gt; is wildly implausible by reputation. I can&amp;rsquo;t comment on the discrepancies for &lt;em&gt;Erkenntnis&lt;/em&gt; and &lt;em&gt;Synthese&lt;/em&gt;, though, since I know much less about their reputations for turnaround.&lt;/p&gt;

&lt;p&gt;I do want to flag that &lt;em&gt;Mind&lt;/em&gt; has radically improved its review times recently, as we&amp;rsquo;ll soon see. But for the present purpose&amp;mdash;validating the Journal Survey data&amp;mdash;we&amp;rsquo;re confined to look at 2011&amp;ndash;13. And the survey responses align much better with &lt;em&gt;Mind&lt;/em&gt;&amp;rsquo;s reputation during that time period than the 2 month average listed in the APA/BPA report.&lt;/p&gt;

&lt;p&gt;In any case, since the wait time data looks to be carrying a fair amount of signal, let&amp;rsquo;s conclude our analysis with some visualizations of it.&lt;/p&gt;

&lt;h1 id=&#34;visualizing-wait-times&#34;&gt;Visualizing Wait Times&lt;/h1&gt;

&lt;p&gt;A journal&amp;rsquo;s average wait time doesn&amp;rsquo;t tell the whole story, of course. Two journals might have the same average wait time even though one of them is much more consistent and predictable. Or, a journal with a high desk-rejection rate might have a low average wait time, but still take a long time with its few externally reviewed submissions. So it&amp;rsquo;s helpful to see the whole picture.&lt;/p&gt;

&lt;p&gt;One way to see the whole picture is with a scatterplot. This also let&amp;rsquo;s us see how a journal&amp;rsquo;s wait times have changed. To make this feasible, I&amp;rsquo;ll focus on two groups of journals I expect to be of broad interest.&lt;/p&gt;

&lt;p&gt;The first is a list of 18 &amp;ldquo;general&amp;rdquo; journals that were highly rated in &lt;a href=&#34;http://leiterreports.typepad.com/blog/2015/09/the-top-20-general-philosophy-journals-2015.html&#34; target=&#34;_blank&#34;&gt;a pair of polls&lt;/a&gt; at Leiter Reports.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:The-poll-results&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:The-poll-results&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; For the sake of visibility, I&amp;rsquo;ll cap these scatterplots at 24 months. The handful of entries with longer wait times are squashed down to 24 so they can still inform the plot.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/journal_surveys/unnamed-chunk-16-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;In addition to the improvements at &lt;em&gt;Mind&lt;/em&gt; mentioned earlier, &lt;em&gt;Phil Review&lt;/em&gt;, &lt;em&gt;PPQ&lt;/em&gt;, &lt;em&gt;CJP&lt;/em&gt;, and &lt;em&gt;Erkenntnis&lt;/em&gt; all seem to be shortening their wait times. &lt;em&gt;APQ&lt;/em&gt; and &lt;em&gt;EJP&lt;/em&gt; on the other hand appear to be drifting upward.&lt;/p&gt;

&lt;p&gt;Keeping that in mind, let&amp;rsquo;s visualize expected wait times at these journals with a ridgeplot. The plot shows a smoothed estimate of the probable wait times for each journal. Note that here I&amp;rsquo;ve truncated the timeline at 12 months, squashing all wait times longer than 12 months down to 12.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/journal_surveys/unnamed-chunk-17-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;Remember though, the ridgeplot reflects old data as much as new. Authors submitting to journals like &lt;em&gt;Mind&lt;/em&gt; and &lt;em&gt;CJP&lt;/em&gt;, where wait times have significantly improved recently, should definitely not just set their expectations according to this plot. Consult the scatterplot!&lt;/p&gt;

&lt;p&gt;Our second group consists of 8 &amp;ldquo;specialty&amp;rdquo; journals drawn from &lt;a href=&#34;http://leiterreports.typepad.com/blog/2013/07/top-philosophy-journals-without-regard-to-area.html&#34; target=&#34;_blank&#34;&gt;another poll&lt;/a&gt; at Leiter Reports. Here I&amp;rsquo;ll cap the scale at 15 months for the sake of visibility:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/journal_surveys/unnamed-chunk-18-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;And for the ridgeplot we&amp;rsquo;ll return to a cap of 12 months:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/journal_surveys/unnamed-chunk-19-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;Again, remember that the ridgeplot reflects out-of-date information for some journals. Consult the scatterplot! And please direct others to do the same if you share any of this on social media.&lt;/p&gt;

&lt;h1 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/journal_surveys/inigo.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A journal&amp;rsquo;s prominence in the survey is a decent &lt;em&gt;comparative&lt;/em&gt; guide to the quantity of submissions it receives.&lt;/li&gt;
&lt;li&gt;Accepted submissions are overrepresented in the survey. Acceptance rates estimated from the survey will pretty consistently overestimate the true rate&amp;mdash;in many cases by a lot.&lt;/li&gt;
&lt;li&gt;Grad students and non-tenured faculty use the surveys a lot more than tenured faculty.&lt;/li&gt;
&lt;li&gt;Acceptance rates increase with seniority.&lt;/li&gt;
&lt;li&gt;Men and women seem to be represented about the same as in the population of journal-submitting philosophers more generally.&lt;/li&gt;
&lt;li&gt;Gender doesn&amp;rsquo;t seem to affect acceptance rate.&lt;/li&gt;
&lt;li&gt;The Survey seems to be a reasonably good guide to expected wait times, though there may be some anomalies (e.g. &lt;em&gt;Synthese&lt;/em&gt; and &lt;em&gt;Erkenntnis&lt;/em&gt;).&lt;/li&gt;
&lt;li&gt;Some journals&amp;rsquo; wait times have been improving significantly, such as &lt;em&gt;CJP&lt;/em&gt;, &lt;em&gt;Erkenntnis&lt;/em&gt;, &lt;em&gt;Mind&lt;/em&gt;, &lt;em&gt;PPQ&lt;/em&gt;, and &lt;em&gt;Phil Review&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:Sources-the-APA&#34;&gt;Sources: the APA/BPA study, &lt;a href=&#34;http://dailynous.com/2015/01/20/closer-look-philosophy-journal-practices/&#34; target=&#34;_blank&#34;&gt;Daily Nous&lt;/a&gt;, and the websites for &lt;a href=&#34;https://philreview.gorgesapps.us/statistics&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;Phil Review&lt;/em&gt;&lt;/a&gt; and &lt;a href=&#34;https://www.journals.uchicago.edu/pb-assets/docs/journals/ethics-editorial-final-2018-02-07.pdf&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;Ethics&lt;/em&gt;&lt;/a&gt;. One notable exception is &lt;em&gt;CJP&lt;/em&gt;, which reported 17% to the APA/BPA but 6% on Daily Nous. The ESF gives 10%. &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:Sources-the-APA&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:The-poll-results&#34;&gt;The poll results identified 20 journals ranked &amp;ldquo;best&amp;rdquo; by respondents. So why does our list only have 18? Because 3 of those 20 aren&amp;rsquo;t covered in the survey data, and I&amp;rsquo;ve included the &amp;ldquo;runner up&amp;rdquo; journal ranked 21st. &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:The-poll-results&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Where Are They Now? The Healy 2100</title>
      <link>http://jonathanweisberg.org/post/Where%20Are%20They%20Now%20The%20Healy%202100/</link>
      <pubDate>Mon, 23 Apr 2018 00:00:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/Where%20Are%20They%20Now%20The%20Healy%202100/</guid>
      <description>

&lt;p&gt;A &lt;a href=&#34;https://www.timeshighereducation.com/news/how-much-research-goes-completely-uncited&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;Times Higher Education&lt;/em&gt;
piece&lt;/a&gt;
making the rounds last week found that most published philosophy papers
are never cited. More exactly, of the studied philosophy papers
published in 2012, more than half had no citations indexed in &lt;a href=&#34;https://clarivate.com/products/web-of-science/&#34; target=&#34;_blank&#34;&gt;Web of
Science&lt;/a&gt; five years
later.&lt;/p&gt;

&lt;p&gt;At Daily Nous, the &lt;a href=&#34;http://dailynous.com/2018/04/19/philosophy-high-rate-uncited-publications/&#34; target=&#34;_blank&#34;&gt;discussion of that
finding&lt;/a&gt;
turned up some interesting follow-up questions and findings. In
particular, Brian Weatherson found &lt;a href=&#34;http://dailynous.com/2018/04/19/philosophy-high-rate-uncited-publications/#comment-141535&#34; target=&#34;_blank&#34;&gt;quite different
figures&lt;/a&gt;
for papers published in &lt;em&gt;prestigious&lt;/em&gt; philosophy journals. In the journals
he looked at, 89% of the papers published in 2012 had at least one
citation in Web of Science five years later.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; And more than half had five
or more citations.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s a pretty striking difference: &amp;gt;50% vs. ~11%! Seems like where
you publish your paper makes a &lt;em&gt;big&lt;/em&gt; difference to your chances of going
uncited.&lt;/p&gt;

&lt;p&gt;Shocking, I know.&lt;/p&gt;

&lt;p&gt;But this got me thinking about &lt;a href=&#34;https://kieranhealy.org/blog/archives/2015/02/25/gender-and-citation-in-four-general-interest-philosophy-journals-1993-2013/&#34; target=&#34;_blank&#34;&gt;Kieran Healy&amp;rsquo;s
analysis&lt;/a&gt;
from a few years back. He found an &amp;ldquo;uncitation&amp;rdquo; rate higher than
Weatherson&amp;rsquo;s 11%&amp;mdash;almost 20%&amp;mdash;even though he was looking at just four of
philosophy&amp;rsquo;s most prominent journals: &lt;em&gt;Journal of Philosophy&lt;/em&gt;, &lt;em&gt;Mind&lt;/em&gt;,
&lt;em&gt;Noûs&lt;/em&gt;, and &lt;em&gt;Philosophical Review&lt;/em&gt;. (He found that around half of the
papers in these journals had five citations or fewer.)&lt;/p&gt;

&lt;p&gt;So I wondered: what&amp;rsquo;s with the discrepancy? Do these journals not
necessarily get the most citations? Or is it that Healy was looking at
papers from 1993 to 2013, and things changed somehow over those two
decades, so that papers published in 2012 tend to get discussed more
than papers from 1993. Or is it just a symptom of when Healy collected
his data? Papers published in, say, 2011 wouldn&amp;rsquo;t have had much time to
gather citations by 2013 when Healy (apparently?) gathered his data.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s take a look.&lt;/p&gt;

&lt;h1 id=&#34;the-healy-2100&#34;&gt;The Healy 2100&lt;/h1&gt;

&lt;p&gt;Since I don&amp;rsquo;t have Healy&amp;rsquo;s raw data, I went to Web of Science and
grabbed their data for all papers published over 1993&amp;ndash;2013 in the
&amp;ldquo;Healy 4&amp;rdquo; journals. I ended up with a couple hundred more papers than
Healy looked at&amp;mdash;not sure why. But the list of 2,100 papers he studied
is &lt;a href=&#34;https://github.com/kjhealy/philpub&#34; target=&#34;_blank&#34;&gt;available on GitHub&lt;/a&gt;. So I
focused on just those to get more of an apples-to-apples comparison.&lt;/p&gt;

&lt;p&gt;Then I tried to reproduce his original findings, especially &lt;a href=&#34;https://kieranhealy.org/blog/archives/2015/02/25/gender-and-citation-in-four-general-interest-philosophy-journals-1993-2013/#the-matthew-effect-is-a-harsh-mistress&#34; target=&#34;_blank&#34;&gt;this
histogram&lt;/a&gt;. Here&amp;rsquo;s my version:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/where_are_they_now_the_healy_2100/unnamed-chunk-2-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I got pretty close, but I didn&amp;rsquo;t manage to reproduce his results
exactly. I found about 19.9% of the papers had no citations by the end
of 2013, compared to Healy&amp;rsquo;s ~18.5%. And I found about 58.6% with five
or fewer citations, compared with Healy&amp;rsquo;s &amp;ldquo;just over half&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Still, the match is pretty close, so let&amp;rsquo;s go on to see how these papers
have aged since 2013.&lt;/p&gt;

&lt;h1 id=&#34;where-are-they-now&#34;&gt;Where Are They Now?&lt;/h1&gt;

&lt;p&gt;If we include citations up to the present day, only about 9.7% of these
2,100 papers have no citations, and about 39.1% have five or fewer.
Here&amp;rsquo;s the updated histogram:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/where_are_they_now_the_healy_2100/unnamed-chunk-4-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So it looks like the discrepancy with Weatherson&amp;rsquo;s result is (partly?)
down to the obvious thing. There hadn&amp;rsquo;t been enough time for the later
papers in Healy&amp;rsquo;s data set to accrue citations.&lt;/p&gt;

&lt;h1 id=&#34;cutting-out-supplements&#34;&gt;Cutting Out Supplements&lt;/h1&gt;

&lt;p&gt;A lot of these 2,100 papers are actually from the two supplements to
&lt;em&gt;Noûs&lt;/em&gt;: &lt;em&gt;Philosophical Issues&lt;/em&gt; and &lt;em&gt;Philosophical Perspectives&lt;/em&gt;. And it
turns out they&amp;rsquo;re making a big difference.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s what things look like when we cut supplements out:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/where_are_they_now_the_healy_2100/unnamed-chunk-6-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now only 3.8% of our 1,677 papers have no citations to date, and just
30.9% have five or fewer.&lt;/p&gt;

&lt;h1 id=&#34;sliding-windows&#34;&gt;Sliding Windows&lt;/h1&gt;

&lt;p&gt;We&amp;rsquo;ve been looking at all citations accumulated to date for these
papers, which for older papers means 25 years&amp;rsquo; worth of opportunity for
discussion. For more direct comparison to the &lt;em&gt;THE&lt;/em&gt; analysis mentioned
at the outset, we can look at just the five-year window following each
paper&amp;rsquo;s publication.&lt;/p&gt;

&lt;p&gt;So, how many citations did these papers accrue just within five years of
being published? Looking at only the &amp;ldquo;core&amp;rdquo; papers again (no
supplements), 14.4% had no citations within five years of publication,
and 69.6% had five or fewer.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s still a bit higher than the 11% (respectively 50%) found by
Weatherson. So we have to ask: are things changing? Are recent papers in
these journals accruing citations faster?&lt;/p&gt;

&lt;p&gt;It seems so. Here are the &amp;ldquo;uncitation&amp;rdquo; rates for the years 1993&amp;ndash;2013:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/where_are_they_now_the_healy_2100/unnamed-chunk-8-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;And here are the &amp;ldquo;five or fewer citations&amp;rdquo; rates:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/where_are_they_now_the_healy_2100/unnamed-chunk-9-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: Eric Schliesser pointed out to me what should have been obvious here, namely the internet. When Google took over the search engine industry around the year 2000, there was a citation boom across the board. So you&amp;rsquo;d naturally expect 2012 to be a very different year than 1993 as far as uncitation goes.&lt;/p&gt;

&lt;p&gt;At first I thought the data plainly vindicated the &amp;ldquo;Google effect&amp;rdquo; hypothesis, and I had some plots up here to show as much. But it turned out Web of Science had snuck a bunch of supplemental &lt;em&gt;Phil Issues&lt;/em&gt;/&lt;em&gt;Phil Perspectives&lt;/em&gt; papers past me unmarked!&lt;/p&gt;

&lt;p&gt;With those removed, the Google effect isn&amp;rsquo;t looking so big. Here&amp;rsquo;s the long view, from 1970 to 2013:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/where_are_they_now_the_healy_2100/unnamed-chunk-10-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Apparently papers with zero citations after five years have been declining for a while now. There does still seem to be a significant Google effect in the &amp;ldquo;five or fewer&amp;rdquo; measure:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/where_are_they_now_the_healy_2100/unnamed-chunk-11-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;But it still looks like there were changes before Google. Does this reflect a growing profession? A faster-moving profession? An increasing number of publications? Longer bibliographies? Just idiosyncracies of Web of Science&amp;rsquo;s indexing? Something else? I&amp;rsquo;m not sure.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Weatherson looked at 391 articles published in 2012 in &lt;em&gt;Philosophical Review&lt;/em&gt;, &lt;em&gt;Mind&lt;/em&gt;, &lt;em&gt;Journal of Philosophy&lt;/em&gt;, &lt;em&gt;Nous&lt;/em&gt;, &lt;em&gt;Philosophical Studies&lt;/em&gt;, &lt;em&gt;Ethics&lt;/em&gt;, &lt;em&gt;Philosophical Quarterly&lt;/em&gt;, &lt;em&gt;Philosophy of Science&lt;/em&gt;, and &lt;em&gt;Australasian Journal of Philosophy&lt;/em&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Journal Submission Rates by Gender: A Look at the APA/BPA Data</title>
      <link>http://jonathanweisberg.org/post/A%20Look%20at%20the%20APA-BPA%20Data/</link>
      <pubDate>Tue, 06 Jun 2017 11:45:04 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/A%20Look%20at%20the%20APA-BPA%20Data/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; &lt;em&gt;editors at CJP and Phil Quarterly have kindly shared some important, additional information. See the edit below for details.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A &lt;a href=&#34;https://link.springer.com/article/10.1007/s11098-017-0919-0&#34; target=&#34;_blank&#34;&gt;new paper&lt;/a&gt; on the representation of women in philosophy journals prompted some debate in the philosophy blogosphere last week. The paper found women to be underrepresented across a range of prominent journals, yet overrepresented in the two journals studied where review was non-anonymous.&lt;/p&gt;

&lt;p&gt;Commenters &lt;a href=&#34;http://dailynous.com/2017/05/26/women-philosophy-journals-new-data/&#34; target=&#34;_blank&#34;&gt;over at Daily Nous&lt;/a&gt; complained about the lack of base-rate data. How many of the submissions to these journals were from women? In some respects, it&amp;rsquo;s hard to know what to make of these findings without such data.&lt;/p&gt;

&lt;p&gt;A few commenters linked to &lt;a href=&#34;http://www.apaonline.org/resource/resmgr/journal_surveys_2014/apa_bpa_survey_data_2014.xlsx&#34; target=&#34;_blank&#34;&gt;a survey&lt;/a&gt; conducted by the APA and BPA a while back, which supplies some numbers along these lines. I was surprised, because I&amp;rsquo;ve wondered about these numbers, but I didn&amp;rsquo;t recall seeing this data-set before. I was excited too because the data-set is huge, in a way: it covers more than 30,000 submissions at 40+ journals over a span of three years!&lt;/p&gt;

&lt;p&gt;So I was keen to give it a closer look. This post walks through that process. But I should warn you up front that the result is kinda disappointing.&lt;/p&gt;

&lt;h1 id=&#34;initial-reservations&#34;&gt;Initial Reservations&lt;/h1&gt;

&lt;p&gt;Right away some conspicuous omissions stand out.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; A good number of the usual suspects aren&amp;rsquo;t included, like &lt;em&gt;Philosophical Studies&lt;/em&gt;, &lt;em&gt;Analysis&lt;/em&gt;, and &lt;em&gt;Australasian Journal of Philosophy&lt;/em&gt;. So the usual worries about response rates and selection bias apply.&lt;/p&gt;

&lt;p&gt;The data are also a bit haphazard and incomplete. Fewer than half of the journals that responded included gender data. And some of those numbers are suspiciously round.&lt;/p&gt;

&lt;p&gt;Still, there&amp;rsquo;s hope. We have data on over ten thousand submissions even after we exclude journals that didn&amp;rsquo;t submit any gender data. As long as they paint a reasonably consistent picture, we stand to learn a lot.&lt;/p&gt;

&lt;h1 id=&#34;first-pass&#34;&gt;First Pass&lt;/h1&gt;

&lt;p&gt;For starters we&amp;rsquo;ll just do some minimal cleaning. We&amp;rsquo;ll exclude data from 2014, since almost no journals supplied it. And we&amp;rsquo;ll lump together the submissions from the remaining three years, 2011&amp;ndash;13, since the gender data isn&amp;rsquo;t broken down by year.&lt;/p&gt;

&lt;p&gt;We can then calculate the following cross-journal tallies for 2011&amp;ndash;13:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Accepted submissions&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Rejected submissions&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Men&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;792&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9104&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Women&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;213&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1893&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The difference here looks notable at first: 17.5% of submitted papers came from women compared with  21.2% of accepted papers, a statistically significant difference (&lt;em&gt;p&lt;/em&gt; = 0.002).&lt;/p&gt;

&lt;p&gt;But if we plot the data by journal, the picture becomes much less clear:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/apa_bpa_data_files/unnamed-chunk-3-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;The dashed line&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; indicates parity: where submission and acceptance rate would be equal. At journals above the line, women make up a larger portion of published authors than they do submitting authors. At journals below the line, it&amp;rsquo;s the reverse.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s pretty striking how much variation there is between journals. For example, &lt;em&gt;BJPS&lt;/em&gt; is 12 points above the parity line while &lt;em&gt;Phil Quarterly&lt;/em&gt; is 9 points below it.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s also notable that it&amp;rsquo;s the largest journals which diverge the most from parity: &lt;em&gt;BJPS&lt;/em&gt;, &lt;em&gt;EJP&lt;/em&gt;, &lt;em&gt;MIND&lt;/em&gt;, and &lt;em&gt;Phil Quarterly&lt;/em&gt;. (Note: &lt;em&gt;Hume Studies&lt;/em&gt; is actually the most extreme by far. But I&amp;rsquo;ve excluded it from the plot because it&amp;rsquo;s very small, and as an extreme outlier it badly skews the &lt;em&gt;y&lt;/em&gt;-axis.)&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s hard to see all the details in the plot, so here&amp;rsquo;s the same data in a table.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;Journal&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;submissions&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;accepted&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;% submissions women&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;% accepted women&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Ancient Philosophy&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;346&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;63&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;24&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;British Journal for the Philosophy of Science&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1267&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;117&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;27&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Canadian Journal of Philosophy&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;792&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;132&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;21&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Dialectica&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;826&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;74&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;12.05&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;15.48&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;European Journal for Philosophy&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1554&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;98&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;11.84&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;25&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Hume Studies&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;152&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;30&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;23.7&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;58.1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Journal of Applied Philosophy&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;510&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;47&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;20&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Journal of Political Philosophy&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1143&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;53&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;35&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;30&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;MIND&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1498&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;74&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Oxford Studies in Ancient Philosophy&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;290&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;43&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;21&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;20.3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Philosophy East and West&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;320&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;66&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;15&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Phronesis&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;388&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;38&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;24&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;25&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;The Journal of Aesthetics and Art Criticism&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;611&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;93&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;29&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;27&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;The Philosophical Quarterly&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2305&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;77&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;5&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&#34;rounders-removed&#34;&gt;Rounders Removed&lt;/h1&gt;

&lt;p&gt;I mentioned that some of the numbers look suspiciously round. Maybe 10% of submissions to &lt;em&gt;MIND&lt;/em&gt; really were from women, compared with 5% of accepted papers. But some of these cases probably involve non-trivial rounding, maybe even eyeballing or guesstimating. So let&amp;rsquo;s see how things look without them.&lt;/p&gt;

&lt;p&gt;If we omit journals where both percentages are round (integer multiples of 5), that leaves ten journals. And the gap from before is even more pronounced: 16.3% of submissions from women compared with  22.9% of accepted papers (&lt;em&gt;p&lt;/em&gt; = 0.0000003).&lt;/p&gt;

&lt;p&gt;But it&amp;rsquo;s still a few, high-volume journals driving the result: &lt;em&gt;BJPS&lt;/em&gt; and &lt;em&gt;EJP&lt;/em&gt; do a ton of business, and each has a large gap. So much so that they&amp;rsquo;re able to overcome the opposite contribution of &lt;em&gt;Phil Quarterly&lt;/em&gt; (which does a mind-boggling amount of business!).&lt;/p&gt;

&lt;h1 id=&#34;editors-anonymous&#34;&gt;Editors Anonymous&lt;/h1&gt;

&lt;p&gt;Naturally I fell to wondering how these big journals differ in their editorial practices. What are they doing differently that leads to such divergent results?&lt;/p&gt;

&lt;p&gt;One thing the data tell us is which journals practice fully anonymous review, with even the editors ignorant of the author&amp;rsquo;s identity. That narrows it down to just three journals: &lt;em&gt;CJP&lt;/em&gt;, &lt;em&gt;Dialectica&lt;/em&gt;, and &lt;em&gt;Phil Quarterly&lt;/em&gt;.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; The tallies then are:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Accepted submissions&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Rejected submissions&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Men&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;240&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3103&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Women&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;43&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;537&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;And now the gap is gone: 14.8% of submissions from women, compared with 15.2% of accepted papers&amp;mdash;not a statistically significant difference (&lt;em&gt;p&lt;/em&gt; = 0.91). That makes it look like the gap is down to editors&amp;rsquo; decisions being influenced by knowledge of the author&amp;rsquo;s gender (whether deliberately or unconsciously).&lt;/p&gt;

&lt;p&gt;But notice again, &lt;em&gt;Phil Quarterly&lt;/em&gt; is still a huge part of this story. It&amp;rsquo;s their high volume and unusually negative differential that compensates for the more modest, positive differentials at &lt;em&gt;CJP&lt;/em&gt; and &lt;em&gt;Dialectica&lt;/em&gt;. So I still want to know more about &lt;em&gt;Phil Quarterly&lt;/em&gt;, and what might explain their unusually negative differential.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: editors at &lt;em&gt;CJP&lt;/em&gt; and &lt;em&gt;Phil Quarterly&lt;/em&gt; kindly wrote with the following, additional information.&lt;/p&gt;

&lt;p&gt;At &lt;em&gt;CJP&lt;/em&gt;, the author&amp;rsquo;s identity is withheld from the editors while they decide whether to send the paper for external review, but then their identity is revealed (presumably to avoid inviting referees who are unacceptably close to the author&amp;mdash;e.g. those identical to the author).&lt;/p&gt;

&lt;p&gt;And chairman of &lt;em&gt;Phil Quarterly&lt;/em&gt;&amp;rsquo;s editorial board, Jessica Brown, writes:&lt;/p&gt;

&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;the PQ is very aware of issues about the representation of women, unsurprisingly given that the editorial board consists of myself, Sarah Broadie and Sophie-Grace Chappell. We monitor data on submissions by women and papers accepted in the journal every year.&lt;/li&gt;
&lt;li&gt;the PQ has for many years had fully anonymised processing including the point at which decisions on papers are made (i.e. accept, reject, R and R etc). So, when we make such decisions we have no idea of the identity of the author.&lt;/li&gt;

&lt;li&gt;&lt;p&gt;While in some years the data has concerned us, more recently the figures do look better which is encouraging:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;16-17: 25% declared female authored papers accepted; 16% submissions&lt;/li&gt;
&lt;li&gt;15-16: 14% accepted; 15% submissions&lt;/li&gt;
&lt;li&gt;14-15: 16% accepted; 16% submissions&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;a-gruesome-conclusion&#34;&gt;A Gruesome Conclusion&lt;/h1&gt;

&lt;p&gt;In the end, I don&amp;rsquo;t see a clear lesson here. Before drawing any conclusions from the aggregated, cross-journal tallies, it seems we&amp;rsquo;d need to know more about the policies and practices of the journals driving them. Otherwise we&amp;rsquo;re liable to be misled to a false generalization about a heterogeneous group.&lt;/p&gt;

&lt;p&gt;Some of that policy-and-practice information is probably publicly available; I haven&amp;rsquo;t had a chance to look. And I bet a lot of it is available informally, if you just talk to the right people. So this data-set could still be informative on our base-rate question. But sadly, I don&amp;rsquo;t think I&amp;rsquo;m currently in a position to make informative use of it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://i.imgur.com/ojvPBaY.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;technical-note&#34;&gt;Technical Note&lt;/h1&gt;

&lt;p&gt;This post was written in R Markdown and the source is &lt;a href=&#34;https://github.com/jweisber/rgo/blob/master/apa bpa data/apa_bpa_data.Rmd&#34; target=&#34;_blank&#34;&gt;available on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;No, I don&amp;rsquo;t mean &lt;em&gt;Ergo&lt;/em&gt;! We published our first issue in 2014 while the survey covers mainly 2011&amp;ndash;13.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;&lt;strong&gt;Edit&lt;/strong&gt;: the parity line was solid blue originally. But that misled some people into reading it as a fitted line. For reference and posterity, &lt;a href=&#34;http://jonathanweisberg.org/img/apa_bpa_data_files/unnamed-chunk-3-2.png&#34;&gt;the original image is here&lt;/a&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;That&amp;rsquo;s if we continue to exclude journals with very round numbers. Adding these journals back in doesn&amp;rsquo;t change the following result, though.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Journals as Ratings Agencies</title>
      <link>http://jonathanweisberg.org/post/Journals%20as%20Ratings%20Agencies/</link>
      <pubDate>Thu, 30 Mar 2017 15:27:04 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/Journals%20as%20Ratings%20Agencies/</guid>
      <description>

&lt;p&gt;Starting in July, philosophy&amp;rsquo;s two most prestigious journals won&amp;rsquo;t reject submitted papers anymore. Instead they&amp;rsquo;ll &amp;ldquo;grade&amp;rdquo; every submission, assigning a rating on the familiar letter-grade scale (A+, A, A-, B+, B, B-, etc.).&lt;/p&gt;

&lt;p&gt;They will, in effect, become ratings agencies.&lt;/p&gt;

&lt;p&gt;They&amp;rsquo;ll still publish papers. Those rated A- or higher can be published in the journal, if the authors want. Or they can seek another venue, if they think they can do better.&lt;/p&gt;

&lt;p&gt;I just made that up. But imagine if it were true&amp;mdash;especially if a bunch of journals did this. How would it change philosophy&amp;rsquo;s publication game?&lt;/p&gt;

&lt;p&gt;Well we&amp;rsquo;d save a lot of wasted labour, for one thing. And we&amp;rsquo;d discourage frivolous submissions, for another.&lt;/p&gt;

&lt;h1 id=&#34;the-bad&#34;&gt;The Bad&lt;/h1&gt;

&lt;p&gt;Under the current arrangement, the system is sagging low under the weight of premature, mediocre, even low-quality submissions. (I&amp;rsquo;d say it&amp;rsquo;s even creaking and cracking.) Editors scrounge miserably for referees, and referees frantically churn out reports and recommendations, mostly for naught.&lt;/p&gt;

&lt;p&gt;In a typical case, the editor rejects the submission and the referees&amp;rsquo; reports are filed away in a database, never to be read again. Maybe the author makes substantial revisions, but very likely they don&amp;rsquo;t&amp;mdash;especially if the paper&amp;rsquo;s main idea is the real limiting factor. The process repeats at another journal, often at several more journals. And in the end all the philosophical public sees is: accepted at &lt;em&gt;International Journal of Such &amp;amp; Such Studies&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Of all the people who&amp;rsquo;ve read and assessed the paper by that point, only two have their assessments directly broadcast to the public. And even then, only the &amp;ldquo;two thumbs more-or-less up&amp;rdquo; part of the signal gets out.&lt;/p&gt;

&lt;p&gt;Yet five, eight, or even ten people have weighed in on the paper by then. They&amp;rsquo;ve thought about its strengths and weaknesses, and they&amp;rsquo;ve generated valuable insights and assessments that could save others time and trouble. Yet only the handling editors and the authors get the direct benefit of that labour.&lt;/p&gt;

&lt;p&gt;The current system even encourages authors to waste editors&amp;rsquo; and referees&amp;rsquo; time. Unless they&amp;rsquo;re in a rush, authors can start at the top of the journal-prestige hierarchy and work their way down. You don&amp;rsquo;t even have to perfect your paper before starting this incredibly inefficient process. With so many journals to try, you&amp;rsquo;ll basically get unlimited kicks at the can. So you might as well let the referees do your homework for you.&lt;/p&gt;

&lt;p&gt;(This doesn&amp;rsquo;t apply to all authors, obviously. Some work in areas that severely limit their can-kicking. And many &lt;em&gt;are&lt;/em&gt; in a rush, to get jobs and tenure.)&lt;/p&gt;

&lt;h1 id=&#34;the-good&#34;&gt;The Good&lt;/h1&gt;

&lt;p&gt;But, if a paper were publicly assigned a grade every place it was submitted, authors might be more realistic in deciding where to submit. They might also wait until their paper is truly ready for public consumption before imposing on editors and referees.&lt;/p&gt;

&lt;p&gt;Readers would also benefit from seeing a paper&amp;rsquo;s transcript. Not only could it inform their decision about whether to read the paper, it could aid their sense of how its contribution is received by peers and experts.&lt;/p&gt;

&lt;p&gt;Referees would also have better incentives, to take on referee work and to be more diligent about it. They would know that their labour would have a greater impact, and that their assessment would have a more lasting effect.&lt;/p&gt;

&lt;p&gt;Editors could even limit submissions based on their grade-history, e.g. &amp;ldquo;no submissions already graded  by two other journals&amp;rdquo;, or &amp;ldquo;no submissions with an average grade less than a B&amp;rdquo;. (Ideally, different journals would have different policies here, to allow some variety.)&lt;/p&gt;

&lt;h1 id=&#34;the-ugly&#34;&gt;The Ugly&lt;/h1&gt;

&lt;p&gt;Of course, several high-profile journals would have to take the lead to make this kind of thing happen. And there would have to be strong norms within the discipline about publicizing grades: requiring they be listed alongside the paper on CVs and websites, for example&lt;/p&gt;

&lt;p&gt;And there would be costs.&lt;/p&gt;

&lt;p&gt;Everybody has their favourite story about the groundbreaking paper that got rejected five times, but was finally published in &lt;em&gt;The Posh Journal of Philosophy Review&lt;/em&gt;, and has since been cited a gajillion times. Such papers could be weighed down by having their grade-transcripts publicized. (On the plus side, we could have a new genre of great paper: the cult classic!)&lt;/p&gt;

&lt;p&gt;Also, some authors have to rely on referee feedback more than others, because of their limited philosophical networks. They&amp;rsquo;d likely find their papers with longer, more checkered grade-transcripts, exacerbating an existing injustice.&lt;/p&gt;

&lt;p&gt;And, in the end, the present proposal might only be a band-aid. If there really is an oversubmission problem in academic philosophy (as I suspect there is), it&amp;rsquo;s probably caused by increased pressure to publish&amp;mdash;because jobs are scarce, and administrators demand it, for example. Turning journals into ratings agencies wouldn&amp;rsquo;t relieve that pressure, even if it would help to manage some of its bad effects.&lt;/p&gt;

&lt;h1 id=&#34;decision-r-r&#34;&gt;Decision: R&amp;amp;R&lt;/h1&gt;

&lt;p&gt;In the end, I&amp;rsquo;m undecided about this proposal. I think it has some very attractive features, but the costs give me pause (much the same as the alternatives I&amp;rsquo;m aware of, like &lt;a href=&#34;http://davidfaraci.com/populus&#34; target=&#34;_blank&#34;&gt;Populus&lt;/a&gt;). I&amp;rsquo;m only certain that we can&amp;rsquo;t keep going as we have been; it won&amp;rsquo;t end well.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Editorial Gravity</title>
      <link>http://jonathanweisberg.org/post/Editorial%20Gravity/</link>
      <pubDate>Wed, 22 Feb 2017 10:44:10 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/Editorial%20Gravity/</guid>
      <description>

&lt;p&gt;We&amp;rsquo;ve all been there. One referee is positive, the other negative, and the editor decides to reject the submission.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve heard it said editors tend to be conservative given the recommendations of their referees. And that jibes with my experience as an author.&lt;/p&gt;

&lt;p&gt;So is there anything to it&amp;mdash;is &amp;ldquo;editorial gravity&amp;rdquo; a real thing? And if it is, how strong is its pull? Is there some magic function editors use to compute their decision based on the referees&amp;rsquo; recommendations?&lt;/p&gt;

&lt;p&gt;In this post I&amp;rsquo;ll consider how things shake out at &lt;a href=&#34;http://www.ergophiljournal.org/&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;Ergo&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;decision-rules&#34;&gt;Decision Rules&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;Ergo&lt;/em&gt; doesn&amp;rsquo;t have any rule about what an editor&amp;rsquo;s decision should be given the referees&amp;rsquo; recommendations. In fact, we explicitly discourage our editors from relying on any such heuristic. Instead we encourage them to rely on their judgment about the submission&amp;rsquo;s merits, informed by the substance of the referees&amp;rsquo; reports.&lt;/p&gt;

&lt;p&gt;Still, maybe there&amp;rsquo;s some natural law of journal editing waiting to be discovered here, or some unwritten rule.&lt;/p&gt;

&lt;p&gt;Referees choose from four possible recommendations at &lt;em&gt;Ergo&lt;/em&gt;: Reject, Major Revisions, Minor Revisions, or Accept. Let&amp;rsquo;s consider four simple rules we might use to predict an editor&amp;rsquo;s decision, given the recommendations of their referees.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Max: the editor follows the recommendation of the most positive referee. (Ha!)&lt;/li&gt;
&lt;li&gt;Mean: the editor &amp;ldquo;splits the difference&amp;rdquo; between the referees&amp;rsquo; recommendations.

&lt;ul&gt;
&lt;li&gt;Accept + Major Revisions → Minor Revisions, for example.&lt;/li&gt;
&lt;li&gt;When the difference is intermediate between possible decisions, we&amp;rsquo;ll stipulate that this rule &amp;ldquo;rounds down&amp;rdquo;.

&lt;ul&gt;
&lt;li&gt;Major Revisions + Minor Revisions → Major Revisions, for example.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Min: the editor follows the recommendation of the most negative referee.&lt;/li&gt;
&lt;li&gt;Less-than-Min: the editor&amp;rsquo;s decision is a step more negative than either of the referees&amp;rsquo;.

&lt;ul&gt;
&lt;li&gt;Major Revisions + Minor Revisions → Reject, for example.&lt;/li&gt;
&lt;li&gt;Except obviously that Reject + anything → Reject.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Do any of these rules do a decent job of predicting editorial decisions? If so, which does best?&lt;/p&gt;

&lt;h1 id=&#34;a-test&#34;&gt;A Test&lt;/h1&gt;

&lt;p&gt;Let&amp;rsquo;s run the simplest test possible. We&amp;rsquo;ll go through the externally reviewed submissions in &lt;em&gt;Ergo&lt;/em&gt;&amp;rsquo;s database and see how often each rule makes the correct prediction.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/editorial_gravity_files/unnamed-chunk-2-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Not only was Min the most accurate rule, its predictions were correct 85% of the time! (The sample size here is 233 submissions, by the way.) Apparently, editorial gravity is a real thing, at least at &lt;em&gt;Ergo&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Of course, &lt;em&gt;Ergo&lt;/em&gt; might be atypical here. It&amp;rsquo;s a new journal, and online-only with no regular publication schedule. So there&amp;rsquo;s some pressure to play it safe, and no incentive to accept papers in order to fill space.&lt;/p&gt;

&lt;p&gt;But let&amp;rsquo;s suppose for a moment that &lt;em&gt;Ergo&lt;/em&gt; is typical as far as editorial gravity goes. That raises some questions. Here are two.&lt;/p&gt;

&lt;h1 id=&#34;two-questions&#34;&gt;Two Questions&lt;/h1&gt;

&lt;p&gt;First question: can we improve on the Min rule? Is there a not-too-complicated heuristic that&amp;rsquo;s even more accurate?&lt;/p&gt;

&lt;p&gt;Visualizing our data might help us spot any patterns. Typically there are two referees, so we can plot most submissions on a plane according to the referees&amp;rsquo; recommendations. Then we can colour them according to the editor&amp;rsquo;s decision. Adding a little random jitter to make all the points visible:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/editorial_gravity_files/unnamed-chunk-3-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;To my eye this looks a lot like the pattern of concentric-corners you&amp;rsquo;d expect from the Min rule. Though not exactly, especially when the two referees strongly disagree&amp;mdash;the top-left and bottom-right corners of the plot. Still, other than treating cases of strong disagreement as a tossup, no simple way of improving on the Min rule jumps out at me.&lt;/p&gt;

&lt;p&gt;Second question: if editorial gravity is a thing, is it a good thing or a bad thing?&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ll leave that as an exercise for the reader.&lt;/p&gt;

&lt;h1 id=&#34;technical-note&#34;&gt;Technical Note&lt;/h1&gt;

&lt;p&gt;This post was written in R Markdown and the source code is &lt;a href=&#34;https://github.com/jweisber/rgo/blob/master/editorial gravity/editorial gravity.Rmd&#34; target=&#34;_blank&#34;&gt;available on GitHub&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gender &amp; Journal Referees</title>
      <link>http://jonathanweisberg.org/post/Referee%20Gender/</link>
      <pubDate>Mon, 20 Feb 2017 09:34:10 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/Referee%20Gender/</guid>
      <description>

&lt;p&gt;We looked at author gender in &lt;a href=&#34;http://jonathanweisberg.org/post/Author Gender/&#34;&gt;a previous post&lt;/a&gt;, today let&amp;rsquo;s consider referees. Does their gender have any predictive value?&lt;/p&gt;

&lt;p&gt;Once again our discussion only covers men and women because we don&amp;rsquo;t have the data to support a deeper analysis.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Using data from &lt;a href=&#34;http://www.ergophiljournal.org/&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;Ergo&lt;/em&gt;&lt;/a&gt;, we&amp;rsquo;ll consider the following questions:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Requests&lt;/em&gt;. How are requests to referee distributed between men and women? Are men more likely to be invited, for example?&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Responses&lt;/em&gt;. Does gender inform a referee&amp;rsquo;s response to a request? Are women more likely to say &amp;lsquo;yes&amp;rsquo;, for example?&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Response-speed&lt;/em&gt;. Does gender inform how quickly a referee responds to an invitation (whether to agree or to decline)? Do men take longer to agree/decline an invitation, for example?&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Completion-speed&lt;/em&gt;. If a referee does agree to provide a report, does their gender inform how quickly they&amp;rsquo;ll complete that report? Do men and women tend to complete their reports in the same time-frame?&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Recommendations&lt;/em&gt;. Does gender inform how positive/negative a referee&amp;rsquo;s recommendation is? Are men and women equally likely to recommend that a submission be rejected, for example?&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Influence&lt;/em&gt;. Does a referee&amp;rsquo;s gender affect the influence of their recommendation on the editor&amp;rsquo;s decison? Are the recommendations of male referees more likely to be followed, for example?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A quick overview of our data set: there are a total of 1526 referee-requests in &lt;em&gt;Ergo&lt;/em&gt;&amp;rsquo;s database. But only 1394 are included in this analysis. I&amp;rsquo;ve excluded:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Requests to review an invited resubmission, since these are a different sort of beast.&lt;/li&gt;
&lt;li&gt;Pending requests and reports, since the data for these are incomplete.&lt;/li&gt;
&lt;li&gt;A handfull of cases where the referee&amp;rsquo;s gender is either unknown, or doesn&amp;rsquo;t fit the male/female classification.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;requests&#34;&gt;Requests&lt;/h1&gt;

&lt;p&gt;How are requests distributed between men and women? 322 of our 1394 requests went to women, or 23.1% (1072 went to men, or 76.9%).&lt;/p&gt;

&lt;p&gt;How does this compare to the way men and women are represented in academic philosophy in general? Different sources and different subpopulations yield a range of estimates.&lt;/p&gt;

&lt;p&gt;At the low end, we saw in &lt;a href=&#34;http://jonathanweisberg.org/post/Author Gender/&#34;&gt;an earlier post&lt;/a&gt; that about 15.3% of &lt;em&gt;Ergo&lt;/em&gt;&amp;rsquo;s submissions come from women. The PhilPapers survey yields a range from 16.2% (&lt;a href=&#34;https://philpapers.org/surveys/demographics.pl&#34; target=&#34;_blank&#34;&gt;all respondents&lt;/a&gt;) to 18.4% (&lt;a href=&#34;https://philpapers.org/surveys/demographics.pl?affil=Target+faculty&amp;amp;survey=8&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;target&amp;rdquo; faculty&lt;/a&gt;). And sources cited in &lt;a href=&#34;http://www.faculty.ucr.edu/~eschwitz/SchwitzPapers/WomenInPhil-160315b.pdf&#34; target=&#34;_blank&#34;&gt;Schwitzgebel &amp;amp; Jennings&lt;/a&gt; estimate the percentage of women faculty in various English speaking countries at 23% for Australia, 24% for the U.K., and 19&amp;ndash;26% for the U.S.&lt;/p&gt;

&lt;p&gt;So we have a range of baseline estimates from 15% to 26%. For comparison, the 95% confidence interval around our 23.1% finding is (21%, 25.4%).&lt;/p&gt;

&lt;h1 id=&#34;responses&#34;&gt;Responses&lt;/h1&gt;

&lt;p&gt;Do men and women differ in their responses to these requests? Here are the raw numbers:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Agreed&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Declined / No Response / Canceled&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Female&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;101&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;221&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Male&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;403&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;669&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The final column calls for some explanation. I&amp;rsquo;m lumping togther several scenarios here: (i) the referee responds to decline the request, (ii) the referee never responds, (iii) the editors cancel the request because it was made in error. Unfortunately, these three scenarios are hard to distinguish based on the raw data. For example, sometimes a referee declines by email rather than via our online system, and the handling editor then cancels the request instead of marking it as &amp;ldquo;Declined&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;With that in mind, here are the proportions graphically:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/referee_gender_files/unnamed-chunk-6-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Men agreed more often than women: approximately 38% vs. 31%. And this difference is statistically significant.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:0&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:0&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Note that women and men accounted for about 20% and 80% of the &amp;ldquo;Agreed&amp;rdquo; responses, respectively. Whether this figure differs significantly from the gender makeup of &amp;ldquo;the general population&amp;rdquo; depends, as before, on the source and subpopulation we use for that estimate.&lt;/p&gt;

&lt;p&gt;We saw that estimates of female representation ranged from roughly 15% to 26%. For comparison, the 95% confidence interval around our 20% finding is (16.8%, 23.8%).&lt;/p&gt;

&lt;h1 id=&#34;response-speed&#34;&gt;Response-speed&lt;/h1&gt;

&lt;p&gt;Do men and women differ in response-speed&amp;mdash;in how quickly they respond to a referee request (whether to agree or to decline)?&lt;/p&gt;

&lt;p&gt;The average response-time for women is 1.92 days, and for men it&amp;rsquo;s 1.58 days. This difference is not statistically significant.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;A boxplot likewise suggests that men and women have similar interquartile ranges:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/referee_gender_files/unnamed-chunk-9-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;h1 id=&#34;completion-speed&#34;&gt;Completion-speed&lt;/h1&gt;

&lt;p&gt;What about completion-speed: is there any difference in how long men and women take to complete their reports?&lt;/p&gt;

&lt;p&gt;Women took 27.6 days on average, while men took 23.8 days. This difference is statistically significant.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Does that mean men are more likely to complete their reports on time? Not necessarily. Here&amp;rsquo;s a frequency polygram showing when reports were completed:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/referee_gender_files/unnamed-chunk-11-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;The spike at the four-week mark corresponds to the standard due date. We ask referees to submit their reports within 28 days of the initial request.&lt;/p&gt;

&lt;p&gt;It looks like men had a stronger tendency to complete their reports early. But were they more likely to complete them on time?&lt;/p&gt;

&lt;p&gt;One way to tackle this question is to look at how completed reports accumulate with time (the &lt;a href=&#34;https://en.wikipedia.org/wiki/Empirical_distribution_function&#34; target=&#34;_blank&#34;&gt;empirical cumulative distribution&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/referee_gender_files/unnamed-chunk-12-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;As expected, the plot shows that men completed their reports early with greater frequency. But it also looks like women and men converged around the four-week mark, when reports were due.&lt;/p&gt;

&lt;p&gt;Another way of approaching the question is to classify reports as either &amp;ldquo;On Time&amp;rdquo; or &amp;ldquo;Late&amp;rdquo;, according to whether they were completed before Day 29.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;On Time&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Late&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Female&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;38&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Male&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;242&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;121&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/referee_gender_files/unnamed-chunk-14-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;A chi-square test of independence then finds no statistically significant difference.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:6&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:6&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Apparently men and women differed in their tendency to be early, but not necessarily in their tendency to be on time.&lt;/p&gt;

&lt;h1 id=&#34;recommendations&#34;&gt;Recommendations&lt;/h1&gt;

&lt;p&gt;Did male and female referees differ in their recommendations to the editors?&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Ergo&lt;/em&gt; offers referees four recommendations to choose from. The raw numbers:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Reject&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Major Revisions&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Minor Revisions&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Accept&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Female&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;42&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;29&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Male&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;154&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;103&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;61&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;45&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;In terms of frequencies:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/referee_gender_files/unnamed-chunk-16-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;The differences here are not statistically significant according to a chi-square test of independence.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:5&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:5&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h1 id=&#34;influence&#34;&gt;Influence&lt;/h1&gt;

&lt;p&gt;Does a referee&amp;rsquo;s gender affect whether the editor follows their recommendation? We can tackle this question a few different ways.&lt;/p&gt;

&lt;p&gt;One way is to just tally up those cases where the editor&amp;rsquo;s decision was the same as the referee&amp;rsquo;s recommendation, and those where it was different.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Same&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Different&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Female&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;51&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;37&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Male&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;206&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;157&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/referee_gender_files/unnamed-chunk-17-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;Clearly there&amp;rsquo;s no statistically significant difference between male and female referees here.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:7&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:7&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;A second approach would be to assign numerical ranks to referees&amp;rsquo; recommendations and editors&amp;rsquo; decisions: Reject = 1, Major Revisions = 2, etc. Then we can consider how far the editor&amp;rsquo;s decision is from the referee&amp;rsquo;s recommendation. For example, a decision of Accept is 3 away from a recommendation of Reject, while a decision of Major Revisions is 2 away from a recommendation of Accept.&lt;/p&gt;

&lt;p&gt;By this measure, the average distance between the referee&amp;rsquo;s recommendation and the editor&amp;rsquo;s decision was 0.57 for women and 0.56 for men&amp;mdash;clearly not a statistically significant difference.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:8&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:8&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Men received more requests to referee than women, as expected given the well known gender imbalance in academic philosophy. The distribution of requests between men (76.9%) and women (23.1%) was in line with some estimates of the gender makeup of academic philosophy, though not all estimates.&lt;/p&gt;

&lt;p&gt;Men were more likely to agree to a request (38% vs. 31%), a statistically significant difference. Women accounted for about 20% of the &amp;ldquo;Agreed&amp;rdquo; responses, however, consistent with most (but not all) estimates of the gender makeup of academic philosophy.&lt;/p&gt;

&lt;p&gt;There was no statistically significant difference in response-speed, but there was in the speed with which reports were completed (23.8 days on average for men, 27.6 days for women). This difference appears to be due to a stronger tendency on the part of men to complete their reports early, though not necessarily a greater chance of meeting the deadline.&lt;/p&gt;

&lt;p&gt;Finally, there was no statistically significant difference in the recommendations of male and female referees, or in editors&amp;rsquo; uptake of those recommendations.&lt;/p&gt;

&lt;h1 id=&#34;technical-notes&#34;&gt;Technical Notes&lt;/h1&gt;

&lt;p&gt;This post was written in R Markdown and the source is &lt;a href=&#34;https://github.com/jweisber/rgo/blob/master/referee%20gender/referee%20gender.Rmd&#34; target=&#34;_blank&#34;&gt;available on GitHub&lt;/a&gt;. I&amp;rsquo;m new to both R and classical statistics, and this post is a learning exercise for me. So I encourage you to check the code and contact me with corrections.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Unlike in the previous analysis of author gender, however, here we do have a few known cases where either (i) the referee identifies as neither male nor female, or (ii) they identify as something more specific, e.g. &amp;ldquo;transgender male&amp;rdquo; rather than just &amp;ldquo;male&amp;rdquo;. But these cases are still too few for statistical analysis.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:0&#34;&gt;$\chi^2$(1, &lt;em&gt;N&lt;/em&gt; = 1394) = 3.89, &lt;em&gt;p&lt;/em&gt; = 0.05.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:0&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;&lt;em&gt;t&lt;/em&gt;(437.43) = -1.63, &lt;em&gt;p&lt;/em&gt; = 0.1
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;&lt;em&gt;t&lt;/em&gt;(144.26) = -2.46, &lt;em&gt;p&lt;/em&gt; = 0.02
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;$\chi^2$(1, &lt;em&gt;N&lt;/em&gt; = 451) = 2.59, &lt;em&gt;p&lt;/em&gt; = 0.11.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:6&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;$\chi^2$(3, &lt;em&gt;N&lt;/em&gt; = 451) = 3.6, &lt;em&gt;p&lt;/em&gt; = 0.31.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:5&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;$\chi^2$(1, &lt;em&gt;N&lt;/em&gt; = 451) = 0.01, &lt;em&gt;p&lt;/em&gt; = 0.93.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:7&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;&lt;em&gt;t&lt;/em&gt;(117.57) = 0.07, &lt;em&gt;p&lt;/em&gt; = 0.95.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:8&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>In Defense of Reviewer 2</title>
      <link>http://jonathanweisberg.org/post/Reviewer%202/</link>
      <pubDate>Mon, 06 Feb 2017 10:36:10 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/Reviewer%202/</guid>
      <description>

&lt;p&gt;Spare a thought for Reviewer 2, that much-maligned shade of academe. There&amp;rsquo;s even &lt;a href=&#34;https://twitter.com/hashtag/reviewer2&#34; target=&#34;_blank&#34;&gt;a hashtag&lt;/a&gt; dedicated to the joke:&lt;/p&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet tw-align-center&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;A rare glimpse of reviewer 2, seen here in their natural habitat &lt;a href=&#34;https://t.co/lpT1BVhDCX&#34;&gt;pic.twitter.com/lpT1BVhDCX&lt;/a&gt;&lt;/p&gt;&amp;mdash; Aidan McGlynn (@AidanMcGlynn) &lt;a href=&#34;https://twitter.com/AidanMcGlynn/status/820647829446283264&#34;&gt;January 15, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;http://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;But is it just a joke? Order could easily matter here.&lt;/p&gt;

&lt;p&gt;Referees invited later weren&amp;rsquo;t the editor&amp;rsquo;s first choice, after all. Maybe they&amp;rsquo;re less competent, less likely to appreciate your brilliant insights as an author. Or maybe they&amp;rsquo;re more likely to miss well-disguised flaws! Then we should expect Reviewer 2 to be the more &lt;em&gt;generous&lt;/em&gt; one.&lt;/p&gt;

&lt;p&gt;Come to think of it, we can order referees in other ways beside order-of-invite. We might order them according to who completes their report fastest, for example. And faster referees might be more careless, hence more dismissive. Or they might be less critical and thus more generous.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s a lot to consider. Let&amp;rsquo;s investigate, using &lt;a href=&#34;http://www.ergophiljournal.org/&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;Ergo&lt;/em&gt;&lt;/a&gt;&amp;rsquo;s data, &lt;a href=&#34;http://jonathanweisberg.org/tags/rgo/&#34;&gt;as usual&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;severity-generosity&#34;&gt;Severity &amp;amp; Generosity&lt;/h1&gt;

&lt;p&gt;Reviewer 2 is accused of a lot. It&amp;rsquo;s not just that their overall take is more severe; they also tend to miss the point. They&amp;rsquo;re irresponsible and superficial in their reading. And to the extent they do appreciate the author&amp;rsquo;s point, their objections are poorly thought out. What&amp;rsquo;s more, if they bother to demand revisions, their demands are unreasonable.&lt;/p&gt;

&lt;p&gt;We can&amp;rsquo;t measure these things directly, of course. But we can estimate a referee&amp;rsquo;s generosity indirectly, using their recommendation to the editors as a proxy.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Ergo&lt;/em&gt;&amp;rsquo;s referees choose from four possible recommendations: Reject, Major Revisions, Minor Revisions, and Accept. To estimate a referee&amp;rsquo;s generosity, we&amp;rsquo;ll assign these recommendations numerical ranks, from 1 (Reject) up through 4 (Accept).&lt;/p&gt;

&lt;p&gt;The higher this number, the more generous the referee; the lower, the more severe.&lt;/p&gt;

&lt;h1 id=&#34;invite-order&#34;&gt;Invite Order&lt;/h1&gt;

&lt;p&gt;Is there any connection between the order in which referees are invited and their severity?&lt;/p&gt;

&lt;p&gt;Usually an editor has to try a few people before they get two takers. So we can assign each potential referee an &amp;ldquo;invite rank&amp;rdquo;. The first person asked has rank 1, the second person asked has rank 2, and so on.&lt;/p&gt;

&lt;p&gt;Is there a correlation between invite rank and severity?&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s a plot of invite rank (&lt;em&gt;x&lt;/em&gt;-axis) and generosity (&lt;em&gt;y&lt;/em&gt;-axis). (The points have non-integer heights because I&amp;rsquo;ve added some random  &lt;a href=&#34;http://r4ds.had.co.nz/data-visualisation.html#position-adjustments&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;jitter&amp;rdquo;&lt;/a&gt; to make them all visible. Otherwise you&amp;rsquo;d just see an uninformative grid.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/reviewer_2_files/unnamed-chunk-2-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The blue curve shows the overall trend in the data.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; It&amp;rsquo;s basically flat all the way through, except at the far-right end where the data is too sparse to be informative.&lt;/p&gt;

&lt;p&gt;We can also look at the classic measure of correlation known as &lt;a href=&#34;https://en.wikipedia.org/wiki/Spearman&#39;s_rank_correlation_coefficient&#34; target=&#34;_blank&#34;&gt;Spearman&amp;rsquo;s rho&lt;/a&gt;. The estimate is essentially 0 given our data ($r_s$ = 0.01).&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Evidently, invite-rank has no discernible impact on severity.&lt;/p&gt;

&lt;h1 id=&#34;speed&#34;&gt;Speed&lt;/h1&gt;

&lt;p&gt;But now let&amp;rsquo;s look at the speed with which a referee completes their report:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/reviewer_2_files/unnamed-chunk-4-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Here an upward trend is discernible. And our estimate of Spearman&amp;rsquo;s rho agrees: $r_s$ = 0.1, a small but non-trivial correlation.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Apparently, referees who take longer tend to be more generous!&lt;/p&gt;

&lt;h1 id=&#34;my-take&#34;&gt;My Take&lt;/h1&gt;

&lt;p&gt;I find these results encouraging, for the most part.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s nice to know that an editor&amp;rsquo;s first choice for a referee is the same as their fifth, as far as how severe or generous they&amp;rsquo;re likely to be.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s also nice to know that the speed with which a referee completes their report doesn&amp;rsquo;t &lt;em&gt;hugely&lt;/em&gt; inform heir severity.&lt;/p&gt;

&lt;p&gt;One we might well worry that faster referees are unduly severe. But this worry is tempered by a few considerations.&lt;/p&gt;

&lt;p&gt;For one thing, the effect we found is small enough that it could just be noise. It is detectable using tools like regression and significance testing, so it&amp;rsquo;s not to be dismissed out of hand. But we might also do well to heed the wisdom of &lt;a href=&#34;https://xkcd.com/1725/&#34; target=&#34;_blank&#34;&gt;XKCD&lt;/a&gt; here:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://imgs.xkcd.com/comics/linear_regression_2x.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Even if the effect is real, though, it could be a good thing just as easily as a bad thing.&lt;/p&gt;

&lt;p&gt;True, referees who work fast might be sloppy and dismissive. And those who take longer might feel guiltier and thus be unduly generous.&lt;/p&gt;

&lt;p&gt;But maybe referees who are more on the ball are both more prompt and more apt to spot a submission&amp;rsquo;s flaws. Or (as my coeditor Franz Huber pointed out) manuscripts that should clearly be rejected might be easier to referee on average, hence faster.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s hard to know what to make of this effect, if it is an effect. Clearly, &lt;a href=&#34;https://twitter.com/hashtag/moreresearchisneeded&#34; target=&#34;_blank&#34;&gt;#MoreResearchIsNeeded&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;technical-notes&#34;&gt;Technical Notes&lt;/h1&gt;

&lt;p&gt;This post was written in R Markdown and the source is &lt;a href=&#34;https://github.com/jweisber/rgo/blob/master/reviewer%202/reviewer%202.Rmd&#34; target=&#34;_blank&#34;&gt;available on GitHub&lt;/a&gt;. I&amp;rsquo;m new to both R and statistics, and this post is a learning exercise for me. So I encourage you to check the code and contact me with corrections.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Specifically, the blue curve is a regression curve using the &lt;a href=&#34;https://en.wikipedia.org/wiki/Local_regression#Definition_of_a_LOESS_model&#34; target=&#34;_blank&#34;&gt;LOESS&lt;/a&gt; method of fit.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;A significance test of the null hypothesis $\rho_s$ = 0 yields &lt;em&gt;p&lt;/em&gt; = 0.87.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;Testing the null hypothesis $\rho_s$ = 0 yields &lt;em&gt;p&lt;/em&gt; = 0.03.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Gender &amp; Journal Submissions</title>
      <link>http://jonathanweisberg.org/post/Author%20Gender/</link>
      <pubDate>Thu, 26 Jan 2017 10:36:10 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/Author%20Gender/</guid>
      <description>

&lt;p&gt;Does an author&amp;rsquo;s gender affect the fate of their submission to an academic journal? It&amp;rsquo;s a big question, even if we restrict ourselves to philosophy journals.&lt;/p&gt;

&lt;p&gt;But we can make a start by using &lt;a href=&#34;http://www.ergophiljournal.org&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;Ergo&lt;/em&gt;&lt;/a&gt; as one data-point. I&amp;rsquo;ll examine two questions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Question 1: Does gender affect the decision rendered at &lt;em&gt;Ergo&lt;/em&gt;? Are men more likely to have their papers accepted, for example?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Question 2: Does gender affect time-to-decision at &lt;em&gt;Ergo&lt;/em&gt;? For example, do women have to wait longer on average for a decision?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;background&#34;&gt;Background&lt;/h1&gt;

&lt;p&gt;Some important background and caveats before we begin:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Our data set goes back to Feb. 11, 2015, when &lt;em&gt;Ergo&lt;/em&gt; moved to its current online system for handling submissions. We do have records going back to Jun. 2013, when the journal launched. But integrating the data from the two systems is a programming hassle I haven&amp;rsquo;t faced up to yet.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;We&amp;rsquo;ll exclude submissions that were withdrawn by the author before a decision could be rendered. Usually, when an author withdraws a submission, it&amp;rsquo;s so that they can resubmit a trivially-corrected manuscript five minutes later. So this data mostly just gets in the way.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;We&amp;rsquo;ll also exclude submissions that were still under review as of Jan. 1, 2017, since the data there is incomplete.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The gender data we&amp;rsquo;ll be using was gathered manually by &lt;em&gt;Ergo&lt;/em&gt;&amp;rsquo;s managing editors (me and Franz Huber). In most cases we didn&amp;rsquo;t know the author personally. So we did a quick google to see whether we could infer the author&amp;rsquo;s gender based on public information, like pronouns and/or pictures. When we weren&amp;rsquo;t confident that we could, we left their gender as &amp;ldquo;unknown&amp;rdquo;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;This analysis covers only men and women, because there haven&amp;rsquo;t yet been any cases where we could confidently infer that an author identified as another gender. And the &amp;ldquo;gender unknown&amp;rdquo; cases are too few for reliable statistical analysis.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Since we only have data for the gender of the submitting author, our analysis will overlook co-authors.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With that in mind, a brief overview: our data set contains $696$ submissions over almost two years (Feb. 11, 2015 up to Jan. 1, 2017), but only $639$ of these are included in this analysis. The $52$ submissions that were in-progress as of Jan. 1, 2017, or were withdrawn by the author, have been excluded. Another $5$ cases where the author&amp;rsquo;s gender was unknown were also excluded.&lt;/p&gt;

&lt;h1 id=&#34;gender-decisions&#34;&gt;Gender &amp;amp; Decisions&lt;/h1&gt;

&lt;p&gt;Does an author&amp;rsquo;s gender affect the journal&amp;rsquo;s decision about whether their submission is accepted? We can slice this question a few different ways:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Does gender affect the first-round decision to reject/accept/R&amp;amp;R?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Does gender affect the likelihood of desk-rejection, specifically?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Does gender affect the chance of converting an R&amp;amp;R into an accept?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Does gender affect the ultimate decision to accept/reject (whether via an intervening R&amp;amp;R or not)?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The short answer to all these questions is: no, at least not in a statistically significant way. But there are some wrinkles. So let&amp;rsquo;s take each question in turn.&lt;/p&gt;

&lt;h2 id=&#34;first-round-decisions&#34;&gt;First-Round Decisions&lt;/h2&gt;

&lt;p&gt;Does gender affect the first-round decision to reject/accept/R&amp;amp;R?&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Ergo&lt;/em&gt; has two kinds of R&amp;amp;R, Major Revisions and Minor Revisions. Here are the raw numbers:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Reject&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Major Revisions&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Minor Revisions&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Accept&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Female&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;76&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Male&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;438&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;41&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Graphically, in terms of percentages:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/author_gender_files/unnamed-chunk-4-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;There are differences here, of course: women were asked to make major revisions more frequently than men, for example. And men received verdicts of minor revisions or outright acceptance more often than women.&lt;/p&gt;

&lt;p&gt;Are these differences significant? They don&amp;rsquo;t look it from the bar graph. And a standard chi-square test agrees.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h2 id=&#34;desk-rejections&#34;&gt;Desk Rejections&lt;/h2&gt;

&lt;p&gt;Things are a little more interesting if we separate out desk rejections from rejections-after-external-review. The raw numbers:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Desk Reject&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Non-desk Reject&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Major Revisions&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Minor Revisions&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Accept&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Female&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;61&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Male&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;311&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;127&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;41&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;In terms of percentages for men and women:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/author_gender_files/unnamed-chunk-9-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;The differences here are more pronounced. For example, women had their submissions desk-rejected more frequently, a difference of about 8.5%.&lt;/p&gt;

&lt;p&gt;But once again, the differences are not statistically significant according to the standard chi-square test.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h2 id=&#34;ultimate-decisions&#34;&gt;Ultimate Decisions&lt;/h2&gt;

&lt;p&gt;What if we just consider a submission&amp;rsquo;s ultimate fate&amp;mdash;whether it&amp;rsquo;s accepted or rejected in the end? Here the results are pretty clear:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Reject&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Accept&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Female&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;78&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Male&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;450&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;38&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/author_gender_files/unnamed-chunk-13-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;Pretty obviously there&amp;rsquo;s no significant difference, and a chi-square test agrees.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h2 id=&#34;conversions&#34;&gt;Conversions&lt;/h2&gt;

&lt;p&gt;Our analysis so far suggests that men and women probably have about equal chance of converting an R&amp;amp;R into an accept. Looking at the numbers directly corroborates that thought:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Reject&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Accept&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Female&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Male&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;33&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/author_gender_files/unnamed-chunk-16-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;As before, a standard chi-square test agrees.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; Though, of course, the numbers here are small and shouldn&amp;rsquo;t be given too much weight.&lt;/p&gt;

&lt;h2 id=&#34;conclusion-so-far&#34;&gt;Conclusion So Far&lt;/h2&gt;

&lt;p&gt;None of the data so far yielded a significant difference between men and women. None even came particularly close (see the footnotes for the numerical details). So it seems the journal&amp;rsquo;s decisions are independent of gender, or nearly so.&lt;/p&gt;

&lt;h1 id=&#34;gender-time-to-decision&#34;&gt;Gender &amp;amp; Time-to-Decision&lt;/h1&gt;

&lt;p&gt;Authors don&amp;rsquo;t just care what decision is rendered, of course. They also care that decisions are made quickly. Can men and women expect similar wait-times?&lt;/p&gt;

&lt;p&gt;The average time-to-decision is 23.3 days. But for men it&amp;rsquo;s 23.9 days while for women it&amp;rsquo;s only 19.6. This looks like a significant difference. And although it isn&amp;rsquo;t quite significant according to a standard $t$ test, it very nearly is.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:5&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:5&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;What might be going on here? Let&amp;rsquo;s look at the observed distributions for men and women:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/author_gender_files/unnamed-chunk-19-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;A striking difference is that there are so many more submissions from men than from women. But otherwise these distributions actually look quite similar. Each is a bimodal distribution with one peak for desk-rejections around one week, and another, smaller peak for externally reviewed submissions around six or seven weeks.&lt;/p&gt;

&lt;p&gt;We noticed earlier that women had more desk-rejections by about 8.5%. And while that difference wasn&amp;rsquo;t statistically significant, it may still be what&amp;rsquo;s causing the almost-significant difference we see with time-to-decision (especially if men also have a few extra outliers, as seems to be the case).&lt;/p&gt;

&lt;p&gt;To test this hypothesis, we can separate out desk-rejections and externally reviewed submissions. Graphically:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/author_gender_files/unnamed-chunk-20-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/author_gender_files/unnamed-chunk-20-2.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;Aside from the raw numbers, the distributions for men and for women look very similar. And if we run separate $t$ tests for desk-rejections and for externally reviewed submissions, gender differences are no longer close to significance. For desk-rejections $p = 0.24$. And for externally reviewed submissions $p = 0.46$.&lt;/p&gt;

&lt;h1 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;Apparently an author&amp;rsquo;s gender has little or no effect on the content or speed of &lt;em&gt;Ergo&lt;/em&gt;&amp;rsquo;s decision. I&amp;rsquo;d &lt;em&gt;like&lt;/em&gt; to think this is a result of the journal&amp;rsquo;s &lt;a href=&#34;http://www.ergophiljournal.org/review.html&#34; target=&#34;_blank&#34;&gt;strong commitment to triple-anonymous review&lt;/a&gt;. But without data from other journals to make comparisons, we can&amp;rsquo;t really infer much about potential causes. And, of course, we can&amp;rsquo;t generalize to other journals with any confidence, either.&lt;/p&gt;

&lt;h1 id=&#34;technical-notes&#34;&gt;Technical Notes&lt;/h1&gt;

&lt;p&gt;This post was written in R Markdown and the source is &lt;a href=&#34;https://github.com/jweisber/rgo/blob/master/author%20gender/author%20gender.Rmd&#34; target=&#34;_blank&#34;&gt;available on GitHub&lt;/a&gt;. I&amp;rsquo;m new to both R and classical statistics, and this post is a learning exercise for me. So I encourage you to check the code and contact me with corrections.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;&lt;p&gt;Specifically, $\chi^2(3, N = 587) = 1.89$, $p = 0.6$. This raises the question of power, and for a small effect size ($w = .1$) power is only about $0.51$. But it increases quickly to $0.99$ at $w = .2$.&lt;/p&gt;

&lt;p&gt;Given the small numbers in some of the columns though, especially the Accept column, we might prefer a different test than $\chi^2$. The more precise $G$ test yields $p = 0.46$, still fairly large. And Fisher&amp;rsquo;s exact test yields $p = 0.72$.&lt;/p&gt;

&lt;p&gt;We might also do an ordinal analysis, since decisions have a natural desirability ordering for authors: Accept &amp;gt; Minor Revisions &amp;gt; Major Revisions &amp;gt; Reject. We can test for a linear trend by assigning integer ranks from 4 down through 1 &lt;a href=&#34;http://ca.wiley.com/WileyCDA/WileyTitle/productCd-0470463635.html&#34; target=&#34;_blank&#34;&gt;(Agresti 2007)&lt;/a&gt;.  A test of the &lt;a href=&#34;https://onlinecourses.science.psu.edu/stat504/node/91&#34; target=&#34;_blank&#34;&gt;Mantel-Haenszel statistic&lt;/a&gt; $M^2$ then yields $p = 0.82$.&lt;/p&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;

&lt;li id=&#34;fn:2&#34;&gt;&lt;p&gt;Here we have $\chi^2(4, N = 587) = 4.64$, $p = 0.33$. As before, the power for a small effect ($w = .1$) is only middling, about 0.46, but increases quickly to near certainty ($0.98$) by $w = .2$.&lt;/p&gt;

&lt;p&gt;Instead of $\chi^2$ we might again consider a $G$ test, which yields $p = 0.24$, or Fisher&amp;rsquo;s exact test which yields $p = 0.37$.&lt;/p&gt;

&lt;p&gt;For an ordinal test using the ranking Desk Reject &amp;lt; Non-desk Reject &amp;lt; Major Revisions &amp;lt; etc., the Mantel-Haenszel statistic $M^2$ now yields $p = 0.39$.&lt;/p&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;Here we have $\chi^2(1, N = 571) = 0.11$, $p = 0.74$.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;$\chi^2(1, N = 52) = 0$, $p = 1$.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;Specifically, $t(137.71) = 1.78$, $p = 0.08$. Although a $t$ test may not actually be the best choice here, since (as we&amp;rsquo;re about to see) the sampling distributions aren&amp;rsquo;t normal, but rather bimodal. Still, we can compare this result to non-parametric tests like Wilcoxon-Mann-Whitney ($p = 0.1$) or the bootstrap-$t$  ($p = 0.07$). These $p$-values don&amp;rsquo;t quite cross the customary $\alpha = .05$ threshold either, but they are still small.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:5&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Thursday Conundrum</title>
      <link>http://jonathanweisberg.org/post/The%20Thursday%20Conundrum/</link>
      <pubDate>Mon, 16 Jan 2017 10:14:00 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/The%20Thursday%20Conundrum/</guid>
      <description>

&lt;p&gt;In &lt;a href=&#34;http://jonathanweisberg.org/post/An%20Editors%20Favourite%20Days/&#34;&gt;an earlier post&lt;/a&gt; we saw that Mondays and Thursdays are good for editors, at least at &lt;a href=&#34;http://www.ergophiljournal.org/&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;Ergo&lt;/em&gt;&lt;/a&gt;. Potential referees say yes more often when invited on these days. But why?&lt;/p&gt;

&lt;p&gt;Mondays aren&amp;rsquo;t too puzzling. It&amp;rsquo;s the start of a new week, so people are fresh, and maybe just a little deluded about how productive the coming week will prove to be.&lt;/p&gt;

&lt;p&gt;But Thursdays? They don&amp;rsquo;t seem especially special. I tried &lt;a href=&#34;http://jonathanweisberg.org/post/An%20Editors%20Favourite%20Days/#theory&#34;&gt;speculating &lt;em&gt;a priori&lt;/em&gt;&lt;/a&gt; about what might be going on there. But it&amp;rsquo;d be nice to have a hypothesis that&amp;rsquo;s grounded in some data.&lt;/p&gt;

&lt;h1 id=&#34;virtual-mondays&#34;&gt;Virtual Mondays?&lt;/h1&gt;

&lt;p&gt;At first I thought it might be something subtle. Maybe the day the invite is sent isn&amp;rsquo;t as important as when the referee &lt;em&gt;responds&lt;/em&gt;. An invitation sent on Thursday might not be answered until the following Monday. Whereas invites sent on Monday might tend to be answered the same day. Then Thursday would end up being a kind of virtual Monday, as far as referees responding to invites goes.&lt;/p&gt;

&lt;p&gt;That didn&amp;rsquo;t seem to fit the data, though. For one thing, if you look at which days referees are least likely to &lt;em&gt;respond&lt;/em&gt; negatively, it&amp;rsquo;s Mondays and Thursdays again:
&lt;img src=&#34;http://jonathanweisberg.org/img/the_thursday_conundrum_files/unnamed-chunk-2-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;For another, if you look at when referees respond to requests sent on Monday, it&amp;rsquo;s the same pattern as for requests sent on Thursday. In either case, referees typically respond the same day, or in the next couple of days. Here&amp;rsquo;s the pattern for Monday-invites:
&lt;img src=&#34;http://jonathanweisberg.org/img/the_thursday_conundrum_files/unnamed-chunk-3-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;
And here are Thursday-invites:
&lt;img src=&#34;http://jonathanweisberg.org/img/the_thursday_conundrum_files/unnamed-chunk-4-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;
In case you&amp;rsquo;re curious, here are all the days of the week, tiled according to day-of-invite:
&lt;img src=&#34;http://jonathanweisberg.org/img/the_thursday_conundrum_files/unnamed-chunk-5-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;
The pattern is pretty similar regardless of the day the invite is sent (except for the predictable effect of weekends, which tend to dampen responses accross the board).&lt;/p&gt;

&lt;h1 id=&#34;the-beleaguered&#34;&gt;The Beleaguered&lt;/h1&gt;

&lt;p&gt;So my current hypothesis is much more flat-footed: it&amp;rsquo;s mainly a matter of when referees are busy. Monday they&amp;rsquo;re feeling fresh from the weekend, as I suggested. But why would Thursday be less overwhelming for referees? Maybe because they get fewer invitations then.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s test that hypothesis. Here are the total numbers of invites sent out each day of the week, over the last two years at &lt;em&gt;Ergo&lt;/em&gt;:
&lt;img src=&#34;http://jonathanweisberg.org/img/the_thursday_conundrum_files/unnamed-chunk-6-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;
The overall pattern is pretty much what you&amp;rsquo;d expect. Weekends are quiet (because even editors have lives). Then things pick up on Monday and Tuesday as the workweek begins, before declining again as the week wears on.&lt;/p&gt;

&lt;p&gt;Note the uptick from Thursday to Friday, though: a difference of about 30 invitations. On a scale ranging from ~100 to ~250, that may be a non-trivial difference. And the same pattern shows up in both years we have data for:
&lt;img src=&#34;http://jonathanweisberg.org/img/the_thursday_conundrum_files/unnamed-chunk-7-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;
So maybe Thursdays are good because things are quieting down as the weekend approaches, and referees are receiving fewer requests. But by Friday it&amp;rsquo;s too late. The editors of the world are scrambling to lock down referees before the weekend. And, probably, referees aren&amp;rsquo;t keen to clutter up their desks just as they&amp;rsquo;re about to go into a weekend break.&lt;/p&gt;

&lt;h1 id=&#34;how-is-a-raven-like-a-writing-desk&#34;&gt;How is a Raven Like a Writing Desk?&lt;/h1&gt;

&lt;p&gt;But if Thursdays are good because fewer requests go out then, shouldn&amp;rsquo;t Mondays be terrible? We just saw that the start of the week is the busiest time as far as number of requests sent to referees.&lt;/p&gt;

&lt;p&gt;My guess is that Monday and Thursday are to be explained somewhat differently. Thursdays are distinguished by their quietude, whereas Mondays are marked by vim and vigour. People are fresh, as I said. But also, the onslaught of the week&amp;rsquo;s workload hasn&amp;rsquo;t really hit yet.&lt;/p&gt;

&lt;p&gt;In support of this last hypothesis, notice that the following pattern is quite robust: weekends are quiet, followed by a burst of activity early in the week, followed by decline towards the next weekend. We saw this pattern with editors sending requests to referees. But we see it other places too.&lt;/p&gt;

&lt;p&gt;For example, here&amp;rsquo;s how the quantity of submissions the journal receives varies over the week:
&lt;img src=&#34;http://jonathanweisberg.org/img/the_thursday_conundrum_files/unnamed-chunk-8-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;
And here are the numbers of referee reports completed each day:
&lt;img src=&#34;http://jonathanweisberg.org/img/the_thursday_conundrum_files/unnamed-chunk-9-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;
Pretty clearly, authors, editors, and referees are all quietest on the weekends, and most active at the week&amp;rsquo;s start. (We also see in this last graph, as  with editors contacting referees, that there&amp;rsquo;s a feeble resurgence toward week&amp;rsquo;s end&amp;mdash;presumably in an attempt to clear the docket before the weekend.)&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;So here&amp;rsquo;s my theory, at least for now.&lt;/p&gt;

&lt;p&gt;Referees are game on Mondays for the obvious reasons: they&amp;rsquo;ve had the weekend to recharge and catch up, and the onslaught of Monday&amp;rsquo;s and Tuesday&amp;rsquo;s new submissions&amp;mdash;and the corresponding wave of invitations to referees&amp;mdash;hasn&amp;rsquo;t reverberated out into the referee-verse just yet. (Not to mention other demands, like teaching.)&lt;/p&gt;

&lt;p&gt;Referees are game on Thursdays, too, but for somewhat different reasons. As the week wears on, authors and editors wind down, so referees find fewer invites in their inboxes. They&amp;rsquo;ve also completed their existing assignments earlier in the week, maybe even  submitted their own papers. So they&amp;rsquo;re game, until the next day, Friday, when editors do their last-minute, pre-weekend scramble&amp;mdash;which is especially ill-timed since referees are switching out of work-mode anyway.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s a bit unlovely and disunified, this explanation. But not entirely. Mondays and Thursdays do have something in common on this story. They&amp;rsquo;re both days when things are calmer for referees, albeit calm in different ways and for somewhat different reasons.&lt;/p&gt;

&lt;h1 id=&#34;technical-notes&#34;&gt;Technical Notes&lt;/h1&gt;

&lt;p&gt;This post was written in R Markdown and the source is &lt;a href=&#34;https://github.com/jweisber/rgo/blob/master/thursday%20conundrum/the%20thursday%20conundrum.Rmd&#34; target=&#34;_blank&#34;&gt;available on GitHub&lt;/a&gt;. I&amp;rsquo;m new to R and data science, and this post is a learning exercise for me. So I encourage you to check the code and contact me with corrections.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An Editor&#39;s Favourite Days of the Week</title>
      <link>http://jonathanweisberg.org/post/An%20Editors%20Favourite%20Days/</link>
      <pubDate>Mon, 09 Jan 2017 10:13:49 -0500</pubDate>
      
      <guid>http://jonathanweisberg.org/post/An%20Editors%20Favourite%20Days/</guid>
      <description>

&lt;p&gt;Finding willing referees is one of the biggest challenges for a journal editor. Are referees more willing some days of the week than others? Apparently they are, on Mondays&amp;hellip; and Thursdays, for some reason. At least, that&amp;rsquo;s how things have gone at &lt;a href=&#34;http://ergophiljournal.org/&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;Ergo&lt;/em&gt;&lt;/a&gt; the last couple years (2015 and 2016).&lt;/p&gt;

&lt;h1 id=&#34;data&#34;&gt;Data&lt;/h1&gt;

&lt;p&gt;Consider the &amp;ldquo;bounce rate&amp;rdquo; for a given day of the week: the portion of invites sent on that day that end up being declined (&lt;em&gt;bounce rate = #declined / #invited&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;Editors prefer a lower bounce rate. And, on average over the last two years, the lowest bounce rates at &lt;em&gt;Ergo&lt;/em&gt; were on Monday and Thursday:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/editor_favourite_days_files/unnamed-chunk-2-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s an odd pattern&amp;hellip; maybe it&amp;rsquo;s not a pattern at all? To check, let&amp;rsquo;s look at 2015 and 2016 separately:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jonathanweisberg.org/img/editor_favourite_days_files/unnamed-chunk-3-1.png&#34; alt=&#34;&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;

&lt;p&gt;It sure looks like the same pattern each year.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:0&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:0&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Moreover, going back to the overall data from the first graph, there&amp;rsquo;s pretty significant fluctuation: from a minimum of 0.44 on Thursdays to a maximum of 0.6 on Tuesdays/Fridays/Saturdays, a difference of 0.16. That would be a lot of fluctuation if it were just random noise. Given how large the sample is (1280 invitations in all), it seems pretty safe to say this is a real thing.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;What about the difference between Mondays and Thursdays&amp;mdash;is that significant or just noise? Well, it may not look trivial, but it&amp;rsquo;s not statistically significant according to the standard test for such things.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; As for the other days of the week, they&amp;rsquo;re generally even closer together, and the same test suggests there&amp;rsquo;s nothing significant in the variation there, either.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Apparently, the most we can say is that Mondays and Thursdays stand out from the rest of the week.&lt;/p&gt;

&lt;h1 id=&#34;theory&#34;&gt;Theory&lt;/h1&gt;

&lt;p&gt;So what&amp;rsquo;s the explanation?&lt;/p&gt;

&lt;p&gt;Monday I get. It&amp;rsquo;s the only day of the week that immediately follows the weekend (except on long weekends, obviously). So Mondays are when we&amp;rsquo;re most recovered from the burnout of the previous week. They&amp;rsquo;re when we&amp;rsquo;re full of expectations and plans for the coming week, and most deceived about how productive and unbusy Thursday and Friday will be.&lt;/p&gt;

&lt;p&gt;But Thursdays are more puzzling. Why would they be special? A colleague suggested false optimism about free time as an explanation. &amp;ldquo;That makes sense for Mondays,&amp;rdquo; I thought, &amp;ldquo;but not Thursdays&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Maybe it does make sense after all, though. Thursday is the closest you can get to the end of the week without being a Friday. And Fridays are basically the weekend, right? So maybe Thursday is late enough in the week that it seems there&amp;rsquo;s a whole work-week ahead to fill (like Monday but with even more &lt;a href=&#34;https://en.wikipedia.org/wiki/Temporal_discounting&#34; target=&#34;_blank&#34;&gt;temporal discounting&lt;/a&gt;). Whereas Fridays, well, dammit! That&amp;rsquo;s basically the weekend already. And you just agreed to a bunch of referee work yesterday!&lt;/p&gt;

&lt;h1 id=&#34;future-research&#34;&gt;Future Research&lt;/h1&gt;

&lt;p&gt;Well, it&amp;rsquo;s a theory. Clearly, more research is needed. In a future post I&amp;rsquo;ll dig into the data some more to look for possible explanations.&lt;/p&gt;

&lt;h1 id=&#34;technical-notes&#34;&gt;Technical Notes&lt;/h1&gt;

&lt;p&gt;This post was written in R Markdown and the source is &lt;a href=&#34;https://github.com/jweisber/rgo/blob/master/editor%20favourite%20days/editor%20favourite%20days.Rmd&#34; target=&#34;_blank&#34;&gt;available on GitHub&lt;/a&gt;. I&amp;rsquo;m new to both R and classical statistics, and this post is a learning exercise for me. So I encourage you to check the code and contact me with corrections.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:0&#34;&gt;Except for Saturdays where the 2015 data is sparse, at only 32 invites. Just 6 referees would have had to respond differently to Saturday-invites in 2015 to eliminate the difference from 2016.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:0&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:1&#34;&gt;For the statistically inclined, if the null hypothesis is that a referee&amp;rsquo;s response is independent of the day of the week of the invite, then: $\chi^2(6, N = 1280) = 18.93$, $p = 0.004$. If instead the null hypothesis is that a referee&amp;rsquo;s response is independent of whether or not the day of the invite is a Monday or Thursday, then: $\chi^2(1, N = 1280) = 16.35$, $p = 10^{-4}$.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;$\chi^2(1, N = 403) = 0.98$, $p = 0.322$.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;$\chi^2(4, N = 877) = 0.9$, $p = 0.924$.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>